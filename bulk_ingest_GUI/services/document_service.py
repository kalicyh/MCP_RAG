"""
ÊñáÊ°£ÊúçÂä°Áî®‰∫é Bulk Ingest GUI
‰∏é MCP ÊúçÂä°Âô®ÁöÑÊñ∞Ê®°ÂùóÂåñÁªìÊûÑÈõÜÊàê

Ê≠§ÊúçÂä°‰ΩøÁî®‰∏é MCP ÊúçÂä°Âô®Áõ∏ÂêåÁöÑÊï∞ÊçÆÂ∫ìÔºå‰ª•‰øùÊåÅ GUI ÂíåÊúçÂä°Âô®‰πãÈó¥ÁöÑÊï∞ÊçÆ‰∏ÄËá¥ÊÄß„ÄÇ
"""

import os
import sys
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import threading
import queue
from pathlib import Path

# Configurar sys.path para importaciones absolutas
current_dir = Path(__file__).parent.resolve()
project_root = current_dir.parent.parent.resolve()
sys.path.insert(0, str(current_dir.parent))
sys.path.insert(0, str(project_root))

# Importar el wrapper de rag_core que maneja la configuraci√≥n del servidor MCP
import bulk_ingest_GUI.rag_core_wrapper as rag_core_wrapper

from models.document_model import DocumentPreview, DocumentMetadata
from gui_utils.constants import SUPPORTED_EXTENSIONS, is_supported_file
from gui_utils.exceptions import (
    ProcessingError, FileProcessingError, DirectoryNotFoundError,
    UnsupportedFileTypeError, ValidationError
)


class DocumentService:
    """
    Áî®‰∫éÂ§ÑÁêÜÊñáÊ°£ÁöÑÊúçÂä°Ôºå‰∏é rag_core.py ÈõÜÊàê
    ‰ΩøÁî®‰∏é MCP ÊúçÂä°Âô®Áõ∏ÂêåÁöÑÊï∞ÊçÆÂ∫ì‰ª•‰øùÊåÅ‰∏ÄËá¥ÊÄß
    """
    
    def __init__(self, config_service):
        self.config_service = config_service
        self.processing_queue = queue.Queue()
        self.processing_thread = None
        self.stop_processing = False
        
        # Â§ÑÁêÜÁªüËÆ°Êï∞ÊçÆ
        self.stats = {
            'total_processed': 0,  # ÊÄªÂ§ÑÁêÜÊï∞
            'successful': 0,       # ÊàêÂäüÊï∞
            'failed': 0,           # Â§±Ë¥•Êï∞
            'skipped': 0,          # Ë∑≥ËøáÊï∞
            'total_size': 0        # ÊÄªÂ§ßÂ∞è
        }
        
        # È™åËØÅ rag_core ÊòØÂê¶ÂèØÁî®
        try:
            rag_core_wrapper.get_rag_functions()
            print("‚úÖ DocumentService: rag_core ÈÖçÁΩÆÊàêÂäü")
        except ImportError as e:
            print(f"‚ùå DocumentService: ÈÖçÁΩÆ rag_core Êó∂Âá∫Èîô: {e}")
            raise
    
    def process_directory(self, directory_path: str, save_markdown: bool = True, 
                         progress_callback=None, log_callback=None) -> List[DocumentPreview]:
        """
        Â§ÑÁêÜÁõÆÂΩï‰∏≠ÁöÑÊâÄÊúâÊñáÊ°£
        
        ÂèÇÊï∞Ôºö
            directory_path: Ë¶ÅÂ§ÑÁêÜÁöÑÁõÆÂΩïË∑ØÂæÑ
            save_markdown: ÊòØÂê¶‰øùÂ≠ò‰∏∫ Markdown Ê†ºÂºè
            progress_callback: Áî®‰∫éÊä•ÂëäËøõÂ∫¶ÁöÑÂõûË∞ÉÂáΩÊï∞
            log_callback: Áî®‰∫éÊä•ÂëäÊó•ÂøóÁöÑÂõûË∞ÉÂáΩÊï∞
            
        ËøîÂõûÔºö
            Â§ÑÁêÜÁöÑÊñáÊ°£ÂàóË°®
        """
        if not os.path.isdir(directory_path):
            raise DirectoryNotFoundError(directory_path)
        
        # Ê∏ÖÁ©∫ÁªüËÆ°Êï∞ÊçÆ
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_size': 0
        }
        
        # Êü•ÊâæÊâÄÊúâÊîØÊåÅÁöÑÊñá‰ª∂
        files_to_process = self._find_supported_files(directory_path)
        
        if not files_to_process:
            if log_callback:
                log_callback("Êú™Âú®ÁõÆÂΩï‰∏≠ÊâæÂà∞ÊîØÊåÅÁöÑÊñá‰ª∂")
            return []
        
        if log_callback:
            log_callback(f"ÊâæÂà∞ {len(files_to_process)} ‰∏™Êñá‰ª∂ÂæÖÂ§ÑÁêÜ")
        
        # Â§ÑÁêÜÊñá‰ª∂
        processed_documents = []
        
        for i, file_path in enumerate(files_to_process):
            if self.stop_processing:
                if log_callback:
                    log_callback("Áî®Êà∑ÂÅúÊ≠¢‰∫ÜÂ§ÑÁêÜ")
                break
            
            try:
                # Êä•ÂëäËøõÂ∫¶
                if progress_callback:
                    progress_callback(i + 1, len(files_to_process), os.path.basename(file_path))
                
                # Â§ÑÁêÜÂçï‰∏™ÊñáÊ°£
                document = self._process_single_file(file_path, save_markdown, log_callback)
                
                if document:
                    processed_documents.append(document)
                    self.stats['successful'] += 1
                    self.stats['total_size'] += document.metadata.size_bytes
                else:
                    self.stats['skipped'] += 1
                
                self.stats['total_processed'] += 1
                
            except Exception as e:
                self.stats['failed'] += 1
                self.stats['total_processed'] += 1
                if log_callback:
                    log_callback(f"Â§ÑÁêÜ {os.path.basename(file_path)} Êó∂Âá∫Èîô: {str(e)}")
        
        if log_callback:
            log_callback(f"Â§ÑÁêÜÂÆåÊàê: {self.stats['successful']} ÊàêÂäü, "
                        f"{self.stats['failed']} Â§±Ë¥•, {self.stats['skipped']} Ë∑≥Ëøá")
        
        return processed_documents
    
    def _find_supported_files(self, directory_path: str) -> List[str]:
        """Êü•ÊâæÁõÆÂΩï‰∏≠ÊâÄÊúâÊîØÊåÅÁöÑÊñá‰ª∂"""
        supported_files = []
        
        for root, _, files in os.walk(directory_path):
            for file in files:
                file_path = os.path.join(root, file)
                if is_supported_file(file):
                    supported_files.append(file_path)
        
        return supported_files
    
    def _process_single_file(self, file_path: str, save_markdown: bool, 
                           log_callback=None) -> Optional[DocumentPreview]:
        """
        ‰ΩøÁî® rag_core.py Â§ÑÁêÜÂçï‰∏™Êñá‰ª∂
        
        ÂèÇÊï∞Ôºö
            file_path: Ë¶ÅÂ§ÑÁêÜÁöÑÊñá‰ª∂Ë∑ØÂæÑ
            save_markdown: ÊòØÂê¶‰øùÂ≠ò‰∏∫ Markdown Ê†ºÂºè
            log_callback: Áî®‰∫éÊä•ÂëäÊó•ÂøóÁöÑÂõûË∞ÉÂáΩÊï∞
            
        ËøîÂõûÔºö
            Â¶ÇÊûúÊàêÂäüÂ§ÑÁêÜÔºåËøîÂõû DocumentPreviewÔºõÂ¶ÇÊûúË∑≥ËøáÔºåËøîÂõû None
        """
        try:
            if log_callback:
                log_callback(f"Ê≠£Âú®Â§ÑÁêÜ: {os.path.basename(file_path)}")
            
            # ‰ΩøÁî® rag_core Âä†ËΩΩÂ∏¶ÊúâÁªìÊûÑÂåñÂÖÉÁ¥†ÁöÑÊñáÊ°£
            markdown_content, metadata, structural_elements = rag_core_wrapper.load_document_with_elements(file_path)
            
            if not markdown_content or markdown_content.isspace():
                if log_callback:
                    log_callback(f"ÊñáÊ°£‰∏∫Á©∫: {os.path.basename(file_path)}")
                return None
            
            # ÂàõÂª∫Â¢ûÂº∫ÁöÑÂÖÉÊï∞ÊçÆ
            enhanced_metadata = self._create_enhanced_metadata(file_path, metadata, markdown_content)
            
            # Â¶ÇÊûúÈúÄË¶ÅÔºå‰øùÂ≠ò‰∏∫ Markdown Ê†ºÂºè
            if save_markdown:
                md_path = self._save_markdown_copy(file_path, markdown_content)
                if md_path:
                    enhanced_metadata['converted_to_md'] = md_path
            
            # ÂàõÂª∫ÊñáÊ°£
            document = DocumentPreview(
                file_path=file_path,
                markdown_content=markdown_content,
                file_type=os.path.splitext(file_path)[1].lower(),
                original_name=os.path.basename(file_path),
                metadata=enhanced_metadata,
                structural_elements=structural_elements
            )
            
            # È™åËØÅÊñáÊ°£
            document.validate()
            
            if log_callback:
                log_callback(f"‚úÖ {os.path.basename(file_path)} Â§ÑÁêÜÊàêÂäü ({len(markdown_content)} Â≠óÁ¨¶)")
            
            return document
            
        except Exception as e:
            if log_callback:
                log_callback(f"‚ùå Â§ÑÁêÜ {os.path.basename(file_path)} Êó∂Âá∫Èîô: {str(e)}")
            raise FileProcessingError(file_path, e)
    
    def _create_enhanced_metadata(self, file_path: str, original_metadata: Dict[str, Any], 
                                content: str) -> Dict[str, Any]:
        """ÂàõÂª∫ÊñáÊ°£ÁöÑÂ¢ûÂº∫ÂÖÉÊï∞ÊçÆ"""
        enhanced_metadata = original_metadata.copy() if original_metadata else {}
        
        # Ê∑ªÂä†È¢ùÂ§ñ‰ø°ÊÅØ
        enhanced_metadata.update({
            'file_path': file_path,  # Êñá‰ª∂Ë∑ØÂæÑ
            'original_name': os.path.basename(file_path),  # ÂéüÂßãÊñá‰ª∂Âêç
            'file_type': os.path.splitext(file_path)[1].lower(),  # Êñá‰ª∂Á±ªÂûã
            'processed_date': datetime.now().isoformat(),  # Â§ÑÁêÜÊó•Êúü
            'size_bytes': len(content.encode('utf-8')),  # Êñá‰ª∂Â§ßÂ∞èÔºàÂ≠óËäÇÔºâ
            'word_count': len(content.split()),  # ÂçïËØçÊï∞
            'character_count': len(content),  # Â≠óÁ¨¶Êï∞
            'processing_service': 'document_service_v2'  # Â§ÑÁêÜÊúçÂä°ÂêçÁß∞
        })
        
        return enhanced_metadata
    
    def _save_markdown_copy(self, file_path: str, content: str) -> Optional[str]:
        """‰ΩøÁî® MCP ÊúçÂä°Âô®ÈÖçÁΩÆ‰øùÂ≠òÊñáÊ°£ÁöÑ Markdown ÂâØÊú¨"""
        try:
            # Ëé∑Âèñ MCP ÊúçÂä°Âô®ÁöÑÈÖçÁΩÆ
            current_dir = Path(__file__).parent.resolve()
            project_root = current_dir.parent.parent.resolve()
            mcp_data_dir = project_root / "mcp_server_organized" / "data" / "documents"
            
            # Â¶ÇÊûúÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåÂàôÂàõÂª∫
            mcp_data_dir.mkdir(parents=True, exist_ok=True)
            
            # ÁîüÊàê Markdown Êñá‰ª∂Âêç
            original_name = os.path.basename(file_path)
            name_without_ext = os.path.splitext(original_name)[0]
            md_filename = f"{name_without_ext}.md"
            md_filepath = mcp_data_dir / md_filename
            
            # ‰øùÂ≠òÊñá‰ª∂
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return str(md_filepath)
            
        except Exception as e:
            rag_core_wrapper.log(f"‰øùÂ≠ò Markdown ÂâØÊú¨Êó∂Âá∫Èîô: {e}")
            return None
    
    def store_documents(self, documents: List[DocumentPreview], 
                       progress_callback=None, log_callback=None) -> Dict[str, Any]:
        """Â≠òÂÇ®ÊñáÊ°£Âà∞Êï∞ÊçÆÂ∫ì‰∏≠Ôºå‰ΩøÁî® rag_core.py"""
        if not documents:
            return {'status': 'no_documents', 'message': 'Ê≤°ÊúâÊñáÊ°£ÂèØÂ≠òÂÇ®'}
        
        try:
            if log_callback:
                log_callback("üöÄ ÂºÄÂßãÂ∞ÜÊñáÊ°£Â≠òÂÇ®Âà∞Êï∞ÊçÆÂ∫ì...")
            
            # ÈÖçÁΩÆÂêëÈáèÊï∞ÊçÆÂ∫ìÔºå‰ΩøÁî® rag_core
            if log_callback:
                log_callback("‚öôÔ∏è Ê≠£Âú®ÈÖçÁΩÆÂêëÈáèÊï∞ÊçÆÂ∫ì...")
            
            vector_store = rag_core_wrapper.get_vector_store()
            
            if log_callback:
                log_callback("‚úÖ ÂêëÈáèÊï∞ÊçÆÂ∫ìÈÖçÁΩÆÊàêÂäü")
            
            # Â≠òÂÇ®ÁªüËÆ°Êï∞ÊçÆ
            storage_stats = {
                'total_documents': len(documents),
                'successful': 0,
                'failed': 0,
                'errors': []
            }
            
            # Â§ÑÁêÜÊØè‰∏™ÊñáÊ°£
            for i, document in enumerate(documents):
                if progress_callback:
                    progress_callback(i + 1, len(documents), document.original_name)
                
                try:
                    # ÂàõÂª∫Â≠òÂÇ®ÂÖÉÊï∞ÊçÆ
                    source_metadata = {
                        "source": document.original_name,  # Êù•Ê∫êÊñá‰ª∂Âêç
                        "file_path": document.file_path,  # Êñá‰ª∂Ë∑ØÂæÑ
                        "file_type": document.file_type,  # Êñá‰ª∂Á±ªÂûã
                        "processed_date": datetime.now().isoformat(),  # Â§ÑÁêÜÊó•Êúü
                        "converted_to_md": "ÊòØ" if hasattr(document.metadata, 'converted_to_md') else "Âê¶",  # ÊòØÂê¶ËΩ¨Êç¢‰∏∫ Markdown
                        "size_bytes": document.metadata.size_bytes,  # Êñá‰ª∂Â§ßÂ∞è
                        "word_count": document.metadata.word_count,  # ÂçïËØçÊï∞
                        "processing_method": document.metadata.processing_method  # Â§ÑÁêÜÊñπÊ≥ï
                    }
                    
                    # Â¶ÇÊûúÊúâÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÊ∑ªÂä†Âà∞ÂÖÉÊï∞ÊçÆ‰∏≠
                    if hasattr(document.metadata, 'structural_info'):
                        source_metadata['structural_info'] = document.metadata.structural_info
                    
                    # ‰ΩøÁî® rag_core ËøõË°åËØ≠‰πâÂàÜÂùóÂ≠òÂÇ®
                    rag_core_wrapper.add_text_to_knowledge_base_enhanced(
                        document.markdown_content,
                        vector_store,
                        source_metadata,
                        use_semantic_chunking=True,
                        structural_elements=document.structural_elements
                    )
                    
                    storage_stats['successful'] += 1
                    
                    if log_callback:
                        log_callback(f"‚úÖ {document.original_name} ÊàêÂäüÂ≠òÂÇ®")
                    
                except Exception as e:
                    storage_stats['failed'] += 1
                    error_msg = f"Â≠òÂÇ® {document.original_name} Êó∂Âá∫Èîô: {str(e)}"
                    storage_stats['errors'].append(error_msg)
                    
                    if log_callback:
                        log_callback(f"‚ùå {error_msg}")
            
            # ÊúÄÁªàÁªìÊûú
            if log_callback:
                log_callback(f"üéâ Â≠òÂÇ®ÂÆåÊàê: {storage_stats['successful']} ÊàêÂäü, "
                           f"{storage_stats['failed']} Â§±Ë¥•")
            
            return {
                'status': 'completed',
                'stats': storage_stats,
                'message': f"Â≠òÂÇ®ÂÆåÊàê: {storage_stats['successful']} ‰∏™ÊñáÊ°£"
            }
            
        except Exception as e:
            error_msg = f"Â≠òÂÇ®ËøáÁ®ã‰∏≠ÂèëÁîü‰∏ÄËà¨ÊÄßÈîôËØØ: {str(e)}"
            if log_callback:
                log_callback(f"üí• {error_msg}")
            
            return {
                'status': 'error',
                'message': error_msg,
                'stats': storage_stats
            }
    
    def get_cache_statistics(self) -> Dict[str, Any]:
        """Obtiene estad√≠sticas del cache de embeddings usando rag_core"""
        try:
            return rag_core_wrapper.get_cache_stats()
        except Exception as e:
            return {'error': str(e)}
    
    def clear_cache(self):
        """Limpia el cache de embeddings usando rag_core"""
        try:
            rag_core_wrapper.clear_embedding_cache()
            return {'status': 'success', 'message': 'Cache limpiado exitosamente'}
        except Exception as e:
            return {'status': 'error', 'message': f'Error limpiando cache: {str(e)}'}
    
    def validate_document(self, document: DocumentPreview) -> bool:
        """Valida un documento usando las reglas del servicio"""
        try:
            return document.validate()
        except Exception:
            return False
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """Obtiene estad√≠sticas del procesamiento"""
        return self.stats.copy()
    
    def stop_processing(self):
        """Detiene el procesamiento en curso"""
        self.stop_processing = True
    
    def reset_statistics(self):
        """Reinicia lasÁªüËÆ°Êï∞ÊçÆÂ§ÑÁêÜ"""
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_size': 0
        }
    
    def get_database_statistics(self) -> Dict[str, Any]:
        """Obtiene estad√≠sticas avanzadas de la base de datos usando rag_core"""
        try:
            return rag_core_wrapper.get_vector_store_stats_advanced()
        except Exception as e:
            return {'error': str(e)}