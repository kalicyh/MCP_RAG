"""
MCP æ–‡æ¡£å·¥å…·
===========

æ­¤æ¨¡å—åŒ…å«ä¸æ–‡æ¡£å¤„ç†ç›¸å…³çš„å·¥å…·ã€‚
ä» rag_server.py è¿ç§»è€Œæ¥ï¼Œç”¨äºæ¨¡å—åŒ–æ¶æ„ã€‚

æ³¨æ„ï¼šè¿™äº›å‡½æ•°è¢«è®¾è®¡ä¸ºåœ¨ä¸»æœåŠ¡å™¨ä¸­ä½¿ç”¨ @mcp.tool() è£…é¥°å™¨ã€‚
"""

import os
import tempfile
import requests
from datetime import datetime
from urllib.parse import urlparse
from rag_core_openai import (
    add_text_to_knowledge_base,
    add_text_to_knowledge_base_enhanced,
    load_document_with_elements
)
from utils.logger import log
from models import DocumentModel, MetadataModel

# å¿…é¡»åœ¨æœåŠ¡å™¨ä¸­å¯ç”¨çš„å…¨å±€å˜é‡
rag_state = {}
initialize_rag_func = None
save_processed_copy_func = None
md_converter = None  # deprecated placeholder, MarkItDown support removed

def set_rag_state(state):
    """è®¾ç½®å…¨å±€ RAG çŠ¶æ€ã€‚"""
    global rag_state
    rag_state = state

def set_initialize_rag_func(func):
    """è®¾ç½® RAG åˆå§‹åŒ–å‡½æ•°ã€‚"""
    global initialize_rag_func
    initialize_rag_func = func

def set_md_converter(converter):
    """Deprecated: previously used to set a MarkItDown converter. This is now a no-op kept for API compatibility."""
    global md_converter
    md_converter = converter
    log("è­¦å‘Š: set_md_converter å·²å¼ƒç”¨ â€” MarkItDown æ”¯æŒå·²ç§»é™¤ï¼Œä¼ å…¥çš„è½¬æ¢å™¨å°†è¢«å¿½ç•¥ã€‚")

def set_save_processed_copy_func(func):
    """è®¾ç½®ä¿å­˜å¤„ç†å‰¯æœ¬çš„å‡½æ•°ã€‚"""
    global save_processed_copy_func
    save_processed_copy_func = func

def initialize_rag():
    """åˆå§‹åŒ– RAG ç³»ç»Ÿã€‚"""
    if initialize_rag_func:
        initialize_rag_func()
    elif "initialized" in rag_state:
        return
    # æ­¤å‡½æ•°å¿…é¡»åœ¨ä¸»æœåŠ¡å™¨ä¸­å®ç°
    pass

def save_processed_copy(file_path: str, processed_content: str, processing_method: str = "unstructured") -> str:
    """ä¿å­˜æ–‡æ¡£çš„å¤„ç†å‰¯æœ¬ã€‚"""
    if save_processed_copy_func:
        return save_processed_copy_func(file_path, processed_content, processing_method)
    return ""

def learn_text(text: str, source_name: str = "manual_input") -> str:
    """
    å°†æ–°çš„æ–‡æœ¬ç‰‡æ®µæ·»åŠ åˆ° RAG çŸ¥è¯†åº“ä»¥ä¾›å°†æ¥å‚è€ƒã€‚
    å½“æ‚¨æƒ³è¦æ•™æˆAIåº”è¯¥è®°ä½çš„æ–°ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - æ·»åŠ äº‹å®ã€å®šä¹‰æˆ–è§£é‡Š
    - å­˜å‚¨å¯¹è¯ä¸­çš„é‡è¦ä¿¡æ¯
    - ä¿å­˜ç ”ç©¶å‘ç°æˆ–ç¬”è®°
    - æ·»åŠ ç‰¹å®šä¸»é¢˜çš„ä¸Šä¸‹æ–‡

    Args:
        text: è¦å­¦ä¹ å¹¶å­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­çš„æ–‡æœ¬å†…å®¹ã€‚
        source_name: æ¥æºçš„æè¿°æ€§åç§°ï¼ˆä¾‹å¦‚ï¼Œ"user_notes"ã€"research_paper"ã€"conversation_summary"ï¼‰ã€‚
    """
    log(f"MCP Server: æ­£åœ¨å¤„ç†æ¥è‡ªæº {source_name} çš„ {len(text)} ä¸ªå­—ç¬¦çš„æ–‡æœ¬")
    initialize_rag()
    
    try:
        # ä½¿ç”¨ MetadataModel åˆ›å»ºç»“æ„åŒ–å…ƒæ•°æ®
        metadata_model = MetadataModel(
            source=source_name,
            input_type="manual_text",
            processed_date=datetime.now(),
            processing_method="manual_input",
            chunking_method="standard",
            chunk_count=1,
            avg_chunk_size=len(text)
        )
        
        # è½¬æ¢ä¸ºå­—å…¸ä»¥ä¸æ ¸å¿ƒå…¼å®¹
        source_metadata = metadata_model.to_dict()
        
        # ä½¿ç”¨æ ¸å¿ƒå‡½æ•°å°†æ–‡æœ¬ä¸å…ƒæ•°æ®ä¸€èµ·æ·»åŠ 
        add_text_to_knowledge_base(text, rag_state["vector_store"], source_metadata)
        log(f"MCP Server: æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“")
        return f"æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚ç‰‡æ®µ: '{text[:70]}...' (æ¥æº: {source_name})"
    except Exception as e:
        log(f"MCP Server: æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return f"æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}"

def learn_document(file_path: str) -> str:
    """
    ä½¿ç”¨é«˜çº§ Unstructured å¤„ç†å’ŒçœŸæ­£çš„è¯­ä¹‰åˆ†å—è¯»å–å’Œå¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    å½“æ‚¨æƒ³è¦é€šè¿‡æ™ºèƒ½å¤„ç†ä»æ–‡æ¡£æ–‡ä»¶ä¸­æ•™æˆAIæ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼šPDFã€DOCXã€PPTXã€XLSXã€TXTã€HTMLã€CSVã€JSONã€XMLã€ODTã€ODPã€ODSã€RTFã€
    å›¾åƒï¼ˆPNGã€JPGã€TIFFã€BMP å¸¦OCRï¼‰ã€ç”µå­é‚®ä»¶ï¼ˆEMLã€MSGï¼‰ä»¥åŠæ€»å…±è¶…è¿‡25ç§æ ¼å¼ã€‚
    
    é«˜çº§åŠŸèƒ½ï¼š
    - åŸºäºæ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ï¼‰çš„çœŸæ­£è¯­ä¹‰åˆ†å—
    - æ™ºèƒ½æ–‡æ¡£ç»“æ„ä¿æŒï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ï¼‰
    - è‡ªåŠ¨å™ªéŸ³å»é™¤ï¼ˆé¡µçœ‰ã€é¡µè„šã€æ— å…³å†…å®¹ï¼‰
    - ç»“æ„å…ƒæ•°æ®æå–
    - ä»»ä½•æ–‡æ¡£ç±»å‹çš„å¥å£®å›é€€ç³»ç»Ÿ
    - é€šè¿‡è¯­ä¹‰è¾¹ç•Œå¢å¼ºä¸Šä¸‹æ–‡ä¿æŒ
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - å¤„ç†å…·æœ‰å¤æ‚å¸ƒå±€çš„ç ”ç©¶è®ºæ–‡æˆ–æ–‡ç« 
    - ä»å¸¦æœ‰è¡¨æ ¼å’Œåˆ—è¡¨çš„æŠ¥å‘Šæˆ–æ‰‹å†Œä¸­æ·»åŠ å†…å®¹
    - ä»å¸¦æœ‰æ ¼å¼çš„ç”µå­è¡¨æ ¼å¯¼å…¥æ•°æ®
    - å°†æ¼”ç¤ºæ–‡ç¨¿è½¬æ¢ä¸ºå¯æœç´¢çš„çŸ¥è¯†
    - ä½¿ç”¨OCRå¤„ç†æ‰«ææ–‡æ¡£
    
    æ–‡æ¡£å°†é€šè¿‡çœŸæ­£çš„è¯­ä¹‰åˆ†å—è¿›è¡Œæ™ºèƒ½å¤„ç†ï¼Œå¹¶ä½¿ç”¨å¢å¼ºçš„å…ƒæ•°æ®å­˜å‚¨ã€‚
    å¤„ç†åçš„æ–‡æ¡£å‰¯æœ¬å°†è¢«ä¿å­˜ä»¥ä¾›éªŒè¯ã€‚

    Args:
        file_path: è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶çš„ç»å¯¹æˆ–ç›¸å¯¹è·¯å¾„ã€‚
    """
    log(f"MCP Server: å¼€å§‹é«˜çº§æ–‡æ¡£å¤„ç†: {file_path}")
    log(f"MCP Server: DEBUG - æ¥æ”¶åˆ°çš„è·¯å¾„: {repr(file_path)}")
    log(f"MCP Server: DEBUG - æ£€æŸ¥ç»å¯¹è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.abspath(file_path)}")
    initialize_rag()  # ç¡®ä¿ RAG ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ª
    
    try:
        if not os.path.exists(file_path):
            log(f"MCP Server: åœ¨è·¯å¾„ä¸­æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}")
            return f"é”™è¯¯: åœ¨ '{file_path}' ä¸­æœªæ‰¾åˆ°æ–‡ä»¶"

        log(f"MCP Server: ä½¿ç”¨é«˜çº§ Unstructured ç³»ç»Ÿå¤„ç†æ–‡æ¡£...")
        
        # ä½¿ç”¨æ–°çš„å¸¦ç»“æ„å…ƒç´ çš„å¤„ç†ç³»ç»Ÿ
        processed_content, raw_metadata, structural_elements = load_document_with_elements(file_path)

        if not processed_content or processed_content.isspace():
            log(f"MCP Server: è­¦å‘Š: æ–‡æ¡£å·²å¤„ç†ä½†æ— æ³•æå–å†…å®¹: {file_path}")
            return f"è­¦å‘Š: æ–‡æ¡£ '{file_path}' å·²å¤„ç†ï¼Œä½†æ— æ³•æå–æ–‡æœ¬å†…å®¹ã€‚"

        log(f"MCP Server: æ–‡æ¡£å¤„ç†æˆåŠŸ ({len(processed_content)} ä¸ªå­—ç¬¦)")
        
        # åˆ›å»ºç»“æ„åŒ–æ–‡æ¡£æ¨¡å‹
        file_name = os.path.basename(file_path)
        file_type = os.path.splitext(file_path)[1].lower()
        file_size = os.path.getsize(file_path)
        
        # æå–ç»“æ„ä¿¡æ¯
        structural_info = raw_metadata.get("structural_info", {})
        titles_count = structural_info.get("titles_count", 0)
        tables_count = structural_info.get("tables_count", 0)
        lists_count = structural_info.get("lists_count", 0)
        total_elements = structural_info.get("total_elements", 0)
        
        # åˆ›å»º DocumentModel
        document_model = DocumentModel(
            file_path=file_path,
            file_name=file_name,
            file_type=file_type,
            file_size=file_size,
            content=processed_content,  # åŸå§‹å†…å®¹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä¸å¤„ç†åçš„å†…å®¹ç›¸åŒï¼‰
            processed_content=processed_content,
            processing_method=raw_metadata.get("processing_method", "unstructured_enhanced"),
            processing_date=datetime.now(),
            structural_elements=structural_elements or [],
            total_elements=total_elements,
            titles_count=titles_count,
            tables_count=tables_count,
            lists_count=lists_count,
            chunk_count=0  # åˆ†å—åå°†è®¡ç®—
        )
        
        # åˆ›å»º MetadataModel
        metadata_model = MetadataModel(
            source=file_name,
            input_type="file_upload",
            processed_date=datetime.now(),
            file_path=file_path,
            file_type=file_type,
            file_size=file_size,
            processing_method=raw_metadata.get("processing_method", "unstructured_enhanced"),
            structural_info=structural_info,
            total_elements=total_elements,
            titles_count=titles_count,
            tables_count=tables_count,
            lists_count=lists_count,
            narrative_blocks=structural_info.get("narrative_blocks", 0),
            other_elements=structural_info.get("other_elements", 0),
            chunking_method="semantic" if structural_elements else "standard",
            avg_chunk_size=len(processed_content) / max(total_elements, 1)
        )
        
        # éªŒè¯æ–‡æ¡£
        if not document_model.is_valid():
            log(f"MCP Server: é”™è¯¯: æ ¹æ®æ¨¡å‹æ ‡å‡†æ–‡æ¡£æ— æ•ˆ")
            return f"é”™è¯¯: å¤„ç†åçš„æ–‡æ¡£ä¸ç¬¦åˆæœ‰æ•ˆæ€§æ ‡å‡†"
        
        log(f"MCP Server: æ–‡æ¡£å’Œå…ƒæ•°æ®æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        log(f"MCP Server: æ–‡æ¡£æ‘˜è¦: {document_model.get_summary()}")
        log(f"MCP Server: å…ƒæ•°æ®æ‘˜è¦: {metadata_model.get_summary()}")
        
        # ä¿å­˜å¤„ç†åçš„å‰¯æœ¬
        log(f"MCP Server: ä¿å­˜å¤„ç†åçš„å‰¯æœ¬...")
        saved_copy_path = save_processed_copy(file_path, processed_content, document_model.processing_method)
        
        # ä½¿ç”¨çœŸæ­£çš„è¯­ä¹‰åˆ†å—å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“
        log(f"MCP Server: ä½¿ç”¨ç»“æ„å…ƒæ•°æ®å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“...")
        
        # å°†å…ƒæ•°æ®è½¬æ¢ä¸ºå­—å…¸ä»¥ä¸æ ¸å¿ƒå…¼å®¹
        enhanced_metadata = metadata_model.to_dict()
        
        # ä½¿ç”¨å¢å¼ºå‡½æ•°å’Œç»“æ„å…ƒç´ è¿›è¡ŒçœŸæ­£çš„è¯­ä¹‰åˆ†å—
        add_text_to_knowledge_base_enhanced(
            processed_content, 
            rag_state["vector_store"], 
            enhanced_metadata, 
            use_semantic_chunking=True,
            structural_elements=structural_elements
        )
        
        log(f"MCP Server: å¤„ç†å®Œæˆ - æ–‡æ¡£å¤„ç†æˆåŠŸ")
        
        # ä½¿ç”¨çš„åˆ†å—ä¿¡æ¯
        chunking_info = ""
        if structural_elements and len(structural_elements) > 1:
            chunking_info = f"ğŸ§  é«˜çº§è¯­ä¹‰åˆ†å— åŒ…å« {len(structural_elements)} ä¸ªç»“æ„å…ƒç´ "
        elif metadata_model.is_rich_content():
            chunking_info = f"ğŸ“Š å¢å¼ºè¯­ä¹‰åˆ†å— åŸºäºç»“æ„å…ƒæ•°æ®"
        else:
            chunking_info = f"ğŸ“ ä¼˜åŒ–ä¼ ç»Ÿåˆ†å—"
        
        return f"""âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ
ğŸ“„ æ–‡ä»¶: {document_model.file_name}
ğŸ“‹ ç±»å‹: {(document_model.file_type or 'unknown').upper()}
ğŸ”§ æ–¹æ³•: {document_model.processing_method}
{chunking_info}
ğŸ“Š å¤„ç†å­—ç¬¦æ•°: {len(processed_content):,}
ğŸ“ˆ ç»“æ„: {titles_count} ä¸ªæ ‡é¢˜, {tables_count} ä¸ªè¡¨æ ¼, {lists_count} ä¸ªåˆ—è¡¨
ğŸ’¾ ä¿å­˜çš„å‰¯æœ¬: {saved_copy_path if saved_copy_path else "ä¸å¯ç”¨"}
âœ… éªŒè¯: ä½¿ç”¨ç»“æ„åŒ–æ¨¡å‹å¤„ç†çš„æ–‡æ¡£"""

    except Exception as e:
        log(f"MCP Server: å¤„ç†æ–‡æ¡£ '{file_path}' æ—¶å‡ºé”™: {e}")
        return f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}"

def learn_from_url(url: str) -> str:
    """
    å¤„ç†æ¥è‡ª URLï¼ˆç½‘é¡µæˆ– YouTube è§†é¢‘ï¼‰çš„å†…å®¹å¹¶å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    å½“æ‚¨æƒ³è¦ä»ç½‘ç»œå†…å®¹ä¸­æ•™æˆAIè€Œæ— éœ€ä¸‹è½½æ–‡ä»¶æ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    æ”¯æŒçš„ URL ç±»å‹ï¼š
    - ç½‘é¡µï¼ˆHTML å†…å®¹ï¼‰
    - YouTube è§†é¢‘ï¼ˆè½¬å½•æ–‡æœ¬ï¼‰
    - MarkItDown å¯ä»¥å¤„ç†çš„ä»»ä½• URL
    - ç›´æ¥æ–‡ä»¶ä¸‹è½½ï¼ˆPDFã€DOCX ç­‰ï¼‰- å°†ä½¿ç”¨å¢å¼ºçš„ Unstructured å¤„ç†
    
    ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ï¼š
    - ä»æ–°é—»æ–‡ç« æˆ–åšå®¢æ–‡ç« æ·»åŠ å†…å®¹
    - å¤„ç† YouTube è§†é¢‘è½¬å½•æ–‡æœ¬
    - ä»ç½‘é¡µå¯¼å…¥ä¿¡æ¯
    - å°†ç½‘ç»œå†…å®¹è½¬æ¢ä¸ºå¯æœç´¢çš„çŸ¥è¯†
    - ç›´æ¥ä» URL å¤„ç†æ–‡æ¡£
    
    å†…å®¹å°†è¢«æ™ºèƒ½å¤„ç†å¹¶ä½¿ç”¨å¢å¼ºçš„å…ƒæ•°æ®å­˜å‚¨ã€‚
    å¤„ç†å†…å®¹çš„å‰¯æœ¬å°†è¢«ä¿å­˜ä»¥ä¾›éªŒè¯ã€‚

    Args:
        url: è¦å¤„ç†çš„ç½‘é¡µæˆ–è§†é¢‘çš„ URLã€‚
    """
    log(f"MCP Server: å¼€å§‹å¤„ç† URL: {url}")
    initialize_rag()
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥æ–‡ä»¶ä¸‹è½½ URL
        parsed_url = urlparse(url)
        file_extension = os.path.splitext(parsed_url.path)[1].lower()
        
        # æ”¯æŒå¢å¼ºå¤„ç†çš„æ‰©å±•ååˆ—è¡¨
        enhanced_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', 
                              '.txt', '.html', '.htm', '.csv', '.json', '.xml', '.rtf',
                              '.odt', '.odp', '.ods', '.md', '.yaml', '.yml']
        
        if file_extension in enhanced_extensions:
            log(f"MCP Server: æ£€æµ‹åˆ°å¯ä¸‹è½½æ–‡ä»¶ ({file_extension})ï¼Œä½¿ç”¨å¢å¼ºå¤„ç†...")
            
            # è®¾ç½®ä¸‹è½½è¶…æ—¶
            timeout_seconds = 30
            
            # ä½¿ç”¨è¶…æ—¶ä¸‹è½½æ–‡ä»¶
            log(f"MCP Server: ä½¿ç”¨ {timeout_seconds} ç§’è¶…æ—¶ä¸‹è½½æ–‡ä»¶...")
            response = requests.get(url, stream=True, timeout=timeout_seconds)
            response.raise_for_status()
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    temp_file.write(chunk)
                temp_file_path = temp_file.name
            
            log(f"MCP Server: æ–‡ä»¶ä¸´æ—¶ä¸‹è½½åˆ°: {temp_file_path}")
            
            try:
                # ä½¿ç”¨è¶…æ—¶å¢å¼ºå¤„ç†
                log(f"MCP Server: å¼€å§‹ Unstructured å¤„ç†ï¼ˆå¤§å‹ PDF å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
                
                # å¯¹äº PDFï¼Œä½¿ç”¨æ›´å¿«çš„é…ç½®é¿å…æŒ‚èµ·
                # ç»Ÿä¸€ä½¿ç”¨äº‘ç«¯æ ¸å¿ƒçš„åŠ è½½å‡½æ•°ï¼ˆæœ€å°å®ç°ï¼‰
                processed_content, metadata, structural_elements = load_document_with_elements(temp_file_path)
                
                log(f"MCP Server: ä¸‹è½½çš„æ–‡ä»¶å¤„ç†æˆåŠŸ ({len(processed_content)} ä¸ªå­—ç¬¦)")
                
                # ä¿å­˜å¤„ç†åçš„å‰¯æœ¬
                log(f"MCP Server: ä¿å­˜å¤„ç†åçš„å‰¯æœ¬...")
                processing_method = metadata.get("processing_method", "unstructured_enhanced")
                domain = parsed_url.netloc.replace('.', '_')
                path = parsed_url.path.replace('/', '_').replace('.', '_')
                if not path or path == '_':
                    path = 'homepage'
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{domain}_{path}_{timestamp}_{processing_method}.md"
                processed_filepath = os.path.join("./data/documents", filename)
                
                try:
                    os.makedirs("./data/documents", exist_ok=True)
                    with open(processed_filepath, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    log(f"MCP Server: å¤„ç†åçš„å‰¯æœ¬ä¿å­˜åœ¨: {processed_filepath}")
                except Exception as e:
                    log(f"MCP Server è­¦å‘Š: æ— æ³•ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {e}")
                    processed_filepath = ""
                
                # ä¸°å¯Œå…ƒæ•°æ®
                enhanced_metadata = metadata.copy()
                enhanced_metadata.update({
                    "source": url,
                    "domain": parsed_url.netloc,
                    "input_type": "url_download",
                    "converted_to_md": processed_filepath if processed_filepath else "No",
                    "server_processed_date": datetime.now().isoformat()
                })
                
                # ä½¿ç”¨å¢å¼ºå¤„ç†
                log(f"MCP Server: å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“...")
                add_text_to_knowledge_base_enhanced(
                    processed_content, 
                    rag_state["vector_store"], 
                    enhanced_metadata, 
                    use_semantic_chunking=True,
                    structural_elements=structural_elements if 'structural_elements' in locals() else None
                )
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_file_path)
                    log(f"MCP Server: ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {temp_file_path}")
                except Exception as e:
                    log(f"MCP Server è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {e}")
                
                log(f"MCP Server: å¤„ç†å®Œæˆ - URL å¤„ç†æˆåŠŸ")
                
                # å‡†å¤‡ä¿¡æ¯å“åº”
                file_name = os.path.basename(parsed_url.path) if parsed_url.path != '/' else parsed_url.netloc
                file_type = metadata.get("file_type", file_extension)
                processing_method = metadata.get("processing_method", "unstructured_enhanced")
                
                return f"""âœ… URL å¤„ç†æˆåŠŸ
ğŸŒ URL: {url}
ğŸ“„ æ–‡ä»¶: {file_name}
ğŸ“‹ ç±»å‹: {(file_type or 'unknown').upper()}
ğŸ”§ æ–¹æ³•: {processing_method}
ğŸ“Š å¤„ç†å­—ç¬¦æ•°: {len(processed_content):,}
ğŸ’¾ ä¿å­˜çš„å‰¯æœ¬: {processed_filepath if processed_filepath else "ä¸å¯ç”¨"}"""
                
            except Exception as e:
                # å‡ºé”™æ—¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_file_path)
                except:
                    pass
                raise e
        else:
            method_used = None
            log("MCP Server: ä½¿ç”¨è½»é‡å›é€€ï¼ˆrequests + ç®€å•æ¸…æ´—ï¼‰å¤„ç†ç½‘é¡µ...")
            try:
                resp = requests.get(url, timeout=20)
                resp.raise_for_status()
                html = resp.text
                # åŸºç¡€æ¸…æ´—ï¼šç§»é™¤ script/styleï¼Œå»æ ‡ç­¾ï¼Œå‹ç¼©ç©ºç™½
                import re
                # å»æ‰ script/style å†…å®¹
                html = re.sub(r"<script[\s\S]*?</script>", " ", html, flags=re.IGNORECASE)
                html = re.sub(r"<style[\s\S]*?</style>", " ", html, flags=re.IGNORECASE)
                # æ›¿æ¢å¸¸è§å—æ ‡ç­¾ä¸ºæ¢è¡Œ
                html = re.sub(r"</?(p|div|br|li|ul|ol|tr|td|th|h[1-6])[^>]*>", "\n", html, flags=re.IGNORECASE)
                # å»æ‰€æœ‰å…¶ä½™æ ‡ç­¾
                text = re.sub(r"<[^>]+>", " ", html)
                # HTML å®ä½“æœ€ç®€æ›¿æ¢
                text = (text
                        .replace("&nbsp;", " ")
                        .replace("&amp;", "&")
                        .replace("&lt;", "<")
                        .replace("&gt;", ">")
                        )
                # å‹ç¼©ç©ºç™½å¹¶æˆªæ–­å°¾éƒ¨ç©ºè¡Œ
                text = re.sub(r"\s+", " ", text).strip()
                processed_content = text
                if not processed_content:
                    return f"è­¦å‘Š: URL '{url}' å·²è·å–ï¼Œä½†å†…å®¹ä¸ºç©ºã€‚"
                log(f"MCP Server: å›é€€ç½‘é¡µæå–æˆåŠŸ ({len(processed_content)} ä¸ªå­—ç¬¦)")
                method_used = "html_fallback"
            except Exception as e:
                log(f"MCP Server: å›é€€ç½‘é¡µæå–å¤±è´¥: {e}")
                return f"å¤„ç† URL æ—¶å‡ºé”™: {e}"
            
            # ä¿å­˜å¤„ç†åçš„å‰¯æœ¬
            log(f"MCP Server: ä¿å­˜å¤„ç†åçš„å‰¯æœ¬...")
            domain = parsed_url.netloc.replace('.', '_')
            path = parsed_url.path.replace('/', '_').replace('.', '_')
            if not path or path == '_':
                path = 'homepage'
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{domain}_{path}_{timestamp}_web.md"
            processed_filepath = os.path.join("./data/documents", filename)
            
            try:
                os.makedirs("./data/documents", exist_ok=True)
                with open(processed_filepath, 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                log(f"MCP Server: å¤„ç†åçš„å‰¯æœ¬ä¿å­˜åœ¨: {processed_filepath}")
            except Exception as e:
                log(f"MCP Server è­¦å‘Š: æ— æ³•ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {e}")
                processed_filepath = ""
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = {
                "source": url,
                "domain": parsed_url.netloc,
                "input_type": "url_web",
                "processed_date": datetime.now().isoformat(),
                "processing_method": method_used or "html_fallback",
                "converted_to_md": processed_filepath if processed_filepath else "No",
                "server_processed_date": datetime.now().isoformat()
            }
            
            # å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“
            log(f"MCP Server: å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†åº“...")
            add_text_to_knowledge_base(processed_content, rag_state["vector_store"], metadata)
            
            log(f"MCP Server: å¤„ç†å®Œæˆ - URL å¤„ç†æˆåŠŸ")
            
            # å‡†å¤‡ä¿¡æ¯å“åº”
            return f"""âœ… URL å¤„ç†æˆåŠŸ
ğŸŒ URL: {url}
ğŸ“‹ ç±»å‹: ç½‘é¡µ
ğŸ”§ æ–¹æ³•: {('HTML å›é€€')}
ğŸ“Š å¤„ç†å­—ç¬¦æ•°: {len(processed_content):,}
ğŸ’¾ ä¿å­˜çš„å‰¯æœ¬: {processed_filepath if processed_filepath else "ä¸å¯ç”¨"}"""
                
    except Exception as e:
        log(f"MCP Server: å¤„ç† URL '{url}' æ—¶å‡ºé”™: {e}")
        return f"å¤„ç† URL æ—¶å‡ºé”™: {e}" 