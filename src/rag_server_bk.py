import os
from datetime import datetime
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from markitdown import MarkItDown
from urllib.parse import urlparse

# --- å¯¼å…¥ RAG æ ¸å¿ƒæ¨¡å— ---
from rag_core import (
    add_text_to_knowledge_base,           # å°†æ–‡æœ¬æ·»åŠ åˆ°çŸ¥è¯†åº“çš„å‡½æ•°
    add_text_to_knowledge_base_enhanced,  # å¢å¼ºç‰ˆæ–‡æœ¬æ·»åŠ å‡½æ•°
    load_document_with_fallbacks,         # å¸¦å›é€€æœºåˆ¶çš„æ–‡æ¡£åŠ è½½å‡½æ•°
    get_qa_chain,                         # è·å–é—®ç­”é“¾çš„å‡½æ•°
    get_vector_store,                     # è·å–å‘é‡å­˜å‚¨çš„å‡½æ•°
    search_with_metadata_filters,         # å¸¦å…ƒæ•°æ®è¿‡æ»¤å™¨çš„æœç´¢å‡½æ•°
    create_metadata_filter,               # åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å™¨çš„å‡½æ•°
    get_document_statistics,              # è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    get_cache_stats,                      # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    print_cache_stats,                    # æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    clear_embedding_cache,                # æ¸…é™¤åµŒå…¥ç¼“å­˜çš„å‡½æ•°
    log,  # å¯¼å…¥æ—¥å¿—å‡½æ•°
    optimize_vector_store,
    get_vector_store_stats,
    reindex_vector_store,
    get_optimal_vector_store_profile,
    load_document_with_elements
)

# --- æœåŠ¡å™¨åˆå§‹åŒ–ä¸é…ç½® ---
load_dotenv()
mcp = FastMCP("ragmcp")

rag_state = {}  # çŠ¶æ€åªä¿å­˜å·²å°±ç»ªçš„ç»„ä»¶

md_converter = MarkItDown()  # åˆå§‹åŒ– MarkItDown è½¬æ¢å™¨ï¼ˆç”¨äº URL å¤„ç†ï¼‰

# ç”¨äºä¿å­˜ Markdown å‰¯æœ¬çš„æ–‡ä»¶å¤¹
CONVERTED_DOCS_DIR = "./converted_docs"

def warm_up_rag_system():
    """
    é¢„åŠ è½½ RAG ç³»ç»Ÿçš„é‡å‹ç»„ä»¶ï¼Œé¿å…é¦–æ¬¡è°ƒç”¨å·¥å…·æ—¶çš„å»¶è¿Ÿå’Œå†²çªã€‚
    """
    if "warmed_up" in rag_state:
        return
    
    log("MCPæœåŠ¡å™¨: æ­£åœ¨é¢„çƒ­RAGç³»ç»Ÿ...")
    log("MCPæœåŠ¡å™¨: æ­£åœ¨é¢„åŠ è½½åµŒå…¥æ¨¡å‹åˆ°å†…å­˜...")
    
    # æ­¤è°ƒç”¨å¼ºåˆ¶åŠ è½½åµŒå…¥æ¨¡å‹
    get_vector_store()
    
    rag_state["warmed_up"] = True
    log("MCPæœåŠ¡å™¨: RAGç³»ç»Ÿå·²é¢„çƒ­å¹¶å‡†å¤‡å°±ç»ªã€‚")

def ensure_converted_docs_directory():
    """ç¡®ä¿å­˜åœ¨ç”¨äºå­˜å‚¨è½¬æ¢æ–‡æ¡£çš„æ–‡ä»¶å¤¹ã€‚"""
    if not os.path.exists(CONVERTED_DOCS_DIR):
        os.makedirs(CONVERTED_DOCS_DIR)
        log(f"MCPæœåŠ¡å™¨: å·²åˆ›å»ºè½¬æ¢æ–‡æ¡£æ–‡ä»¶å¤¹: {CONVERTED_DOCS_DIR}")

def save_processed_copy(file_path: str, processed_content: str, processing_method: str = "éç»“æ„åŒ–") -> str:
    """
    ä¿å­˜å¤„ç†åçš„æ–‡æ¡£å‰¯æœ¬ä¸º Markdown æ ¼å¼ã€‚
    
    å‚æ•°ï¼š
        file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
        processed_content: å¤„ç†åçš„å†…å®¹
        processing_method: ä½¿ç”¨çš„å¤„ç†æ–¹æ³•
    
    è¿”å›ï¼š
        ä¿å­˜çš„ Markdown æ–‡ä»¶è·¯å¾„
    """
    ensure_converted_docs_directory()
    
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰
    original_filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(original_filename)[0]
    
    # åˆ›å»ºåŒ…å«æ–¹æ³•ä¿¡æ¯çš„ Markdown æ–‡ä»¶å
    md_filename = f"{name_without_ext}_{processing_method}.md"
    md_filepath = os.path.join(CONVERTED_DOCS_DIR, md_filename)
    
    # ä¿å­˜å†…å®¹åˆ° Markdown æ–‡ä»¶
    try:
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(processed_content)
        log(f"MCPæœåŠ¡å™¨: å·²ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {md_filepath}")
        return md_filepath
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨è­¦å‘Š: æ— æ³•ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {e}")
        return ""

def initialize_rag():
    """
    ä½¿ç”¨æ ¸å¿ƒåˆå§‹åŒ– RAG ç³»ç»Ÿçš„æ‰€æœ‰ç»„ä»¶ã€‚
    """
    if "initialized" in rag_state:
        return

    log("MCPæœåŠ¡å™¨: é€šè¿‡æ ¸å¿ƒåˆå§‹åŒ–RAGç³»ç»Ÿ...")
    
    # ä»æ ¸å¿ƒè·å–å‘é‡å­˜å‚¨å’Œ QA é“¾
    vector_store = get_vector_store()
    qa_chain = get_qa_chain(vector_store)
    
    rag_state["vector_store"] = vector_store
    rag_state["qa_chain"] = qa_chain
    rag_state["initialized"] = True
    log("MCPæœåŠ¡å™¨: RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸã€‚")

# --- å·¥å…·å®ç° ---

@mcp.tool()
def learn_text(text: str, source_name: str = "æ‰‹åŠ¨è¾“å…¥") -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æ·»åŠ ä¸€æ®µæ–°æ–‡æœ¬ä»¥ä¾›å°†æ¥å‚è€ƒã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - æ·»åŠ äº‹å®ã€å®šä¹‰æˆ–è§£é‡Š
    - å­˜å‚¨å¯¹è¯ä¸­çš„é‡è¦ä¿¡æ¯
    - ä¿å­˜ç ”ç©¶å‘ç°æˆ–ç¬”è®°
    - æ·»åŠ ç‰¹å®šä¸»é¢˜çš„ä¸Šä¸‹æ–‡

    å‚æ•°ï¼š
        text: è¦å­¦ä¹ å¹¶å­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­çš„æ–‡æœ¬å†…å®¹ã€‚
        source_name: æ¥æºçš„æè¿°æ€§åç§°ï¼ˆä¾‹å¦‚ "ç”¨æˆ·ç¬”è®°", "ç ”ç©¶è®ºæ–‡", "å¯¹è¯æ‘˜è¦"ï¼‰ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†æ¥è‡ª {source_name} çš„æ–‡æœ¬ï¼Œå…± {len(text)} å­—ç¬¦")
    initialize_rag()
    
    try:
        # åˆ›å»ºæºå…ƒæ•°æ®
        source_metadata = {
            "source": source_name,
            "input_type": "æ‰‹åŠ¨æ–‡æœ¬",
            "processed_date": datetime.now().isoformat()
        }
        # ä½¿ç”¨æ ¸å¿ƒå‡½æ•°æ·»åŠ æ–‡æœ¬åŠå…ƒæ•°æ®
        add_text_to_knowledge_base(text, rag_state["vector_store"], source_metadata)
        log(f"MCPæœåŠ¡å™¨: æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“")
        return f"æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚ç‰‡æ®µ: '{text[:70]}...' (æ¥æº: {source_name})"
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return f"æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}"

@mcp.tool()
def learn_document(file_path: str) -> str:
    """
    è¯»å–å¹¶å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œé‡‡ç”¨é«˜çº§ Unstructured å¤„ç†å’ŒçœŸå®è¯­ä¹‰åˆ†å—ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - å¤„ç†å¤æ‚å¸ƒå±€çš„è®ºæ–‡æˆ–æ–‡ç« 
    - æ·»åŠ åŒ…å«è¡¨æ ¼å’Œåˆ—è¡¨çš„æŠ¥å‘Šæˆ–æ‰‹å†Œå†…å®¹
    - å¯¼å…¥å¸¦æ ¼å¼çš„ç”µå­è¡¨æ ¼æ•°æ®
    - å°†æ¼”ç¤ºæ–‡ç¨¿è½¬æ¢ä¸ºå¯æœç´¢çŸ¥è¯†
    - å¤„ç†å¸¦ OCR çš„æ‰«ææ–‡æ¡£

    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼šPDFã€DOCXã€PPTXã€XLSXã€TXTã€HTMLã€CSVã€JSONã€XMLã€ODTã€ODPã€ODSã€RTFã€
    å›¾åƒï¼ˆPNGã€JPGã€TIFFã€BMPï¼Œå¸¦ OCRï¼‰ã€é‚®ä»¶ï¼ˆEMLã€MSGï¼‰ç­‰ 25+ ç§æ ¼å¼ã€‚

    é«˜çº§åŠŸèƒ½ï¼š
    - åŸºäºæ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€ç« èŠ‚ã€åˆ—è¡¨ï¼‰çš„çœŸå®è¯­ä¹‰åˆ†å—
    - æ™ºèƒ½æ–‡æ¡£ç»“æ„ä¿å­˜ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ï¼‰
    - è‡ªåŠ¨å»å™ªï¼ˆé¡µçœ‰ã€é¡µè„šã€æ— å…³å†…å®¹ï¼‰
    - ç»“æ„åŒ–å…ƒæ•°æ®æå–
    - å¼ºå¤§å›é€€ç³»ç»Ÿï¼Œé€‚ç”¨äºä»»ä½•æ–‡æ¡£ç±»å‹
    - é€šè¿‡è¯­ä¹‰è¾¹ç•Œå¢å¼ºä¸Šä¸‹æ–‡ä¿å­˜

    æ–‡æ¡£å°†é€šè¿‡çœŸå®è¯­ä¹‰åˆ†å—æ™ºèƒ½å¤„ç†ï¼Œå¹¶ä¸å¢å¼ºå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨ã€‚
    å¤„ç†åçš„å‰¯æœ¬å°†ä¿å­˜ä»¥ä¾›éªŒè¯ã€‚

    å‚æ•°ï¼š
        file_pathï¼šè¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶çš„ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: å¼€å§‹é«˜çº§æ–‡æ¡£å¤„ç†: {file_path}")
    log(f"MCPæœåŠ¡å™¨: è°ƒè¯• - æ¥æ”¶åˆ°è·¯å¾„: {repr(file_path)}")
    log(f"MCPæœåŠ¡å™¨: è°ƒè¯• - æ£€æŸ¥ç»å¯¹è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.abspath(file_path)}")
    initialize_rag()  # ç¡®ä¿ RAG ç³»ç»Ÿå·²å°±ç»ª
    
    try:
        if not os.path.exists(file_path):
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°æ–‡ä»¶è·¯å¾„: {file_path}")
            return f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ '{file_path}'"

        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ç”¨é«˜çº§Unstructuredç³»ç»Ÿå¤„ç†æ–‡æ¡£...")

        # ä½¿ç”¨æ–°ç³»ç»Ÿå¤„ç†ç»“æ„åŒ–å…ƒç´ 
        processed_content, metadata, structural_elements = load_document_with_elements(file_path)

        if not processed_content or processed_content.isspace():
            log(f"MCPæœåŠ¡å™¨: è­¦å‘Š: æ–‡æ¡£å·²å¤„ç†ä½†æœªèƒ½æå–å†…å®¹: {file_path}")
            return f"è­¦å‘Š: æ–‡æ¡£ '{file_path}' å·²å¤„ç†ï¼Œä½†æœªèƒ½æå–æ–‡æœ¬å†…å®¹ã€‚"

        log(f"MCPæœåŠ¡å™¨: æ–‡æ¡£å¤„ç†æˆåŠŸï¼ˆ{len(processed_content)} å­—ç¬¦ï¼‰")

        # ä¿å­˜å¤„ç†å‰¯æœ¬
        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä¿å­˜å¤„ç†å‰¯æœ¬...")
        processing_method = metadata.get("processing_method", "æœªçŸ¥")
        saved_copy_path = save_processed_copy(file_path, processed_content, processing_method)

        # æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“ï¼Œä½¿ç”¨çœŸå®è¯­ä¹‰åˆ†å—
        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“ï¼ˆå«ç»“æ„åŒ–å…ƒæ•°æ®ï¼‰...")

        # ä½¿ç”¨ç»“æ„åŒ–å…ƒç´ è¿›è¡ŒçœŸå®è¯­ä¹‰åˆ†å—
        add_text_to_knowledge_base_enhanced(
            processed_content,
            rag_state["vector_store"],
            metadata,
            use_semantic_chunking=True,
            structural_elements=structural_elements
        )

        log(f"MCPæœåŠ¡å™¨: å¤„ç†å®Œæˆ - æ–‡æ¡£å·²æˆåŠŸå¤„ç†")

        # æ„å»ºè¯¦ç»†å“åº”
        file_name = os.path.basename(file_path)
        file_type = metadata.get("file_type", "æœªçŸ¥")
        processing_method = metadata.get("processing_method", "æœªçŸ¥")

        # åˆ†å—ä¿¡æ¯
        chunking_info = ""
        if structural_elements and len(structural_elements) > 1:
            chunking_info = f"ğŸ§  é«˜çº§è¯­ä¹‰åˆ†å—ï¼Œå…± {len(structural_elements)} ä¸ªç»“æ„åŒ–å…ƒç´ "
        elif metadata.get("structural_info", {}).get("total_elements", 0) > 1:
            chunking_info = f"ğŸ“Š å¢å¼ºè¯­ä¹‰åˆ†å—ï¼ŒåŸºäºç»“æ„åŒ–å…ƒæ•°æ®"
        else:
            chunking_info = f"ğŸ“ ä¼ ç»Ÿåˆ†å— ä¼˜åŒ–"

        return f"""âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ
ğŸ“„ æ–‡ä»¶: {file_name}
ğŸ“‹ ç±»å‹: {file_type.upper()}
ğŸ”§ å¤„ç†æ–¹æ³•: {processing_method}
{chunking_info}
ğŸ“Š å¤„ç†å­—ç¬¦æ•°: {len(processed_content):,}
ğŸ’¾ å‰¯æœ¬ä¿å­˜è·¯å¾„: {saved_copy_path if saved_copy_path else "æ— "}"""

    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†æ–‡æ¡£ '{file_path}' æ—¶å‡ºé”™: {e}")
        return f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}"


@mcp.tool()
def ask_rag(query: str) -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æé—®ï¼Œå¹¶æ ¹æ®å­˜å‚¨çš„ä¿¡æ¯è¿”å›ç­”æ¡ˆã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - è¯¢é—®ç‰¹å®šä¸»é¢˜æˆ–æ¦‚å¿µ
    - è¯·æ±‚è§£é‡Šæˆ–å®šä¹‰
    - ä»å¤„ç†è¿‡çš„æ–‡æ¡£è·å–ä¿¡æ¯
    - åŸºäºå·²å­¦ä¹ çš„æ–‡æœ¬æˆ–æ–‡æ¡£è·å–ç­”æ¡ˆ

    ç³»ç»Ÿå°†æœç´¢æ‰€æœ‰å­˜å‚¨çš„çŸ¥è¯†ï¼Œå¹¶è¿”å›æœ€ç›¸å…³çš„ç»“æœã€‚

    å‚æ•°ï¼š
        query: è¦å‘çŸ¥è¯†åº“æå‡ºçš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†é—®é¢˜: {query}")
    initialize_rag()
    
    try:
        # ä½¿ç”¨æ ‡å‡†QAé“¾ï¼ˆæ— è¿‡æ»¤å™¨ï¼‰
        qa_chain = get_qa_chain(rag_state["vector_store"])
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰ç›¸å…³ä¿¡æ¯
        if not source_documents:
            # æ— å¯ç”¨æ¥æºï¼Œå¯èƒ½ç³»ç»Ÿå°šæœªåŠ è½½ç›¸å…³ä¿¡æ¯
            enhanced_answer = f"ğŸ¤– å›ç­”:\n\nâŒ æœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œæ— æ³•å›ç­”ã€‚\n\n"
            enhanced_answer += "ğŸ’¡ å»ºè®®:\n"
            enhanced_answer += "â€¢ ç¡®ä¿å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æé—®\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€\n"
            enhanced_answer += "â€¢ è€ƒè™‘åŠ è½½æ›´å¤šç›¸å…³æ–‡æ¡£\n\n"
            enhanced_answer += "âš ï¸ æç¤º: ç³»ç»Ÿä»…åŸºäºå·²åŠ è½½ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥æº")
            return enhanced_answer
        
        # æ£€æŸ¥å›ç­”æ˜¯å¦å¯èƒ½æ˜¯å¹»è§‰
        # å¦‚æœæ²¡æœ‰æ¥æºä½†æœ‰å›ç­”ï¼Œå¯èƒ½æ˜¯å¹»è§‰ç»“æœ
        if len(source_documents) == 0 and answer.strip():
            # æœ‰å›ç­”ä½†æ— æ¥æºï¼Œå¯èƒ½ä¸ºå¹»è§‰ç»“æœ
            enhanced_answer = f"ğŸ¤– å›ç­”:\n\nâŒ æœªæ‰¾åˆ°ç‰¹å®šä¿¡æ¯ï¼Œæ— æ³•å‡†ç¡®å›ç­”ã€‚\n\n"
            enhanced_answer += "ğŸ’¡ å»ºè®®:\n"
            enhanced_answer += "â€¢ ç¡®ä¿å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æé—®\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€\n\n"
            enhanced_answer += "âš ï¸ æç¤º: ç³»ç»Ÿä»…åŸºäºå·²åŠ è½½ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰å›ç­”ï¼ˆæ— æ¥æºï¼‰")
            return enhanced_answer
        
        # å¦‚æœæœ‰å¯ç”¨æ¥æºï¼Œæ„å»ºæ­£å¸¸å›ç­”
        enhanced_answer = f"ğŸ¤– å›ç­”:\n\n{answer}\n"
        
        # æ·»åŠ æ¥æºä¿¡æ¯åŠæ›´å¤šè¯¦æƒ…
        if source_documents:
            enhanced_answer += "ğŸ“š ä½¿ç”¨çš„ä¿¡æ¯æ¥æº:\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "æœªçŸ¥æ¥æº")
                
                # --- æ”¹è¿›æ¥æºä¿¡æ¯ ---
                source_info = f"   {i}. {source_name}"
                
                # å¦‚æœæ˜¯æ–‡æ¡£åˆ™æ·»åŠ å®Œæ•´è·¯å¾„
                file_path = metadata.get("file_path")
                if file_path:
                    source_info += f"\n      - è·¯å¾„: `{file_path}`"
                
                # å¦‚æœæœ‰æ–‡ä»¶ç±»å‹åˆ™æ·»åŠ 
                file_type = metadata.get("file_type")
                if file_type:
                    source_info += f"\n      - ç±»å‹: {file_type.upper()}"
                
                # å¦‚æœæœ‰å¤„ç†æ–¹æ³•åˆ™æ·»åŠ 
                processing_method = metadata.get("processing_method")
                if processing_method:
                    method_display = processing_method.replace('_', ' ').title()
                    source_info += f"\n      - å¤„ç†æ–¹æ³•: {method_display}"
                
                # å¦‚æœæœ‰ç»“æ„ä¿¡æ¯åˆ™æ·»åŠ 
                structural_info = metadata.get("structural_info")
                if structural_info:
                    source_info += f"\n      - ç»“æ„: {structural_info.get('total_elements', 'N/A')} ä¸ªå…ƒç´ "
                    titles_count = structural_info.get('titles_count', 0)
                    tables_count = structural_info.get('tables_count', 0)
                    lists_count = structural_info.get('lists_count', 0)
                    if titles_count > 0 or tables_count > 0 or lists_count > 0:
                        structure_details = []
                        if titles_count > 0:
                            structure_details.append(f"{titles_count} ä¸ªæ ‡é¢˜")
                        if tables_count > 0:
                            structure_details.append(f"{tables_count} ä¸ªè¡¨æ ¼")
                        if lists_count > 0:
                            structure_details.append(f"{lists_count} ä¸ªåˆ—è¡¨")
                        source_info += f" ({', '.join(structure_details)})"
                
                # ä»æ‰å¹³å…ƒæ•°æ®é‡æ„ç»“æ„ä¿¡æ¯
                structural_elements = []
                titles_count = metadata.get("structural_titles_count", 0)
                tables_count = metadata.get("structural_tables_count", 0)
                lists_count = metadata.get("structural_lists_count", 0)
                total_elements = metadata.get("structural_total_elements", 0)
                
                if total_elements > 0:
                    structural_details = []
                    if titles_count > 0:
                        structural_details.append(f"{titles_count} ä¸ªæ ‡é¢˜")
                    if tables_count > 0:
                        structural_details.append(f"{tables_count} ä¸ªè¡¨æ ¼")
                    if lists_count > 0:
                        structural_details.append(f"{lists_count} ä¸ªåˆ—è¡¨")
                    
                    if structural_details:
                        source_info += f"\n      - ç»“æ„: {', '.join(structural_details)}"
                
                enhanced_answer += source_info + "\n\n"
        
        # æ·»åŠ å›ç­”è´¨é‡ä¿¡æ¯
        num_sources = len(source_documents)
        if num_sources >= 3:
            enhanced_answer += "\nâœ… é«˜å¯ä¿¡åº¦: å›ç­”åŸºäºå¤šä¸ªæ¥æº"
        elif num_sources == 2:
            enhanced_answer += "\nâš ï¸ ä¸­ç­‰å¯ä¿¡åº¦: å›ç­”åŸºäº2ä¸ªæ¥æº"
        else:
            enhanced_answer += "\nâš ï¸ æœ‰é™å¯ä¿¡åº¦: å›ç­”åŸºäº1ä¸ªæ¥æº"
        
        # å¦‚æœæœ‰æ–‡æ¡£ä½¿ç”¨äº†ç»“æ„åŒ–å…ƒæ•°æ®å¤„ç†åˆ™æ·»åŠ ä¿¡æ¯
        enhanced_docs = [doc for doc in source_documents if hasattr(doc, 'metadata') and doc.metadata.get("processing_method") == "unstructured_enhanced"]
        if enhanced_docs:
            enhanced_answer += f"\nğŸ§  æ™ºèƒ½å¤„ç†: {len(enhanced_docs)} ä¸ªæ¥æºä½¿ç”¨äº†Unstructuredå¤„ç†ï¼ˆä¿ç•™ç»“æ„ï¼‰"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸç”Ÿæˆå›ç­”ï¼Œå…±{len(source_documents)}ä¸ªæ¥æº")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}\n\nğŸ’¡ å»ºè®®:\n- æ£€æŸ¥RAGç³»ç»Ÿæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–\n- å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n- å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·é‡å¯æœåŠ¡å™¨"

@mcp.tool()
def ask_rag_filtered(query: str, file_type: str = None, min_tables: int = None, min_titles: int = None, processing_method: str = None) -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æå‡ºé—®é¢˜ï¼Œä½¿ç”¨ç‰¹å®šè¿‡æ»¤å™¨èšç„¦æœç´¢èŒƒå›´ã€‚
    é€‚ç”¨äºä»ç‰¹å®šç±»å‹çš„æ–‡æ¡£æˆ–å…·æœ‰ç‰¹å®šç‰¹å¾çš„æ–‡æ¡£ä¸­è·å–ä¿¡æ¯ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - ä»…åœ¨PDFæ–‡æ¡£ä¸­æœç´¢: file_type=".pdf"
    - æŸ¥æ‰¾åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£: min_tables=1
    - æŸ¥æ‰¾ç»“æ„è‰¯å¥½çš„æ–‡æ¡£: min_titles=5
    - åœ¨å¢å¼ºå¤„ç†çš„æ–‡æ¡£ä¸­æœç´¢: processing_method="unstructured_enhanced"
    
    é€šè¿‡è¿‡æ»¤æœç´¢èŒƒå›´æä¾›æ›´ç²¾å‡†å’Œç›¸å…³çš„ç»“æœã€‚

    å‚æ•°ï¼š
        query: å‘çŸ¥è¯†åº“æå‡ºçš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚
        file_type: æŒ‰æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼ˆä¾‹å¦‚ ".pdf", ".docx", ".txt"ï¼‰
        min_tables: æ–‡æ¡£å¿…é¡»åŒ…å«çš„æœ€å°‘è¡¨æ ¼æ•°é‡
        min_titles: æ–‡æ¡£å¿…é¡»åŒ…å«çš„æœ€å°‘æ ‡é¢˜æ•°é‡
        processing_method: æŒ‰å¤„ç†æ–¹æ³•è¿‡æ»¤ï¼ˆä¾‹å¦‚ "unstructured_enhanced", "markitdown"ï¼‰
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†å¸¦è¿‡æ»¤å™¨çš„é—®é¢˜: {query}")
    log(f"MCPæœåŠ¡å™¨: åº”ç”¨çš„è¿‡æ»¤å™¨ - ç±»å‹: {file_type}, è¡¨æ ¼: {min_tables}, æ ‡é¢˜: {min_titles}, æ–¹æ³•: {processing_method}")
    initialize_rag()
    
    try:
        # åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å™¨
        metadata_filter = create_metadata_filter(
            file_type=file_type,
            processing_method=processing_method,
            min_tables=min_tables,
            min_titles=min_titles
        )
        
        # ä½¿ç”¨å¸¦è¿‡æ»¤å™¨çš„QAé“¾
        qa_chain = get_qa_chain(rag_state["vector_store"], metadata_filter)
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦åˆè¿‡æ»¤å™¨çš„ç›¸å…³ä¿¡æ¯
        if not source_documents:
            # æ²¡æœ‰ç¬¦åˆè¿‡æ»¤å™¨çš„æ¥æº
            enhanced_answer = f"ğŸ” å›ç­”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:\n\nâŒ åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„ç›¸å…³ä¿¡æ¯ã€‚\n\n"
            
            # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
            if metadata_filter:
                enhanced_answer += "ğŸ“‹ åº”ç”¨çš„è¿‡æ»¤å™¨:\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   â€¢ æœ€å°‘è¡¨æ ¼æ•°: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   â€¢ æœ€å°‘æ ‡é¢˜æ•°: {value['$gte']}\n"
                enhanced_answer += "\n"
            
            enhanced_answer += "ğŸ’¡ å»ºè®®:\n"
            enhanced_answer += "â€¢ å°è¯•æ”¾å®½è¿‡æ»¤å™¨ä»¥è·å¾—æ›´å¤šç»“æœ\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£ç±»å‹\n"
            enhanced_answer += "â€¢ è€ƒè™‘ä½¿ç”¨ `ask_rag()` ä¸å¸¦è¿‡æ»¤å™¨æœç´¢æ•´ä¸ªçŸ¥è¯†åº“\n"
            enhanced_answer += "â€¢ ç¡®è®¤å·²åŠ è½½ç¬¦åˆæŒ‡å®šæ¡ä»¶çš„æ–‡æ¡£\n\n"
            enhanced_answer += "âš ï¸ æ³¨æ„: è¿‡æ»¤å™¨å¯èƒ½è¿‡äºä¸¥æ ¼ã€‚å°è¯•ä½¿ç”¨æ›´å®½æ¾çš„è¿‡æ»¤å™¨ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„æ¥æº")
            return enhanced_answer
        
        # éªŒè¯å“åº”æ˜¯å¦å¯èƒ½æ˜¯å¹»è§‰
        if len(source_documents) == 0 and answer.strip():
            enhanced_answer = f"ğŸ” å“åº”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:\n\nâŒ æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„ç‰¹å®šä¿¡æ¯ã€‚\n\n"
            
            # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
            if metadata_filter:
                enhanced_answer += "ğŸ“‹ åº”ç”¨çš„è¿‡æ»¤å™¨:\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   â€¢ MÃ­nimo de tablas: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   â€¢ MÃ­nimo de tÃ­tulos: {value['$gte']}\n"
                enhanced_answer += "\nğŸ’¡ å»ºè®®:\n"
            enhanced_answer += "â€¢ å°è¯•æ”¾å®½è¿‡æ»¤å™¨ä»¥è·å¾—æ›´å¤šç»“æœ\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£ç±»å‹\n"
            enhanced_answer += "â€¢ è€ƒè™‘ä½¿ç”¨ä¸å¸¦è¿‡æ»¤å™¨çš„ `ask_rag()` æœç´¢æ•´ä¸ªçŸ¥è¯†åº“\n\n"
            enhanced_answer += "âš ï¸ æ³¨æ„: è¿‡æ»¤å™¨å¯èƒ½è¿‡äºä¸¥æ ¼ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å®½æ³›çš„è¿‡æ»¤å™¨ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: è¿‡æ»¤å“åº”æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰ï¼ˆæ— æ¥æºï¼‰")
            return enhanced_answer
        
        # å¦‚æœæœ‰æ¥æºï¼Œæ„å»ºæ­£å¸¸å›ç­”
        enhanced_answer = f"ğŸ” å›ç­”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:\n\n{answer}\n"
        
        # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
        if metadata_filter:
            enhanced_answer += "\nğŸ“‹ åº”ç”¨çš„è¿‡æ»¤å™¨:\n"
            for key, value in metadata_filter.items():
                if key == "file_type":
                    enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                elif key == "processing_method":
                    enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                elif key == "structural_tables_count":
                    enhanced_answer += f"   â€¢ MÃ­nimo de tablas: {value['$gte']}\n"
                elif key == "structural_titles_count":
                    enhanced_answer += f"   â€¢ MÃ­nimo de tÃ­tulos: {value['$gte']}\n"
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        if source_documents:
            enhanced_answer += f"\nğŸ“š æ‰¾åˆ°çš„æ¥æº ({len(source_documents)}):\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "Fuente desconocida")
                
                source_info = f"   {i}. {source_name}"
                
                # InformaciÃ³n estructural
                tables_count = metadata.get("structural_tables_count", 0)
                titles_count = metadata.get("structural_titles_count", 0)
                file_type = metadata.get("file_type", "")
                
                structural_details = []
                if tables_count > 0:
                    structural_details.append(f"{tables_count} tablas")
                if titles_count > 0:
                    structural_details.append(f"{titles_count} tÃ­tulos")
                
                if structural_details:
                    source_info += f" ({', '.join(structural_details)})"
                
                if file_type:
                    source_info += f" [{file_type.upper()}]"
                
                enhanced_answer += source_info + "\n"
        
        # è¿‡æ»¤æœç´¢ä¿¡æ¯
        enhanced_answer += f"\nğŸ¯ è¿‡æ»¤æœç´¢: ç»“æœä»…é™äºç¬¦åˆæŒ‡å®šæ¡ä»¶çš„æ–‡æ¡£ã€‚"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸç”Ÿæˆè¿‡æ»¤å›ç­”ï¼Œå…±{len(source_documents)}ä¸ªæ¥æº")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†è¿‡æ»¤é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ å¤„ç†è¿‡æ»¤é—®é¢˜æ—¶å‡ºé”™: {e}"

@mcp.tool()
def get_knowledge_base_stats() -> str:
    """
    è·å–çŸ¥è¯†åº“çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£ç±»å‹ã€å¤„ç†æ–¹æ³•å’Œç»“æ„ä¿¡æ¯ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“ä¸­æœ‰å“ªäº›ä¿¡æ¯ä»¥åŠå¦‚ä½•å¤„ç†çš„ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - æ£€æŸ¥çŸ¥è¯†åº“ä¸­æœ‰å¤šå°‘æ–‡æ¡£
    - äº†è§£æ–‡ä»¶ç±»å‹çš„åˆ†å¸ƒ
    - æŸ¥çœ‹ä½¿ç”¨äº†å“ªäº›å¤„ç†æ–¹æ³•
    - åˆ†æå­˜å‚¨æ–‡æ¡£çš„ç»“æ„å¤æ‚æ€§
    
    è¿™æœ‰åŠ©äºæ‚¨å°±æœç´¢å†…å®¹å’Œå¦‚ä½•è¿‡æ»¤æŸ¥è¯¢åšå‡ºæ˜æ™ºçš„å†³å®šã€‚

    è¿”å›ï¼š
        å…³äºçŸ¥è¯†åº“å†…å®¹çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯...")
    initialize_rag()
    
    try:
        stats = get_document_statistics(rag_state["vector_store"])
        
        if "error" in stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {stats['error']}"
        
        if stats.get("total_documents", 0) == 0:
            return "ğŸ“Š çŸ¥è¯†åº“ä¸ºç©º\n\nçŸ¥è¯†åº“ä¸­æ²¡æœ‰å­˜å‚¨çš„æ–‡æ¡£ã€‚"
        
        # æ„å»ºè¯¦ç»†å›ç­”
        response = f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\n\n"
        response += f"ğŸ“š æ–‡æ¡£æ€»æ•°: {stats['total_documents']}\n\n"
        
        # æ–‡ä»¶ç±»å‹
        if stats["file_types"]:
            response += "ğŸ“„ æ–‡ä»¶ç±»å‹:\n"
            for file_type, count in sorted(stats["file_types"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                response += f"   â€¢ {file_type.upper()}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # å¤„ç†æ–¹æ³•
        if stats["processing_methods"]:
            response += "ğŸ”§ å¤„ç†æ–¹æ³•:\n"
            for method, count in sorted(stats["processing_methods"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                method_display = method.replace('_', ' ').title()
                response += f"   â€¢ {method_display}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # ç»“æ„ç»Ÿè®¡
        structural = stats["structural_stats"]
        response += "ğŸ—ï¸ ç»“æ„ä¿¡æ¯:\n"
        response += f"   â€¢ åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£: {structural['documents_with_tables']}\n"
        response += f"   â€¢ åŒ…å«æ ‡é¢˜çš„æ–‡æ¡£: {structural['documents_with_titles']}\n"
        response += f"   â€¢ åŒ…å«åˆ—è¡¨çš„æ–‡æ¡£: {structural['documents_with_lists']}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡è¡¨æ ¼æ•°: {structural['avg_tables_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡æ ‡é¢˜æ•°: {structural['avg_titles_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡åˆ—è¡¨æ•°: {structural['avg_lists_per_doc']:.1f}\n\n"
        
        # æœç´¢å»ºè®®
        response += "ğŸ’¡ æœç´¢å»ºè®®:\n"
        if structural['documents_with_tables'] > 0:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `min_tables=1` æœç´¢åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£ä¿¡æ¯\n"
        if structural['documents_with_titles'] > 5:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `min_titles=5` æœç´¢ç»“æ„è‰¯å¥½çš„æ–‡æ¡£\n"
        if ".pdf" in stats["file_types"]:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `file_type=\".pdf\"` ä»…æœç´¢PDFæ–‡æ¡£\n"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸè·å–ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}"

@mcp.tool()
def get_embedding_cache_stats() -> str:
    """
    è·å–åµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    ç”¨äºç›‘æ§ç¼“å­˜æ•ˆç‡å¹¶äº†è§£ç³»ç»Ÿæ€§èƒ½ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡ä»¥æŸ¥çœ‹ç³»ç»Ÿæ˜¯å¦é«˜æ•ˆè¿è¡Œ
    - ç›‘æ§ç¼“å­˜çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
    - äº†è§£åµŒå…¥é‡å¤ä½¿ç”¨çš„é¢‘ç‡
    - è°ƒè¯•æ€§èƒ½é—®é¢˜
    
    è¿™æœ‰åŠ©äºæ‚¨ä¼˜åŒ–ç³»ç»Ÿå¹¶äº†è§£å…¶è¡Œä¸ºã€‚

    è¿”å›ï¼š
        å…³äºåµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨è·å–åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_cache_stats()
        
        if not stats:
            return "ğŸ“Š åµŒå…¥ç¼“å­˜ä¸å¯ç”¨\nåµŒå…¥ç¼“å­˜æœªåˆå§‹åŒ–ã€‚"
        
        # æ„å»ºè¯¦ç»†å›ç­”
        response = f"ğŸ“Š åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\n\n"
        
        # ä¸»è¦æŒ‡æ ‡
        response += f"ğŸ”„ ç¼“å­˜æ´»åŠ¨:\n"
        response += f"   â€¢ æ€»è¯·æ±‚æ•°: {stats['total_requests']}\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­æ¬¡æ•°: {stats['memory_hits']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­æ¬¡æ•°: {stats['disk_hits']}\n"
        response += f"   â€¢ æœªå‘½ä¸­æ¬¡æ•°: {stats['misses']}\n\n"
        
        # æˆåŠŸç‡
        response += f"ğŸ“ˆ æˆåŠŸç‡:\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­ç‡: {stats['memory_hit_rate']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­ç‡: {stats['disk_hit_rate']}\n"
        response += f"   â€¢ æ€»å‘½ä¸­ç‡: {stats['overall_hit_rate']}\n\n"
        
        # å†…å­˜ä½¿ç”¨
        response += f"ğŸ’¾ å†…å­˜ä½¿ç”¨:\n"
        response += f"   â€¢ å†…å­˜ä¸­çš„åµŒå…¥: {stats['memory_cache_size']}\n"
        response += f"   â€¢ æœ€å¤§å†…å­˜å¤§å°: {stats['max_memory_size']}\n"
        response += f"   â€¢ ç¼“å­˜ç›®å½•: {stats['cache_directory']}\n\n"
        
        # æ€§èƒ½åˆ†æ
        total_requests = stats['total_requests']
        if total_requests > 0:
            memory_hit_rate = float(stats['memory_hit_rate'].rstrip('%'))
            overall_hit_rate = float(stats['overall_hit_rate'].rstrip('%'))
            
            response += f"ğŸ¯ æ€§èƒ½åˆ†æ:\n"
            
            if overall_hit_rate > 70:
                response += f"   â€¢ âœ… æ€§èƒ½å“è¶Š: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            elif overall_hit_rate > 50:
                response += f"   â€¢ âš ï¸ æ€§èƒ½ä¸­ç­‰: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ âŒ æ€§èƒ½è¾ƒä½: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            
            if memory_hit_rate > 50:
                response += f"   â€¢ ğŸš€ å†…å­˜ç¼“å­˜é«˜æ•ˆ: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ ğŸ’¾ ä¾èµ–ç£ç›˜å­˜å‚¨: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            
            # ä¼˜åŒ–å»ºè®®
            response += f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:\n"
            if overall_hit_rate < 30:
                response += f"   â€¢ è€ƒè™‘åŒæ—¶å¤„ç†ç›¸ä¼¼çš„æ–‡æ¡£\n"
                response += f"   â€¢ æ£€æŸ¥æ˜¯å¦æœ‰å¾ˆå¤šä¸é‡å¤çš„ç‹¬ç‰¹æ–‡æœ¬\n"
            
            if memory_hit_rate < 30 and total_requests > 100:
                response += f"   â€¢ è€ƒè™‘å¢åŠ å†…å­˜ç¼“å­˜å¤§å°\n"
                response += f"   â€¢ ç£ç›˜å‘½ä¸­æ¯”å†…å­˜å‘½ä¸­é€Ÿåº¦æ…¢\n"
            
            if stats['memory_cache_size'] >= stats['max_memory_size'] * 0.9:
                response += f"   â€¢ å†…å­˜ç¼“å­˜æ¥è¿‘æ»¡è½½\n"
                response += f"   â€¢ å¦‚æœ‰å¯ç”¨RAMï¼Œè€ƒè™‘å¢åŠ max_memory_size\n"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸè·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}"

@mcp.tool()
def clear_embedding_cache_tool() -> str:
    """
    æ¸…é™¤åµŒå…¥ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚
    å½“æ‚¨æƒ³é‡ç½®ç¼“å­˜æˆ–é‡Šæ”¾èµ„æºæ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - ç³»ç»Ÿå†…å­˜ä¸è¶³æ—¶é‡Šæ”¾å†…å­˜
    - æ›´æ”¹åµŒå…¥æ¨¡å‹åé‡ç½®ç¼“å­˜
    - æ¸…é™¤ä¸å†éœ€è¦çš„æ—§ç¼“å­˜åµŒå…¥
    - æ•…éšœæ’é™¤ç¼“å­˜ç›¸å…³é—®é¢˜
    
    è­¦å‘Š: è¿™å°†åˆ é™¤æ‰€æœ‰ç¼“å­˜çš„åµŒå…¥ï¼Œéœ€è¦é‡æ–°è®¡ç®—ã€‚

    è¿”å›ï¼š
        å…³äºç¼“å­˜æ¸…é™¤æ“ä½œçš„ç¡®è®¤æ¶ˆæ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ¸…é™¤åµŒå…¥ç¼“å­˜...")
    
    try:
        clear_embedding_cache()
        
        response = "ğŸ§¹ åµŒå…¥ç¼“å­˜æ¸…é™¤æˆåŠŸ\n\n"
        response += "âœ… å·²åˆ é™¤æ‰€æœ‰ç¼“å­˜ä¸­å­˜å‚¨çš„åµŒå…¥ã€‚\n"
        response += "ğŸ“ ä¸‹æ¬¡éœ€è¦æ—¶å°†ä»å¤´è®¡ç®—åµŒå…¥ã€‚\n"
        response += "ğŸ’¾ å·²é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚\n\n"
        response += "âš ï¸ æ³¨æ„: éœ€è¦æ—¶åµŒå…¥å°†è‡ªåŠ¨é‡æ–°è®¡ç®—ã€‚"
        
        log(f"MCPæœåŠ¡å™¨: åµŒå…¥ç¼“å­˜æ¸…é™¤æˆåŠŸ")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: æ¸…é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return f"âŒ æ¸…é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}"

@mcp.tool()
def optimize_vector_database() -> str:
    """
    ä¼˜åŒ–å‘é‡æ•°æ®åº“ä»¥æé«˜æœç´¢æ€§èƒ½ã€‚
    æ­¤å·¥å…·é‡æ–°ç»„ç»‡å†…éƒ¨ç´¢å¼•ä»¥å®ç°æ›´å¿«çš„æœç´¢ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·å½“ï¼š
    - æœç´¢é€Ÿåº¦ç¼“æ…¢
    - æ·»åŠ äº†è®¸å¤šæ–°æ–‡æ¡£
    - æƒ³è¦æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½
    
    è¿”å›ï¼š
        å…³äºä¼˜åŒ–è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log("MCPæœåŠ¡å™¨: æ­£åœ¨ä¼˜åŒ–å‘é‡æ•°æ®åº“...")
    
    try:
        result = optimize_vector_store()

        if result.get("status") == "success":
            response = f"âœ… å‘é‡æ•°æ®åº“ä¼˜åŒ–æˆåŠŸ\n\n"
            response += f"ğŸ“Š ä¼˜åŒ–å‰ç»Ÿè®¡:\n"
            stats_before = result.get("stats_before", {})
            response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats_before.get('total_documents', 'N/A')}\n"

            response += f"\nğŸ“Š ä¼˜åŒ–åç»Ÿè®¡:\n"
            stats_after = result.get("stats_after", {})
            response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats_after.get('total_documents', 'N/A')}\n"

            response += f"\nğŸš€ ä¼˜åŠ¿:\n"
            response += f"   â€¢ æœç´¢é€Ÿåº¦æ›´å¿«\n"
            response += f"   â€¢ ç»“æœç²¾åº¦æ›´é«˜\n"
            response += f"   â€¢ ç´¢å¼•å·²ä¼˜åŒ–\n"
        else:
            msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
            response = f"âŒ ä¼˜åŒ–æ•°æ®åº“æ—¶å‡ºé”™: {msg}"

        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: ä¼˜åŒ–å‡ºé”™: {e}")
        return f"âŒ ä¼˜åŒ–å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}"

@mcp.tool()
def get_vector_database_stats() -> str:
    """
    è·å–å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    åŒ…æ‹¬æ–‡æ¡£ã€æ–‡ä»¶ç±»å‹å’Œé…ç½®ä¿¡æ¯ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·æ¥ï¼š
    - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    - åˆ†ææ–‡æ¡£åˆ†å¸ƒ
    - è¯Šæ–­æ€§èƒ½é—®é¢˜
    - è§„åˆ’ä¼˜åŒ–
    
    è¿”å›ï¼š
        å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    """
    log("MCPæœåŠ¡å™¨: æ­£åœ¨è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_vector_store_stats()
        
        if "error" in stats:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {stats['error']}"
        
        response = f"ğŸ“Š å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯\n\n"
        
        response += f"ğŸ“š åŸºæœ¬ä¿¡æ¯:\n"
        response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats.get('total_documents', 0)}\n"
        response += f"   â€¢ é›†åˆåç§°: {stats.get('collection_name', 'N/A')}\n"
        response += f"   â€¢ åµŒå…¥ç»´åº¦: {stats.get('embedding_dimension', 'N/A')}\n"
        
        # æ–‡ä»¶ç±»å‹
        file_types = stats.get('file_types', {})
        if file_types:
            response += f"\nğŸ“„ æŒ‰æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:\n"
            for file_type, count in file_types.items():
                response += f"   â€¢ {file_type}: {count} ä¸ªæ–‡æ¡£\n"
        
        # å¤„ç†æ–¹æ³•
        processing_methods = stats.get('processing_methods', {})
        if processing_methods:
            response += f"\nğŸ”§ å¤„ç†æ–¹æ³•:\n"
            for method, count in processing_methods.items():
                response += f"   â€¢ {method}: {count} ä¸ªæ–‡æ¡£\n"
        
        # æ¨èé…ç½®æ–‡ä»¶
        try:
            recommended_profile = get_optimal_vector_store_profile()
            response += f"\nğŸ¯ æ¨èé…ç½®: {recommended_profile}\n"
        except:
            pass
        
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: è·å–ç»Ÿè®¡ä¿¡æ¯å‡ºé”™: {e}")
        return f"âŒ è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}"

@mcp.tool()
def reindex_vector_database(profile: str = 'auto') -> str:
    """
    ä½¿ç”¨ä¼˜åŒ–é…ç½®é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“ã€‚
    æ­¤å·¥å…·ç”¨å½“å‰å¤§å°çš„ä¼˜åŒ–å‚æ•°é‡æ–°åˆ›å»ºç´¢å¼•ã€‚
    
    å‚æ•°:
        profile: é…ç½®æ–‡ä»¶ ('small', 'medium', 'large', 'auto')
                 'auto' è‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®
    
    ä½¿ç”¨æ­¤å·¥å…·å½“ï¼š
    - æ›´æ”¹é…ç½®æ–‡ä»¶
    - æœç´¢éå¸¸ç¼“æ…¢
    - æƒ³è¦ä¸ºç‰¹å®šæ•°æ®åº“å¤§å°ä¼˜åŒ–
    - å­˜åœ¨æŒç»­çš„æ€§èƒ½é—®é¢˜
    
    âš ï¸ æ³¨æ„: æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦æ—¶é—´ï¼Œå–å†³äºæ•°æ®åº“å¤§å°ã€‚
    
    è¿”å›ï¼š
        å…³äºé‡æ–°ç´¢å¼•è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä½¿ç”¨é…ç½®'{profile}'é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“...")
    
    try:
        result = reindex_vector_store(profile=profile)
        
        if result["status"] == "success":
            response = f"âœ… å‘é‡æ•°æ®åº“é‡æ–°ç´¢å¼•æˆåŠŸ\n\n"
            response += f"ğŸ“Š å¤„ç†ä¿¡æ¯:\n"
            response += f"   â€¢ åº”ç”¨çš„é…ç½®: {profile}\n"
            response += f"   â€¢ å¤„ç†çš„æ–‡æ¡£: {result.get('documents_processed', 0)}\n"
            
            response += f"\nğŸš€ é‡æ–°ç´¢å¼•çš„å¥½å¤„:\n"
            response += f"   â€¢ é’ˆå¯¹å½“å‰å¤§å°ä¼˜åŒ–çš„ç´¢å¼•\n"
            response += f"   â€¢ æ›´å¿«æ›´ç²¾ç¡®çš„æœç´¢\n"
            response += f"   â€¢ æ›´å¥½çš„å†…å­˜ä½¿ç”¨\n"
            
        elif result["status"] == "warning":
            response = f"âš ï¸ è­¦å‘Š: {result.get('message', 'æ²¡æœ‰æ–‡æ¡£éœ€è¦é‡æ–°ç´¢å¼•')}"
            
        else:
            response = f"âŒ é‡æ–°ç´¢å¼•æ•°æ®åº“æ—¶å‡ºé”™: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: é‡æ–°ç´¢å¼•å‡ºé”™: {e}")
        return f"âŒ é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}"

# --- è¿è¡ŒæœåŠ¡å™¨çš„å…¥å£ç‚¹ ---
if __name__ == "__main__":
    log("æ­£åœ¨å¯åŠ¨MCP RAGæœåŠ¡å™¨...")
    warm_up_rag_system()  # å¯åŠ¨æ—¶é¢„çƒ­ç³»ç»Ÿ
    mcp.run(transport='stdio') 