# Servidor RAG Personal con MCP

Este proyecto implementa un servidor compatible con el Protocolo de Contexto de Modelo (MCP) que dota a los clientes de IA (como Cursor, Claude for Desktop, etc.) de una capacidad de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG). Permite al modelo de lenguaje acceder a una base de conocimiento privada y local, alimentada por tus propios textos y documentos.

## âœ¨ CaracterÃ­sticas

- **Memoria Persistente para tu IA:** "EnseÃ±a" a tu IA nueva informaciÃ³n que recordarÃ¡ entre sesiones.
- **Procesamiento de Documentos:** Alimenta la base de conocimiento con archivos `.pdf`, `.docx`, `.pptx`, `.txt`, y mÃ¡s, gracias a la integraciÃ³n con [Microsoft MarkItDown](https://github.com/microsoft/markitdown).
- **LLM Local y Privado:** Utiliza modelos de lenguaje locales a travÃ©s de [Ollama](https://ollama.com/) (ej. Llama 3, Mistral), asegurando que tus datos y preguntas nunca salgan de tu mÃ¡quina.
- **100% Local y Offline:** Tanto el modelo de lenguaje como los embeddings se ejecutan en tu mÃ¡quina. NingÃºn dato sale a internet. Una vez descargados los modelos, funciona sin conexiÃ³n.
- **Ingesta Masiva:** Un script dedicado para procesar directorios enteros de documentos y construir la base de conocimiento de manera eficiente.
- **Arquitectura Modular:** La lÃ³gica del RAG estÃ¡ separada de los scripts de servidor y de ingesta, facilitando el mantenimiento y la expansiÃ³n.
- **Copias en Markdown:** Cada documento procesado se guarda automÃ¡ticamente en formato Markdown para verificaciÃ³n y reutilizaciÃ³n.
- **ğŸ†• Metadatos de Fuente:** Rastreabilidad completa de informaciÃ³n con atribuciÃ³n de fuentes en cada respuesta.
- **ğŸ†• Optimizado para Agentes de IA:** Descripciones detalladas y manejo de errores inteligente para uso efectivo por agentes de IA.

---

## ğŸ—ï¸ Arquitectura

El proyecto estÃ¡ dividido en tres componentes principales:

1.  `rag_core.py`: El corazÃ³n del sistema. Contiene toda la lÃ³gica reutilizable para manejar la base de datos vectorial (ChromaDB), procesar texto y crear la cadena de preguntas y respuestas con LangChain. **Incluye soporte para metadatos de fuente.**
2.  `rag_server.py`: El servidor MCP. Expone las herramientas (`learn_text`, `learn_document`, `ask_rag`) que el cliente de IA puede invocar. Se comunica a travÃ©s de `stdio`. **Optimizado con descripciones detalladas para agentes de IA.**
3.  `bulk_ingest.py`: Un script de lÃ­nea de comandos para procesar una carpeta llena de documentos y aÃ±adirlos a la base de conocimiento de forma masiva. **Incluye metadatos de fuente automÃ¡ticos.**

### Archivos de DocumentaciÃ³n:
- `AGENT_INSTRUCTIONS.md`: GuÃ­a completa para agentes de IA sobre cÃ³mo usar el sistema
- `test_rag.py`: Script de prueba para verificar el funcionamiento del sistema

---

## ğŸš€ GuÃ­a de InstalaciÃ³n y ConfiguraciÃ³n

Sigue estos pasos para poner en marcha el sistema.

### Prerrequisitos

- **Python 3.10+**
- **Ollama:** AsegÃºrate de que [Ollama estÃ© instalado](https://ollama.com/) y en ejecuciÃ³n en tu sistema.

### 0. ConfiguraciÃ³n de Ollama (Paso CrÃ­tico)

Ollama es necesario para que el sistema RAG funcione, ya que proporciona el modelo de lenguaje local que genera las respuestas.

#### InstalaciÃ³n de Ollama

**Windows:**
1. Descarga Ollama desde [ollama.com](https://ollama.com/)
2. Ejecuta el instalador y sigue las instrucciones
3. Ollama se ejecutarÃ¡ automÃ¡ticamente como servicio

**macOS/Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Verificar InstalaciÃ³n

```bash
# Verificar que Ollama estÃ¡ funcionando
ollama --version

# Verificar que el servicio estÃ¡ ejecutÃ¡ndose
ollama list
```

#### Descargar Modelos de Lenguaje

El sistema RAG necesita un modelo de lenguaje para generar respuestas. Recomendamos:

```bash
# Modelo recomendado (equilibrio entre velocidad y calidad)
ollama pull llama3

# Alternativas mÃ¡s rÃ¡pidas
ollama pull phi3
ollama pull mistral

# Alternativa mÃ¡s potente (requiere mÃ¡s recursos)
ollama pull llama3.1:8b
```

#### Configurar el Modelo en el Sistema

Una vez descargado el modelo, asegÃºrate de que `rag_core.py` use el modelo correcto:

```python
# En rag_core.py, lÃ­nea ~100, verifica que use tu modelo:
llm = ChatOllama(model="llama3", temperature=0)
```

**Nota:** Si descargaste un modelo diferente, cambia `"llama3"` por el nombre de tu modelo.

#### Probar Ollama

```bash
# Probar que el modelo funciona
ollama run llama3 "Hola, Â¿cÃ³mo estÃ¡s?"
```

Si ves una respuesta generada, Ollama estÃ¡ funcionando correctamente.

#### SoluciÃ³n de Problemas Comunes

**Error: "Ollama is not running"**
```bash
# Iniciar Ollama manualmente
ollama serve
```

**Error: "Model not found"**
```bash
# Verificar modelos disponibles
ollama list

# Descargar el modelo si no estÃ¡
ollama pull llama3
```

**Error: "Out of memory"**
- Usa un modelo mÃ¡s pequeÃ±o: `ollama pull phi3`
- Cierra otras aplicaciones que consuman mucha RAM
- Considera aumentar la memoria virtual en Windows

### 1. ConfiguraciÃ³n del Entorno

```bash
# 1. Clona este repositorio (si estuviera en GitHub) o usa los archivos existentes.
# cd RAG_MCP_Project

# 2. Crea un entorno virtual de Python
python -m venv .venv

# 3. Activa el entorno virtual
# En Windows:
.venv\\Scripts\\activate
# En macOS/Linux:
# source .venv/bin/activate
```

### 2. InstalaciÃ³n de Dependencias

Una vez que el entorno virtual estÃ© activado, instala todas las librerÃ­as necesarias.

#### OpciÃ³n A: InstalaciÃ³n Completa (Recomendada)
```bash
pip install -r requirements.txt
```

#### OpciÃ³n B: InstalaciÃ³n MÃ­nima (Para pruebas rÃ¡pidas)
```bash
pip install -r requirements-minimal.txt
```

#### OpciÃ³n C: InstalaciÃ³n de Desarrollo (Para contribuir al proyecto)
```bash
pip install -r requirements-dev.txt
```

**Nota:** La instalaciÃ³n completa incluye todas las dependencias necesarias. La instalaciÃ³n mÃ­nima omite algunas utilidades opcionales pero mantiene la funcionalidad core. La instalaciÃ³n de desarrollo incluye herramientas de testing y desarrollo.

### 2. VerificaciÃ³n Completa del Sistema

Antes de continuar, vamos a verificar que todo estÃ© funcionando correctamente:

#### Paso 1: Verificar Ollama
```bash
# Verificar que Ollama estÃ¡ ejecutÃ¡ndose
ollama list

# Probar el modelo
ollama run llama3 "Test de funcionamiento"
```

#### Paso 2: Verificar Dependencias de Python
```bash
# Verificar que todas las dependencias estÃ¡n instaladas
python -c "import mcp; print('âœ… MCP instalado correctamente')"
python -c "import langchain; print('âœ… LangChain instalado correctamente')"
python -c "import chromadb; print('âœ… ChromaDB instalado correctamente')"
```

#### Paso 3: Probar el Sistema RAG
```bash
# Ejecutar el script de prueba
python test_rag.py
```

Si todo funciona correctamente, verÃ¡s:
- âœ… Ollama respondiendo a comandos
- âœ… Todas las dependencias importÃ¡ndose sin errores
- âœ… El sistema RAG procesando preguntas y mostrando fuentes

### 3. Descarga del Modelo Local

**Nota:** Si ya descargaste el modelo en el paso 0, puedes saltar esta secciÃ³n.

Abre una terminal y descarga el modelo de lenguaje que usarÃ¡ Ollama para generar las respuestas.

```bash
# Modelo recomendado para el sistema RAG
ollama pull llama3
```

**Alternativas de modelos:**

| Modelo | TamaÃ±o | Velocidad | Calidad | Uso Recomendado |
|--------|--------|-----------|---------|-----------------|
| `llama3` | ~4GB | Media | Alta | âœ… **Recomendado** |
| `phi3` | ~2GB | RÃ¡pida | Buena | Para recursos limitados |
| `mistral` | ~4GB | Media | Alta | Alternativa a llama3 |
| `llama3.1:8b` | ~5GB | Lenta | Muy alta | Para mÃ¡xima calidad |

**Nota:** La primera vez que ejecutes el servidor o el script de ingesta, el modelo de *embedding* (`all-MiniLM-L6-v2`, unos 90MB) se descargarÃ¡ automÃ¡ticamente. Esto solo ocurre una vez.

**Verificar descarga:**
```bash
# Verificar que el modelo estÃ¡ disponible
ollama list

# Probar el modelo
ollama run llama3 "Hola, Â¿puedes ayudarme con el sistema RAG?"
```

### 4. Configurar el Modelo en el CÃ³digo

Si descargaste un modelo diferente a `llama3`, necesitas actualizar la configuraciÃ³n:

#### OpciÃ³n 1: Descargar el Modelo de Embedding (Recomendado la primera vez)
Para evitar esperas la primera vez que se usa el servidor, puedes pre-descargar el modelo de embedding con este comando. VerÃ¡s una barra de progreso:
```bash
python pre_download_model.py
```

#### OpciÃ³n 2: Cambiar en rag_core.py
```python
# Abrir rag_core.py y buscar la lÃ­nea ~100
# Cambiar esta lÃ­nea:
llm = ChatOllama(model="llama3", temperature=0)

# Por tu modelo, por ejemplo:
llm = ChatOllama(model="phi3", temperature=0)
```

#### OpciÃ³n 3: Usar Variable de Entorno (Recomendado)
Crea un archivo `.env` en la raÃ­z del proyecto:

```bash
# Crear archivo .env
echo "OLLAMA_MODEL=llama3" > .env
```

Y modifica `rag_core.py` para usar la variable de entorno:

```python
import os
from dotenv import load_dotenv

load_dotenv()
model_name = os.getenv("OLLAMA_MODEL", "llama3")  # Por defecto llama3
llm = ChatOllama(model=model_name, temperature=0)
```

**Ventajas de usar variable de entorno:**
- FÃ¡cil cambio de modelo sin modificar cÃ³digo
- ConfiguraciÃ³n especÃ­fica por entorno
- No se modifica el cÃ³digo fuente

---

## âœ… Resumen de ConfiguraciÃ³n

Para verificar que todo estÃ¡ listo, ejecuta esta secuencia de comandos:

```bash
# 1. Verificar Ollama
ollama list
ollama run llama3 "Test"

# 2. Verificar dependencias
python -c "import mcp, langchain, chromadb; print('âœ… Todas las dependencias OK')"

# 3. Probar el sistema completo
python test_rag.py
```

**Si todo funciona correctamente, verÃ¡s:**
- âœ… Lista de modelos de Ollama
- âœ… Respuesta del modelo de prueba
- âœ… Todas las dependencias importÃ¡ndose
- âœ… Sistema RAG procesando preguntas con fuentes

**Â¡Tu sistema RAG estÃ¡ listo para usar!** ğŸš€

---

## ğŸ› ï¸ GuÃ­a de Uso

### Uso 1: Poblar la Base de Conocimiento (Ingesta Masiva)

Para aÃ±adir una gran cantidad de documentos de una sola vez, usa el script `bulk_ingest.py`.

1.  Crea una carpeta en tu ordenador (ej. `C:\MisDocumentos`).
2.  Copia todos los documentos que quieres que la IA aprenda en esa carpeta.
3.  Ejecuta el siguiente comando en la terminal (con el entorno virtual activado):

```bash
python bulk_ingest.py --directory "C:\MisDocumentos"
```

El script recorrerÃ¡ todos los archivos soportados, los convertirÃ¡ y los aÃ±adirÃ¡ a la base de datos vectorial en la carpeta `./rag_mcp_db`.

### Uso 2: ConfiguraciÃ³n del Cliente MCP (Ej. Cursor)

Para que tu editor de IA pueda usar el servidor, debes configurarlo.

1.  **Encuentra el archivo de configuraciÃ³n de servidores MCP de tu editor.** Para Cursor, busca un archivo como `mcp_servers.json` en su directorio de configuraciÃ³n (`%APPDATA%\cursor` en Windows). Si no existe, puedes crearlo.

2.  **AÃ±ade la siguiente configuraciÃ³n al archivo JSON.**
    
    Este mÃ©todo utiliza un script de arranque (`run_server.bat`) para asegurar que la codificaciÃ³n de caracteres sea UTF-8, previniendo errores en Windows.

    **Â¡IMPORTANTE!** Debes reemplazar `"D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"` con la ruta absoluta real a la carpeta de este proyecto en tu mÃ¡quina.

    ```json
    {
      "mcpServers": {
        "rag_server_knowledge": {
          "command": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG\\run_server.bat",
          "args": [],
          "workingDirectory": "D:\\ruta\\completa\\a\\tu\\proyecto\\MCP_RAG"
        }
      }
    }
    ```

3.  **Reinicia tu editor.** Al arrancar, deberÃ­a detectar y lanzar tu `run_server.bat`, que a su vez ejecutarÃ¡ `rag_server.py` en segundo plano con el entorno correcto.

### Uso 3: Interactuando con las Herramientas

Una vez configurado, puedes usar las herramientas directamente en el chat de tu editor.

#### Herramientas Disponibles:

**1. `learn_text(text, source_name)` - AÃ±adir informaciÃ³n textual**
```
@rag_server_knowledge learn_text("El punto de fusiÃ³n del titanio es 1,668 Â°C.", "material_properties")
```
- **CuÃ¡ndo usar**: Para aÃ±adir hechos, definiciones, notas de conversaciÃ³n, etc.
- **ParÃ¡metros**: 
  - `text`: El contenido a almacenar
  - `source_name`: Nombre descriptivo de la fuente (opcional, por defecto "manual_input")

**2. `learn_document(file_path)` - Procesar documentos**
```
@rag_server_knowledge learn_document("C:\\Reportes\\informe_q3.pdf")
```
- **CuÃ¡ndo usar**: Para procesar archivos PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML
- **CaracterÃ­sticas**: 
  - ConversiÃ³n automÃ¡tica a Markdown
  - Copia guardada en `./converted_docs/`
  - Metadatos de fuente automÃ¡ticos

**3. `ask_rag(query)` - Consultar informaciÃ³n**
```
@rag_server_knowledge ask_rag("Â¿CuÃ¡l es el punto de fusiÃ³n del titanio?")
```
- **CuÃ¡ndo usar**: Para buscar informaciÃ³n previamente almacenada
- **Respuesta incluye**: 
  - Respuesta generada por IA
  - ğŸ“š Lista de fuentes utilizadas

#### Ejemplo de Flujo Completo:

```bash
# 1. AÃ±adir informaciÃ³n
@rag_server_knowledge learn_text("La temperatura de fusiÃ³n del titanio es 1,668Â°C.", "material_properties")

# 2. Procesar un documento
@rag_server_knowledge learn_document("C:\\Documents\\manual_titanio.pdf")

# 3. Hacer preguntas
@rag_server_knowledge ask_rag("Â¿CuÃ¡l es la temperatura de fusiÃ³n del titanio?")
```

**Respuesta esperada:**
```
La temperatura de fusiÃ³n del titanio es 1,668Â°C.

ğŸ“š Fuentes de informaciÃ³n:
   1. material_properties
   2. manual_titanio.pdf
```

---

## ğŸ§ª Pruebas y VerificaciÃ³n

### Probar el Sistema

Para verificar que todo funciona correctamente:

```bash
# Probar el sistema RAG con metadatos de fuente
python test_rag.py
```

Este script realizarÃ¡ pruebas automÃ¡ticas y mostrarÃ¡ las fuentes de informaciÃ³n utilizadas.

### Verificar la Base de Datos

Los documentos procesados se almacenan en:
- **Base de datos vectorial**: `./rag_mcp_db/`
- **Copias Markdown**: `./converted_docs/`

---

## ğŸ¤– Uso por Agentes de IA

El sistema estÃ¡ optimizado para ser utilizado por agentes de IA. Consulta `AGENT_INSTRUCTIONS.md` para:

- GuÃ­as detalladas de uso
- Ejemplos de casos de uso
- Mejores prÃ¡cticas
- Manejo de errores
- Consideraciones importantes

### CaracterÃ­sticas para Agentes:

- **Descripciones detalladas** de cada herramienta
- **Ejemplos de uso** claros y especÃ­ficos
- **Manejo de errores inteligente** con sugerencias Ãºtiles
- **Metadatos de fuente** para rastreabilidad completa
- **Respuestas estructuradas** con informaciÃ³n de fuentes

---

## ğŸ“‚ Estructura del Proyecto

```
/
â”œâ”€â”€ .venv/                  # Entorno virtual de Python
â”œâ”€â”€ rag_mcp_db/             # Base de datos vectorial (se crea al usarla)
â”œâ”€â”€ converted_docs/          # Copias en Markdown de documentos procesados
â”œâ”€â”€ bulk_ingest.py          # Script para la ingesta masiva de documentos
â”œâ”€â”€ rag_core.py             # LÃ³gica central y reutilizable del sistema RAG
â”œâ”€â”€ rag_server.py           # El servidor MCP (lanzado por run_server.bat)
â”œâ”€â”€ run_server.bat          # Script de arranque para el servidor en Windows
â”œâ”€â”€ requirements.txt        # Dependencias completas (recomendado)
â”œâ”€â”€ requirements-minimal.txt # Dependencias mÃ­nimas para pruebas rÃ¡pidas
â”œâ”€â”€ requirements-dev.txt    # Dependencias de desarrollo
â”œâ”€â”€ pre_download_model.py   # Script para pre-descargar el modelo de embedding
â”œâ”€â”€ test_rag.py             # Script de prueba del sistema RAG
â”œâ”€â”€ AGENT_INSTRUCTIONS.md   # GuÃ­a para agentes de IA
â”œâ”€â”€ proyecto_alpha.txt      # Archivo de ejemplo
â””â”€â”€ README.md               # Este archivo
```