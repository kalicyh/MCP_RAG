import os
from datetime import datetime
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from markitdown import MarkItDown
from urllib.parse import urlparse

# --- Importaciones de nuestro n√∫cleo RAG ---
from rag_core import (
    add_text_to_knowledge_base,           # Funci√≥n para a√±adir texto a la base
    add_text_to_knowledge_base_enhanced,  # Funci√≥n mejorada para a√±adir texto
    load_document_with_fallbacks,         # Nueva funci√≥n de carga con fallbacks
    get_qa_chain,                         # Funci√≥n para obtener la cadena QA
    get_vector_store,                     # Funci√≥n para obtener la base vectorial
    search_with_metadata_filters,         # Nueva funci√≥n de b√∫squeda con filtros
    create_metadata_filter,               # Nueva funci√≥n para crear filtros
    get_document_statistics,              # Nueva funci√≥n para estad√≠sticas
    get_cache_stats,                      # Nueva funci√≥n para estad√≠sticas del cache
    print_cache_stats,                    # Nueva funci√≥n para imprimir estad√≠sticas del cache
    clear_embedding_cache,                # Nueva funci√≥n para limpiar cache
    log,  # Importamos nuestra nueva funci√≥n de log
    optimize_vector_store,
    get_vector_store_stats,
    reindex_vector_store,
    get_optimal_vector_store_profile,
    load_document_with_elements
)

# --- Inicializaci√≥n del Servidor y Configuraci√≥n ---
load_dotenv()
mcp = FastMCP("rag_server_knowledge")

# El estado ahora solo guarda los componentes listos para usar
rag_state = {}

# Inicializamos el conversor de MarkItDown una sola vez (para URLs)
md_converter = MarkItDown()

# Carpeta donde se guardar√°n las copias en Markdown
CONVERTED_DOCS_DIR = "./converted_docs"

def warm_up_rag_system():
    """
    Precarga los componentes pesados del sistema RAG para evitar demoras
    y conflictos en la primera llamada de una herramienta.
    """
    if "warmed_up" in rag_state:
        return
    
    log("MCP Server: Calentando sistema RAG...")
    log("MCP Server: Precargando modelo de embedding en memoria...")
    
    # Esta llamada fuerza la carga del modelo de embedding
    get_vector_store()
    
    rag_state["warmed_up"] = True
    log("MCP Server: Sistema RAG caliente y listo.")

def ensure_converted_docs_directory():
    """Asegura que existe la carpeta para los documentos convertidos."""
    if not os.path.exists(CONVERTED_DOCS_DIR):
        os.makedirs(CONVERTED_DOCS_DIR)
        log(f"MCP Server: Creada carpeta para documentos convertidos: {CONVERTED_DOCS_DIR}")

def save_processed_copy(file_path: str, processed_content: str, processing_method: str = "unstructured") -> str:
    """
    Guarda una copia del documento procesado en formato Markdown.
    
    Args:
        file_path: Ruta original del archivo
        processed_content: Contenido procesado
        processing_method: M√©todo de procesamiento usado
    
    Returns:
        Ruta del archivo Markdown guardado
    """
    ensure_converted_docs_directory()
    
    # Obtener el nombre del archivo original sin extensi√≥n
    original_filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(original_filename)[0]
    
    # Crear el nombre del archivo Markdown con informaci√≥n del m√©todo
    md_filename = f"{name_without_ext}_{processing_method}.md"
    md_filepath = os.path.join(CONVERTED_DOCS_DIR, md_filename)
    
    # Guardar el contenido en el archivo Markdown
    try:
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(processed_content)
        log(f"MCP Server: Copia procesada guardada en: {md_filepath}")
        return md_filepath
    except Exception as e:
        log(f"MCP Server Advertencia: No se pudo guardar copia procesada: {e}")
        return ""

def initialize_rag():
    """
    Inicializa todos los componentes del sistema RAG usando el n√∫cleo.
    """
    if "initialized" in rag_state:
        return

    log("MCP Server: Inicializando sistema RAG v√≠a n√∫cleo...")
    
    # Obtenemos la base de datos y la cadena QA desde nuestro n√∫cleo
    vector_store = get_vector_store()
    qa_chain = get_qa_chain(vector_store)
    
    rag_state["vector_store"] = vector_store
    rag_state["qa_chain"] = qa_chain
    rag_state["initialized"] = True
    log("MCP Server: Sistema RAG inicializado exitosamente.")

# --- Implementaci√≥n de las Herramientas ---

@mcp.tool()
def learn_text(text: str, source_name: str = "manual_input") -> str:
    """
    Adds a new piece of text to the RAG knowledge base for future reference.
    Use this when you want to teach the AI new information that it should remember.
    
    Examples of when to use:
    - Adding facts, definitions, or explanations
    - Storing important information from conversations
    - Saving research findings or notes
    - Adding context about specific topics

    Args:
        text: The text content to be learned and stored in the knowledge base.
        source_name: A descriptive name for the source (e.g., "user_notes", "research_paper", "conversation_summary").
    """
    log(f"MCP Server: Procesando texto de {len(text)} caracteres de la fuente: {source_name}")
    initialize_rag()
    
    try:
        # Crear metadatos de fuente
        source_metadata = {
            "source": source_name,
            "input_type": "manual_text",
            "processed_date": datetime.now().isoformat()
        }
        
        # Usamos la funci√≥n del n√∫cleo para a√±adir el texto con metadatos
        add_text_to_knowledge_base(text, rag_state["vector_store"], source_metadata)
        log(f"MCP Server: Texto a√±adido exitosamente a la base de conocimientos")
        return f"Texto a√±adido exitosamente a la base de conocimientos. Fragmento: '{text[:70]}...' (Fuente: {source_name})"
    except Exception as e:
        log(f"MCP Server: Error al a√±adir texto: {e}")
        return f"Error al a√±adir texto: {e}"

@mcp.tool()
def learn_document(file_path: str) -> str:
    """
    Reads and processes a document file using advanced Unstructured processing with real semantic chunking, and adds it to the knowledge base.
    Use this when you want to teach the AI from document files with intelligent processing.
    
    Supported file types: PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML, ODT, ODP, ODS, RTF, 
    images (PNG, JPG, TIFF, BMP with OCR), emails (EML, MSG), and more than 25 formats total.
    
    Advanced features:
    - REAL semantic chunking based on document structure (titles, sections, lists)
    - Intelligent document structure preservation (titles, lists, tables)
    - Automatic noise removal (headers, footers, irrelevant content)
    - Structural metadata extraction
    - Robust fallback system for any document type
    - Enhanced context preservation through semantic boundaries
    
    Examples of when to use:
    - Processing research papers or articles with complex layouts
    - Adding content from reports or manuals with tables and lists
    - Importing data from spreadsheets with formatting
    - Converting presentations to searchable knowledge
    - Processing scanned documents with OCR
    
    The document will be intelligently processed with REAL semantic chunking and stored with enhanced metadata.
    A copy of the processed document is saved for verification.

    Args:
        file_path: The absolute or relative path to the document file to process.
    """
    log(f"MCP Server: Iniciando procesamiento avanzado de documento: {file_path}")
    log(f"MCP Server: DEBUG - Ruta recibida: {repr(file_path)}")
    log(f"MCP Server: DEBUG - Verificando existencia de ruta absoluta: {os.path.abspath(file_path)}")
    initialize_rag()  # Asegura que el sistema RAG est√© listo
    
    try:
        if not os.path.exists(file_path):
            log(f"MCP Server: Archivo no encontrado en la ruta: {file_path}")
            return f"Error: Archivo no encontrado en '{file_path}'"

        log(f"MCP Server: Procesando documento con sistema Unstructured avanzado...")
        
        # Usar el nuevo sistema de procesamiento con elementos estructurales
        processed_content, metadata, structural_elements = load_document_with_elements(file_path)

        if not processed_content or processed_content.isspace():
            log(f"MCP Server: Advertencia: Documento procesado pero no se pudo extraer contenido: {file_path}")
            return f"Advertencia: El documento '{file_path}' fue procesado, pero no se pudo extraer contenido de texto."

        log(f"MCP Server: Documento procesado exitosamente ({len(processed_content)} caracteres)")
        
        # Guardar copia procesada
        log(f"MCP Server: Guardando copia procesada...")
        processing_method = metadata.get("processing_method", "unknown")
        saved_copy_path = save_processed_copy(file_path, processed_content, processing_method)
        
        # A√±adir contenido a la base de conocimientos con chunking sem√°ntico real
        log(f"MCP Server: A√±adiendo contenido a la base de conocimientos con metadatos estructurales...")
        
        # Usar la funci√≥n mejorada con elementos estructurales para chunking sem√°ntico real
        add_text_to_knowledge_base_enhanced(
            processed_content, 
            rag_state["vector_store"], 
            metadata, 
            use_semantic_chunking=True,
            structural_elements=structural_elements
        )
        
        log(f"MCP Server: Proceso completado - Documento procesado con √©xito")
        
        # Preparar respuesta informativa
        file_name = os.path.basename(file_path)
        file_type = metadata.get("file_type", "unknown")
        processing_method = metadata.get("processing_method", "unknown")
        
        # Informaci√≥n sobre el chunking usado
        chunking_info = ""
        if structural_elements and len(structural_elements) > 1:
            chunking_info = f"üß† **Chunking Sem√°ntico Avanzado** con {len(structural_elements)} elementos estructurales"
        elif metadata.get("structural_info", {}).get("total_elements", 0) > 1:
            chunking_info = f"üìä **Chunking Sem√°ntico Mejorado** basado en metadatos estructurales"
        else:
            chunking_info = f"üìù **Chunking Tradicional** optimizado"
        
        return f"""‚úÖ **Documento procesado exitosamente**
üìÑ **Archivo:** {file_name}
üìã **Tipo:** {file_type.upper()}
üîß **M√©todo:** {processing_method}
{chunking_info}
üìä **Caracteres procesados:** {len(processed_content):,}
üíæ **Copia guardada:** {saved_copy_path if saved_copy_path else "No disponible"}"""

    except Exception as e:
        log(f"MCP Server: Error procesando documento '{file_path}': {e}")
        return f"Error procesando documento: {e}"

@mcp.tool()
def learn_from_url(url: str) -> str:
    """
    Procesa contenido de una URL (p√°gina web o video de YouTube) y lo a√±ade a la base de conocimientos.
    Use this when you want to teach the AI from web content without downloading files.
    
    Supported URL types:
    - Web pages (HTML content)
    - YouTube videos (transcripts)
    - Any URL that MarkItDown can process
    - Direct file downloads (PDF, DOCX, etc.) - will use enhanced Unstructured processing
    
    Examples of when to use:
    - Adding content from news articles or blog posts
    - Processing YouTube video transcripts
    - Importing information from web pages
    - Converting web content to searchable knowledge
    - Processing documents directly from URLs
    
    The content will be intelligently processed and stored with enhanced metadata.
    A copy of the processed content is saved for verification.

    Args:
        url: The URL of the web page or video to process.
    """
    log(f"MCP Server: Iniciando procesamiento de URL: {url}")
    initialize_rag()
    
    try:
        # Verificar si es una URL de descarga directa de archivo
        parsed_url = urlparse(url)
        file_extension = os.path.splitext(parsed_url.path)[1].lower()
        
        # Lista de extensiones que soportan procesamiento mejorado
        enhanced_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', 
                              '.txt', '.html', '.htm', '.csv', '.json', '.xml', '.rtf',
                              '.odt', '.odp', '.ods', '.md', '.yaml', '.yml']
        
        if file_extension in enhanced_extensions:
            log(f"MCP Server: Detectado archivo descargable ({file_extension}), usando procesamiento mejorado...")
            
            # Crear nombre de archivo temporal
            import tempfile
            import requests
            import signal
            
            # Configurar timeout para la descarga
            timeout_seconds = 30
            
            # Descargar el archivo con timeout
            log(f"MCP Server: Descargando archivo con timeout de {timeout_seconds} segundos...")
            response = requests.get(url, stream=True, timeout=timeout_seconds)
            response.raise_for_status()
            
            # Crear archivo temporal
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    temp_file.write(chunk)
                temp_file_path = temp_file.name
            
            log(f"MCP Server: Archivo descargado temporalmente en: {temp_file_path}")
            
            try:
                # Usar el procesamiento mejorado con timeout
                log(f"MCP Server: Iniciando procesamiento con Unstructured (puede tomar varios minutos para PDFs grandes)...")
                
                # Para PDFs, usar configuraci√≥n m√°s r√°pida para evitar colgadas
                if file_extension == '.pdf':
                    log(f"MCP Server: PDF detectado, usando configuraci√≥n optimizada para evitar timeouts...")
                    
                    # Opci√≥n 1: Intentar con PyPDF2 directamente (m√°s r√°pido para Cursor)
                    log(f"MCP Server: Intentando con PyPDF2 directo para mayor velocidad...")
                    try:
                        import PyPDF2
                        with open(temp_file_path, 'rb') as file:
                            pdf_reader = PyPDF2.PdfReader(file)
                            processed_content = ""
                            for page_num, page in enumerate(pdf_reader.pages):
                                page_text = page.extract_text()
                                if page_text:
                                    processed_content += f"\n--- P√°gina {page_num + 1} ---\n{page_text}\n"
                            
                            if processed_content.strip():
                                log(f"MCP Server: PyPDF2 directo exitoso, {len(processed_content)} caracteres extra√≠dos")
                                metadata = {
                                    "source": os.path.basename(temp_file_path),
                                    "file_path": temp_file_path,
                                    "file_type": ".pdf",
                                    "processed_date": datetime.now().isoformat(),
                                    "processing_method": "pypdf2_direct",
                                    "structural_info": {
                                        "total_elements": len(pdf_reader.pages),
                                        "titles_count": 0,
                                        "tables_count": 0,
                                        "lists_count": 0,
                                        "narrative_blocks": len(pdf_reader.pages),
                                        "other_elements": 0,
                                        "total_text_length": len(processed_content),
                                        "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                    }
                                }
                                log(f"MCP Server: Procesamiento con PyPDF2 directo completado")
                            else:
                                # Si PyPDF2 no extrae texto, intentar con Unstructured
                                log(f"MCP Server: PyPDF2 no extrajo texto, intentando con Unstructured...")
                                raise Exception("PyPDF2 no extrajo texto")
                    except Exception as e:
                        log(f"MCP Server: PyPDF2 directo fall√≥: {e}")
                        log(f"MCP Server: Intentando con Unstructured con timeout...")
                        
                        # Opci√≥n 2: Unstructured con timeout (fallback)
                        # Usar threading con timeout para evitar colgadas
                        import threading
                        import time
                        
                        elements = None
                        processing_error = None
                        
                        def process_pdf():
                            nonlocal elements, processing_error
                            try:
                                from rag_core import partition
                                log(f"MCP Server: Iniciando partici√≥n del PDF con strategy='fast'...")
                                log(f"MCP Server: Procesando archivo: {os.path.basename(temp_file_path)}")
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=1000)
                                log(f"MCP Server: Partici√≥n completada, {len(elements)} elementos extra√≠dos")
                            except Exception as e:
                                processing_error = e
                                log(f"MCP Server: Error en partici√≥n: {e}")
                        
                        # Ejecutar procesamiento en hilo separado con timeout
                        thread = threading.Thread(target=process_pdf)
                        thread.daemon = True
                        thread.start()
                        
                        # Esperar m√°ximo 30 segundos para el procesamiento
                        timeout_seconds = 30  # Reducido de 60 a 30 segundos para Cursor
                        
                        # Logs de progreso durante la espera
                        log(f"MCP Server: Esperando procesamiento (timeout: {timeout_seconds}s)...")
                        
                        # Esperar con logs de progreso cada 10 segundos
                        for i in range(0, timeout_seconds, 10):
                            thread.join(10)  # Esperar 10 segundos
                            if not thread.is_alive():
                                break
                            log(f"MCP Server: Procesamiento en progreso... ({i+10}/{timeout_seconds}s)")
                        
                        # Verificar si termin√≥ o si necesitamos esperar m√°s
                        if thread.is_alive():
                            remaining_time = timeout_seconds - (timeout_seconds // 10) * 10
                            if remaining_time > 0:
                                thread.join(remaining_time)
                        
                        if thread.is_alive():
                            log(f"MCP Server: Timeout en procesamiento de PDF despu√©s de {timeout_seconds} segundos")
                            # Intentar con configuraci√≥n m√≠nima
                            log(f"MCP Server: Intentando con configuraci√≥n m√≠nima...")
                            try:
                                from rag_core import partition
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=500)
                                log(f"MCP Server: Partici√≥n m√≠nima completada, {len(elements)} elementos extra√≠dos")
                            except Exception as e:
                                log(f"MCP Server: Error en partici√≥n m√≠nima: {e}")
                                return f"‚ùå **Error de timeout:** El procesamiento del PDF tard√≥ demasiado.\n\nüí° **Consejos:**\n- El PDF puede ser muy grande o complejo\n- Intenta con un PDF m√°s peque√±o\n- Verifica que el archivo no est√© corrupto"
                        
                        if processing_error:
                            log(f"MCP Server: Error en procesamiento: {processing_error}")
                            return f"‚ùå **Error procesando PDF:** {processing_error}\n\nüí° **Consejos:**\n- El archivo puede estar corrupto\n- Intenta con un PDF diferente\n- Verifica que el archivo sea accesible"
                        
                        if not elements:
                            log(f"MCP Server: No se pudieron extraer elementos del PDF")
                            # Intentar con PyPDF2 como fallback
                            log(f"MCP Server: Intentando con PyPDF2 como fallback...")
                            try:
                                import PyPDF2
                                with open(temp_file_path, 'rb') as file:
                                    pdf_reader = PyPDF2.PdfReader(file)
                                    processed_content = ""
                                    for page_num, page in enumerate(pdf_reader.pages):
                                        page_text = page.extract_text()
                                        if page_text:
                                            processed_content += f"\n--- P√°gina {page_num + 1} ---\n{page_text}\n"
                                    
                                    if processed_content.strip():
                                        log(f"MCP Server: PyPDF2 fallback exitoso, {len(processed_content)} caracteres extra√≠dos")
                                        metadata = {
                                            "source": os.path.basename(temp_file_path),
                                            "file_path": temp_file_path,
                                            "file_type": ".pdf",
                                            "processed_date": datetime.now().isoformat(),
                                            "processing_method": "pypdf2_fallback",
                                            "structural_info": {
                                                "total_elements": len(pdf_reader.pages),
                                                "titles_count": 0,
                                                "tables_count": 0,
                                                "lists_count": 0,
                                                "narrative_blocks": len(pdf_reader.pages),
                                                "other_elements": 0,
                                                "total_text_length": len(processed_content),
                                                "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                            }
                                        }
                                    else:
                                        return f"‚ùå **Error:** No se pudo extraer texto del PDF con ning√∫n m√©todo.\n\nüí° **Consejos:**\n- El PDF puede estar escaneado (solo im√°genes)\n- El archivo puede estar corrupto\n- Intenta con un PDF diferente"
                            except ImportError:
                                log(f"MCP Server: PyPDF2 no disponible")
                                return f"‚ùå **Error:** No se pudieron extraer elementos del PDF.\n\nüí° **Consejos:**\n- El archivo puede estar vac√≠o o corrupto\n- Intenta con un PDF diferente"
                            except Exception as e:
                                log(f"MCP Server: Error en PyPDF2 fallback: {e}")
                                return f"‚ùå **Error:** No se pudieron extraer elementos del PDF.\n\nüí° **Consejos:**\n- El archivo puede estar vac√≠o o corrupto\n- Intenta con un PDF diferente"
                        else:
                            log(f"MCP Server: Procesando elementos extra√≠dos...")
                            from rag_core import process_unstructured_elements, extract_structural_metadata
                            processed_content = process_unstructured_elements(elements)
                            log(f"MCP Server: Elementos procesados, {len(processed_content)} caracteres extra√≠dos")
                            
                            metadata = extract_structural_metadata(elements, temp_file_path)
                            metadata["processing_method"] = "unstructured_fast_pdf"
                            log(f"MCP Server: Metadatos estructurales extra√≠dos")
                else:
                    # Para otros formatos, usar el procesamiento normal
                    processed_content, metadata = load_document_with_fallbacks(temp_file_path)
                
                if not processed_content or processed_content.isspace():
                    log(f"MCP Server: Advertencia: Archivo descargado pero no se pudo extraer contenido: {url}")
                    return f"Advertencia: El archivo de la URL '{url}' fue descargado, pero no se pudo extraer contenido de texto."
                
                log(f"MCP Server: Archivo descargado y procesado exitosamente ({len(processed_content)} caracteres)")
                
                # Guardar copia procesada
                log(f"MCP Server: Guardando copia procesada...")
                processing_method = metadata.get("processing_method", "unstructured_enhanced")
                domain = parsed_url.netloc.replace('.', '_')
                path = parsed_url.path.replace('/', '_').replace('.', '_')
                if not path or path == '_':
                    path = 'homepage'
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{domain}_{path}_{timestamp}_{processing_method}.md"
                processed_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
                
                try:
                    ensure_converted_docs_directory()
                    with open(processed_filepath, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    log(f"MCP Server: Copia procesada guardada en: {processed_filepath}")
                except Exception as e:
                    log(f"MCP Server Advertencia: No se pudo guardar copia procesada: {e}")
                    processed_filepath = ""
                
                # Enriquecer metadatos
                enhanced_metadata = metadata.copy()
                enhanced_metadata.update({
                    "source": url,
                    "domain": parsed_url.netloc,
                    "input_type": "url_download",
                    "converted_to_md": processed_filepath if processed_filepath else "No",
                    "server_processed_date": datetime.now().isoformat()
                })
                
                # Usar procesamiento mejorado
                log(f"MCP Server: A√±adiendo contenido a la base de conocimientos...")
                add_text_to_knowledge_base_enhanced(
                    processed_content, 
                    rag_state["vector_store"], 
                    enhanced_metadata, 
                    use_semantic_chunking=True
                )
                
                # Construir respuesta informativa
                structural_info = enhanced_metadata.get("structural_info", {})
                
                response_parts = [
                    f"‚úÖ **Archivo descargado y procesado exitosamente**",
                    f"üåê **URL:** {url}",
                    f"üìÑ **Archivo:** {os.path.basename(parsed_url.path)}",
                    f"üìã **Tipo:** {file_extension.upper()}",
                    f"üîß **M√©todo:** {processing_method.replace('_', ' ').title()}"
                ]
                
                # A√±adir informaci√≥n estructural si est√° disponible
                if structural_info:
                    response_parts.extend([
                        f"üìä **Estructura del documento:**",
                        f"   ‚Ä¢ Elementos totales: {structural_info.get('total_elements', 'N/A')}",
                        f"   ‚Ä¢ T√≠tulos: {structural_info.get('titles_count', 'N/A')}",
                        f"   ‚Ä¢ Tablas: {structural_info.get('tables_count', 'N/A')}",
                        f"   ‚Ä¢ Listas: {structural_info.get('lists_count', 'N/A')}",
                        f"   ‚Ä¢ Bloques narrativos: {structural_info.get('narrative_blocks', 'N/A')}"
                    ])
                
                if processed_filepath:
                    response_parts.append(f"üíæ **Copia guardada:** {processed_filepath}")
                
                response_parts.append(f"üìö **Estado:** A√±adido a la base de conocimientos con chunking sem√°ntico")
                
                log(f"MCP Server: Procesamiento de URL completado exitosamente")
                return "\n".join(response_parts)
                
            finally:
                # Limpiar archivo temporal
                try:
                    os.unlink(temp_file_path)
                    log(f"MCP Server: Archivo temporal eliminado: {temp_file_path}")
                except Exception as e:
                    log(f"MCP Server Advertencia: No se pudo eliminar archivo temporal: {e}")
        
        else:
            # Procesamiento tradicional para p√°ginas web
            log(f"MCP Server: Procesando contenido web con MarkItDown...")
            
            # Usar MarkItDown para procesar la URL directamente con timeout
            try:
                result = md_converter.convert_url(url)
                markdown_content = result.text_content
            except Exception as e:
                log(f"MCP Server: Error con MarkItDown, intentando descarga directa: {e}")
                # Fallback: intentar descarga directa
                import requests
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                markdown_content = response.text

            if not markdown_content or markdown_content.isspace():
                log(f"MCP Server: Advertencia: URL procesada pero no se pudo extraer contenido: {url}")
                return f"Advertencia: La URL '{url}' fue procesada, pero no se pudo extraer contenido de texto."

            log(f"MCP Server: Contenido de URL convertido exitosamente ({len(markdown_content)} caracteres)")
            
            # Guardar copia en Markdown
            log(f"MCP Server: Guardando copia Markdown...")
            
            # Crear nombre de archivo basado en la URL
            domain = parsed_url.netloc.replace('.', '_')
            path = parsed_url.path.replace('/', '_').replace('.', '_')
            if not path or path == '_':
                path = 'homepage'
            
            # Crear nombre de archivo √∫nico
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{domain}_{path}_{timestamp}_markitdown.md"
            md_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
            
            # Guardar el contenido
            try:
                ensure_converted_docs_directory()
                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                log(f"MCP Server: Copia Markdown guardada en: {md_filepath}")
            except Exception as e:
                log(f"MCP Server Advertencia: No se pudo guardar copia Markdown: {e}")
                md_filepath = ""
            
            # A√±adir contenido a la base de conocimientos
            log(f"MCP Server: A√±adiendo contenido a la base de conocimientos...")
            
            # Crear metadatos espec√≠ficos de la URL
            url_metadata = {
                "source": url,
                "domain": parsed_url.netloc,
                "input_type": "url_web",
                "processed_date": datetime.now().isoformat(),
                "processing_method": "markitdown",
                "converted_to_md": md_filepath if md_filepath else "No"
            }
            
            # A√±adir directamente con metadatos
            add_text_to_knowledge_base(markdown_content, rag_state["vector_store"], url_metadata)
            
            # Informaci√≥n sobre el proceso completado
            response_parts = [
                f"‚úÖ **Contenido web procesado exitosamente**",
                f"üåê **URL:** {url}",
                f"üåç **Dominio:** {parsed_url.netloc}",
                f"üîß **M√©todo:** MarkItDown"
            ]
            
            if md_filepath:
                response_parts.append(f"üíæ **Copia guardada:** {md_filepath}")
            
            response_parts.append(f"üìö **Estado:** A√±adido a la base de conocimientos")
            
            log(f"MCP Server: Procesamiento de URL completado exitosamente")
            return "\n".join(response_parts)

    except requests.exceptions.Timeout:
        log(f"MCP Server: Timeout al procesar URL: {url}")
        return f"‚ùå **Error de timeout:** La URL '{url}' tard√≥ demasiado en responder.\n\nüí° **Consejos:**\n- Verifica tu conexi√≥n a internet\n- Intenta m√°s tarde\n- La URL puede estar temporalmente lenta"
    
    except requests.exceptions.ConnectionError:
        log(f"MCP Server: Error de conexi√≥n al procesar URL: {url}")
        return f"‚ùå **Error de conexi√≥n:** No se pudo conectar a la URL '{url}'.\n\nüí° **Consejos:**\n- Verifica tu conexi√≥n a internet\n- La URL puede no estar disponible\n- Intenta m√°s tarde"
    
    except Exception as e:
        log(f"MCP Server: Error procesando URL '{url}': {e}")
        error_msg = f"‚ùå **Error procesando URL '{url}':** {e}"
        
        # Proporcionar informaci√≥n m√°s √∫til para el agente
        if "404" in str(e) or "Not Found" in str(e):
            error_msg += "\n\nüí° **Consejo:** La URL no existe o no es accesible. Verifica que la URL sea correcta."
        elif "timeout" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** La p√°gina tard√≥ demasiado en cargar. Intenta m√°s tarde o verifica tu conexi√≥n a internet."
        elif "permission" in str(e).lower() or "403" in str(e):
            error_msg += "\n\nüí° **Consejo:** No tienes permisos para acceder a esta p√°gina. Algunas p√°ginas bloquean el acceso autom√°tico."
        elif "youtube" in url.lower() and "transcript" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Este video de YouTube no tiene transcripci√≥n disponible o est√° deshabilitada."
        elif "ssl" in str(e).lower() or "certificate" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Problema con el certificado SSL de la p√°gina. Intenta con una URL diferente."
        elif "download" in str(e).lower() or "connection" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Error al descargar el archivo. Verifica que la URL sea accesible y el archivo exista."
        elif "unstructured" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Error en el procesamiento del documento. El archivo puede estar corrupto o ser muy grande."
        
        return error_msg

@mcp.tool()
def ask_rag(query: str) -> str:
    """
    Asks a question to the RAG knowledge base and returns an answer based on the stored information.
    Use this when you want to get information from the knowledge base that has been previously learned.
    
    Examples of when to use:
    - Asking about specific topics or concepts
    - Requesting explanations or definitions
    - Seeking information from processed documents
    - Getting answers based on learned text or documents
    
    The system will search through all stored information and provide the most relevant answer.

    Args:
        query: The question or query to ask the knowledge base.
    """
    log(f"MCP Server: Procesando pregunta: {query}")
    initialize_rag()
    
    try:
        # Usar la cadena QA est√°ndar (sin filtros)
        qa_chain = get_qa_chain(rag_state["vector_store"])
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # Verificar si realmente tenemos informaci√≥n relevante
        if not source_documents:
            # No hay fuentes - el LLM probablemente est√° alucinando
            enhanced_answer = f"ü§ñ **Respuesta:**\n\n‚ùå **No se encontr√≥ informaci√≥n relevante en la base de conocimientos para responder tu pregunta.**\n\n"
            enhanced_answer += "üí° **Sugerencias:**\n"
            enhanced_answer += "‚Ä¢ Verifica que hayas cargado documentos relacionados con tu pregunta\n"
            enhanced_answer += "‚Ä¢ Intenta reformular tu pregunta con t√©rminos m√°s espec√≠ficos\n"
            enhanced_answer += "‚Ä¢ Usa `get_knowledge_base_stats()` para ver qu√© informaci√≥n est√° disponible\n"
            enhanced_answer += "‚Ä¢ Considera cargar m√°s documentos sobre el tema que te interesa\n\n"
            enhanced_answer += "‚ö†Ô∏è **Nota:** El sistema solo puede responder bas√°ndose en la informaci√≥n que ha sido previamente cargada en la base de conocimientos."
            
            log(f"MCP Server: No se encontraron fuentes relevantes para la pregunta")
            return enhanced_answer
        
        # Verificar si la respuesta parece ser una alucinaci√≥n
        # Si no hay fuentes pero hay respuesta, es probable una alucinaci√≥n
        if len(source_documents) == 0 and answer.strip():
            enhanced_answer = f"ü§ñ **Respuesta:**\n\n‚ùå **No se encontr√≥ informaci√≥n espec√≠fica en la base de conocimientos para responder tu pregunta.**\n\n"
            enhanced_answer += "üí° **Sugerencias:**\n"
            enhanced_answer += "‚Ä¢ Verifica que hayas cargado documentos relacionados con tu pregunta\n"
            enhanced_answer += "‚Ä¢ Intenta reformular tu pregunta con t√©rminos m√°s espec√≠ficos\n"
            enhanced_answer += "‚Ä¢ Usa `get_knowledge_base_stats()` para ver qu√© informaci√≥n est√° disponible\n\n"
            enhanced_answer += "‚ö†Ô∏è **Nota:** El sistema solo puede responder bas√°ndose en la informaci√≥n que ha sido previamente cargada en la base de conocimientos."
            
            log(f"MCP Server: Respuesta detectada como posible alucinaci√≥n (sin fuentes)")
            return enhanced_answer
        
        # Si tenemos fuentes, construir respuesta normal
        enhanced_answer = f"ü§ñ **Respuesta:**\n\n{answer}\n"
        
        # A√±adir informaci√≥n de fuentes con m√°s detalles
        if source_documents:
            enhanced_answer += "üìö **Fuentes de informaci√≥n utilizadas:**\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "Fuente desconocida")
                
                # --- Mejoramos la informaci√≥n de la fuente ---
                source_info = f"   {i}. **{source_name}**"
                
                # A√±adir ruta completa si es un documento
                file_path = metadata.get("file_path")
                if file_path:
                    source_info += f"\n      - **Ruta:** `{file_path}`"
                
                # A√±adir tipo de archivo si est√° disponible
                file_type = metadata.get("file_type")
                if file_type:
                    source_info += f"\n      - **Tipo:** {file_type.upper()}"
                
                # A√±adir m√©todo de procesamiento si est√° disponible
                processing_method = metadata.get("processing_method")
                if processing_method:
                    method_display = processing_method.replace('_', ' ').title()
                    source_info += f"\n      - **Procesamiento:** {method_display}"
                
                # A√±adir informaci√≥n estructural si est√° disponible
                structural_info = metadata.get("structural_info")
                if structural_info:
                    source_info += f"\n      - **Estructura:** {structural_info.get('total_elements', 'N/A')} elementos"
                    titles_count = structural_info.get('titles_count', 0)
                    tables_count = structural_info.get('tables_count', 0)
                    lists_count = structural_info.get('lists_count', 0)
                    if titles_count > 0 or tables_count > 0 or lists_count > 0:
                        structure_details = []
                        if titles_count > 0:
                            structure_details.append(f"{titles_count} t√≠tulos")
                        if tables_count > 0:
                            structure_details.append(f"{tables_count} tablas")
                        if lists_count > 0:
                            structure_details.append(f"{lists_count} listas")
                        source_info += f" ({', '.join(structure_details)})"
                
                # Reconstruir informaci√≥n estructural desde metadatos planos
                structural_elements = []
                titles_count = metadata.get("structural_titles_count", 0)
                tables_count = metadata.get("structural_tables_count", 0)
                lists_count = metadata.get("structural_lists_count", 0)
                total_elements = metadata.get("structural_total_elements", 0)
                
                if total_elements > 0:
                    structural_details = []
                    if titles_count > 0:
                        structural_details.append(f"{titles_count} t√≠tulos")
                    if tables_count > 0:
                        structural_details.append(f"{tables_count} tablas")
                    if lists_count > 0:
                        structural_details.append(f"{lists_count} listas")
                    
                    if structural_details:
                        source_info += f"\n      - **Estructura:** {', '.join(structural_details)}"
                
                enhanced_answer += source_info + "\n\n"
        
        # A√±adir informaci√≥n sobre la calidad de la respuesta
        num_sources = len(source_documents)
        if num_sources >= 3:
            enhanced_answer += "\n‚úÖ **Alta confianza:** Respuesta basada en m√∫ltiples fuentes"
        elif num_sources == 2:
            enhanced_answer += "\n‚ö†Ô∏è **Confianza media:** Respuesta basada en 2 fuentes"
        else:
            enhanced_answer += "\n‚ö†Ô∏è **Confianza limitada:** Respuesta basada en 1 fuente"
        
        # A√±adir informaci√≥n sobre el procesamiento si hay documentos con metadatos estructurales
        enhanced_docs = [doc for doc in source_documents if hasattr(doc, 'metadata') and doc.metadata.get("processing_method") == "unstructured_enhanced"]
        if enhanced_docs:
            enhanced_answer += f"\nüß† **Procesamiento inteligente:** {len(enhanced_docs)} fuentes procesadas con Unstructured (preservaci√≥n de estructura)"
        
        log(f"MCP Server: Respuesta generada exitosamente con {len(source_documents)} fuentes")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCP Server: Error procesando pregunta: {e}")
        return f"‚ùå **Error al procesar la pregunta:** {e}\n\nüí° **Sugerencias:**\n- Verifica que el sistema RAG est√© correctamente inicializado\n- Intenta reformular tu pregunta\n- Si el problema persiste, reinicia el servidor"

@mcp.tool()
def ask_rag_filtered(query: str, file_type: str = None, min_tables: int = None, min_titles: int = None, processing_method: str = None) -> str:
    """
    Asks a question to the RAG knowledge base with specific filters to focus the search.
    Use this when you want to get information from specific types of documents or documents with certain characteristics.
    
    Examples of when to use:
    - Searching only in PDF documents: file_type=".pdf"
    - Looking for documents with tables: min_tables=1
    - Finding well-structured documents: min_titles=5
    - Searching in enhanced processed documents: processing_method="unstructured_enhanced"
    
    This provides more targeted and relevant results by filtering the search scope.

    Args:
        query: The question or query to ask the knowledge base.
        file_type: Filter by file type (e.g., ".pdf", ".docx", ".txt")
        min_tables: Minimum number of tables the document must have
        min_titles: Minimum number of titles the document must have
        processing_method: Filter by processing method (e.g., "unstructured_enhanced", "markitdown")
    """
    log(f"MCP Server: Procesando pregunta con filtros: {query}")
    log(f"MCP Server: Filtros aplicados - Tipo: {file_type}, Tablas: {min_tables}, T√≠tulos: {min_titles}, M√©todo: {processing_method}")
    initialize_rag()
    
    try:
        # Crear filtros de metadatos
        metadata_filter = create_metadata_filter(
            file_type=file_type,
            processing_method=processing_method,
            min_tables=min_tables,
            min_titles=min_titles
        )
        
        # Usar la cadena QA con filtros
        qa_chain = get_qa_chain(rag_state["vector_store"], metadata_filter)
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # Verificar si realmente tenemos informaci√≥n relevante con los filtros
        if not source_documents:
            # No hay fuentes que cumplan con los filtros
            enhanced_answer = f"üîç **Respuesta (con filtros aplicados):**\n\n‚ùå **No se encontr√≥ informaci√≥n relevante en la base de conocimientos que cumpla con los filtros especificados.**\n\n"
            
            # Mostrar filtros aplicados
            if metadata_filter:
                enhanced_answer += "üìã **Filtros aplicados:**\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   ‚Ä¢ Tipo de archivo: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   ‚Ä¢ M√©todo de procesamiento: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   ‚Ä¢ M√≠nimo de tablas: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   ‚Ä¢ M√≠nimo de t√≠tulos: {value['$gte']}\n"
                enhanced_answer += "\n"
            
            enhanced_answer += "üí° **Sugerencias:**\n"
            enhanced_answer += "‚Ä¢ Intenta relajar los filtros para obtener m√°s resultados\n"
            enhanced_answer += "‚Ä¢ Usa `get_knowledge_base_stats()` para ver qu√© tipos de documentos est√°n disponibles\n"
            enhanced_answer += "‚Ä¢ Considera usar `ask_rag()` sin filtros para buscar en toda la base de conocimientos\n"
            enhanced_answer += "‚Ä¢ Verifica que hayas cargado documentos que cumplan con los criterios especificados\n\n"
            enhanced_answer += "‚ö†Ô∏è **Nota:** Los filtros pueden ser muy restrictivos. Intenta con filtros m√°s amplios."
            
            log(f"MCP Server: No se encontraron fuentes que cumplan con los filtros especificados")
            return enhanced_answer
        
        # Verificar si la respuesta parece ser una alucinaci√≥n
        if len(source_documents) == 0 and answer.strip():
            enhanced_answer = f"üîç **Respuesta (con filtros aplicados):**\n\n‚ùå **No se encontr√≥ informaci√≥n espec√≠fica que cumpla con los filtros especificados.**\n\n"
            
            # Mostrar filtros aplicados
            if metadata_filter:
                enhanced_answer += "üìã **Filtros aplicados:**\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   ‚Ä¢ Tipo de archivo: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   ‚Ä¢ M√©todo de procesamiento: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   ‚Ä¢ M√≠nimo de tablas: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   ‚Ä¢ M√≠nimo de t√≠tulos: {value['$gte']}\n"
                enhanced_answer += "\n"
            
            enhanced_answer += "üí° **Sugerencias:**\n"
            enhanced_answer += "‚Ä¢ Intenta relajar los filtros para obtener m√°s resultados\n"
            enhanced_answer += "‚Ä¢ Usa `get_knowledge_base_stats()` para ver qu√© tipos de documentos est√°n disponibles\n"
            enhanced_answer += "‚Ä¢ Considera usar `ask_rag()` sin filtros para buscar en toda la base de conocimientos\n\n"
            enhanced_answer += "‚ö†Ô∏è **Nota:** Los filtros pueden ser muy restrictivos. Intenta con filtros m√°s amplios."
            
            log(f"MCP Server: Respuesta filtrada detectada como posible alucinaci√≥n (sin fuentes)")
            return enhanced_answer
        
        # Si tenemos fuentes, construir respuesta normal
        enhanced_answer = f"üîç **Respuesta (con filtros aplicados):**\n\n{answer}\n"
        
        # Mostrar filtros aplicados
        if metadata_filter:
            enhanced_answer += "\nüìã **Filtros aplicados:**\n"
            for key, value in metadata_filter.items():
                if key == "file_type":
                    enhanced_answer += f"   ‚Ä¢ Tipo de archivo: {value}\n"
                elif key == "processing_method":
                    enhanced_answer += f"   ‚Ä¢ M√©todo de procesamiento: {value.replace('_', ' ').title()}\n"
                elif key == "structural_tables_count":
                    enhanced_answer += f"   ‚Ä¢ M√≠nimo de tablas: {value['$gte']}\n"
                elif key == "structural_titles_count":
                    enhanced_answer += f"   ‚Ä¢ M√≠nimo de t√≠tulos: {value['$gte']}\n"
        
        # A√±adir informaci√≥n de fuentes
        if source_documents:
            enhanced_answer += f"\nüìö **Fuentes encontradas ({len(source_documents)}):**\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "Fuente desconocida")
                
                source_info = f"   {i}. **{source_name}**"
                
                # Informaci√≥n estructural
                tables_count = metadata.get("structural_tables_count", 0)
                titles_count = metadata.get("structural_titles_count", 0)
                file_type = metadata.get("file_type", "")
                
                structural_details = []
                if tables_count > 0:
                    structural_details.append(f"{tables_count} tablas")
                if titles_count > 0:
                    structural_details.append(f"{titles_count} t√≠tulos")
                
                if structural_details:
                    source_info += f" ({', '.join(structural_details)})"
                
                if file_type:
                    source_info += f" [{file_type.upper()}]"
                
                enhanced_answer += source_info + "\n"
        
        # Informaci√≥n sobre la b√∫squeda filtrada
        enhanced_answer += f"\nüéØ **B√∫squeda filtrada:** Los resultados se limitaron a documentos que cumplen con los criterios especificados."
        
        log(f"MCP Server: Respuesta filtrada generada exitosamente con {len(source_documents)} fuentes")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCP Server: Error procesando pregunta filtrada: {e}")
        return f"‚ùå **Error al procesar la pregunta filtrada:** {e}"

@mcp.tool()
def get_knowledge_base_stats() -> str:
    """
    Gets comprehensive statistics about the knowledge base, including document types, processing methods, and structural information.
    Use this to understand what information is available in your knowledge base and how it was processed.
    
    Examples of when to use:
    - Checking how many documents are in the knowledge base
    - Understanding the distribution of file types
    - Seeing which processing methods were used
    - Analyzing the structural complexity of stored documents
    
    This helps you make informed decisions about what to search for and how to filter your queries.

    Returns:
        Detailed statistics about the knowledge base contents.
    """
    log(f"MCP Server: Obteniendo estad√≠sticas de la base de conocimientos...")
    initialize_rag()
    
    try:
        stats = get_document_statistics(rag_state["vector_store"])
        
        if "error" in stats:
            return f"‚ùå **Error obteniendo estad√≠sticas:** {stats['error']}"
        
        if stats.get("total_documents", 0) == 0:
            return "üìä **Base de conocimientos vac√≠a**\n\nNo hay documentos almacenados en la base de conocimientos."
        
        # Construir respuesta detallada
        response = f"üìä **Estad√≠sticas de la Base de Conocimientos**\n\n"
        response += f"üìö **Total de documentos:** {stats['total_documents']}\n\n"
        
        # Tipos de archivo
        if stats["file_types"]:
            response += "üìÑ **Tipos de archivo:**\n"
            for file_type, count in sorted(stats["file_types"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                response += f"   ‚Ä¢ {file_type.upper()}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # M√©todos de procesamiento
        if stats["processing_methods"]:
            response += "üîß **M√©todos de procesamiento:**\n"
            for method, count in sorted(stats["processing_methods"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                method_display = method.replace('_', ' ').title()
                response += f"   ‚Ä¢ {method_display}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # Estad√≠sticas estructurales
        structural = stats["structural_stats"]
        response += "üèóÔ∏è **Informaci√≥n estructural:**\n"
        response += f"   ‚Ä¢ Documentos con tablas: {structural['documents_with_tables']}\n"
        response += f"   ‚Ä¢ Documentos con t√≠tulos: {structural['documents_with_titles']}\n"
        response += f"   ‚Ä¢ Documentos con listas: {structural['documents_with_lists']}\n"
        response += f"   ‚Ä¢ Promedio de tablas por documento: {structural['avg_tables_per_doc']:.1f}\n"
        response += f"   ‚Ä¢ Promedio de t√≠tulos por documento: {structural['avg_titles_per_doc']:.1f}\n"
        response += f"   ‚Ä¢ Promedio de listas por documento: {structural['avg_lists_per_doc']:.1f}\n\n"
        
        # Sugerencias de b√∫squeda
        response += "üí° **Sugerencias de b√∫squeda:**\n"
        if structural['documents_with_tables'] > 0:
            response += f"   ‚Ä¢ Usa `ask_rag_filtered` con `min_tables=1` para buscar informaci√≥n en documentos con tablas\n"
        if structural['documents_with_titles'] > 5:
            response += f"   ‚Ä¢ Usa `ask_rag_filtered` con `min_titles=5` para buscar en documentos bien estructurados\n"
        if ".pdf" in stats["file_types"]:
            response += f"   ‚Ä¢ Usa `ask_rag_filtered` con `file_type=\".pdf\"` para buscar solo en documentos PDF\n"
        
        log(f"MCP Server: Estad√≠sticas obtenidas exitosamente")
        return response
        
    except Exception as e:
        log(f"MCP Server: Error obteniendo estad√≠sticas: {e}")
        return f"‚ùå **Error obteniendo estad√≠sticas:** {e}"

@mcp.tool()
def get_embedding_cache_stats() -> str:
    """
    Gets detailed statistics about the embedding cache performance.
    Use this to monitor cache efficiency and understand how the system is performing.
    
    Examples of when to use:
    - Checking cache hit rates to see if the system is working efficiently
    - Monitoring memory usage of the cache
    - Understanding how often embeddings are being reused
    - Debugging performance issues
    
    This helps you optimize the system and understand its behavior.

    Returns:
        Detailed statistics about the embedding cache performance.
    """
    log(f"MCP Server: Obteniendo estad√≠sticas del cache de embeddings...")
    
    try:
        stats = get_cache_stats()
        
        if not stats:
            return "üìä **Cache de embeddings no disponible**\n\nEl cache de embeddings no est√° inicializado."
        
        # Construir respuesta detallada
        response = f"üìä **Estad√≠sticas del Cache de Embeddings**\n\n"
        
        # M√©tricas principales
        response += f"üîÑ **Actividad del cache:**\n"
        response += f"   ‚Ä¢ Total de solicitudes: {stats['total_requests']}\n"
        response += f"   ‚Ä¢ Hits en memoria: {stats['memory_hits']}\n"
        response += f"   ‚Ä¢ Hits en disco: {stats['disk_hits']}\n"
        response += f"   ‚Ä¢ Misses (no encontrados): {stats['misses']}\n\n"
        
        # Tasas de √©xito
        response += f"üìà **Tasas de √©xito:**\n"
        response += f"   ‚Ä¢ Tasa de hits en memoria: {stats['memory_hit_rate']}\n"
        response += f"   ‚Ä¢ Tasa de hits en disco: {stats['disk_hit_rate']}\n"
        response += f"   ‚Ä¢ Tasa de hits total: {stats['overall_hit_rate']}\n\n"
        
        # Uso de memoria
        response += f"üíæ **Uso de memoria:**\n"
        response += f"   ‚Ä¢ Embeddings en memoria: {stats['memory_cache_size']}\n"
        response += f"   ‚Ä¢ Tama√±o m√°ximo: {stats['max_memory_size']}\n"
        response += f"   ‚Ä¢ Directorio de cache: {stats['cache_directory']}\n\n"
        
        # An√°lisis de rendimiento
        total_requests = stats['total_requests']
        if total_requests > 0:
            memory_hit_rate = float(stats['memory_hit_rate'].rstrip('%'))
            overall_hit_rate = float(stats['overall_hit_rate'].rstrip('%'))
            
            response += f"üéØ **An√°lisis de rendimiento:**\n"
            
            if overall_hit_rate > 70:
                response += f"   ‚Ä¢ ‚úÖ Excelente rendimiento: {overall_hit_rate:.1f}% de hits\n"
            elif overall_hit_rate > 50:
                response += f"   ‚Ä¢ ‚ö†Ô∏è Rendimiento moderado: {overall_hit_rate:.1f}% de hits\n"
            else:
                response += f"   ‚Ä¢ ‚ùå Rendimiento bajo: {overall_hit_rate:.1f}% de hits\n"
            
            if memory_hit_rate > 50:
                response += f"   ‚Ä¢ üöÄ Cache en memoria efectivo: {memory_hit_rate:.1f}% de hits en memoria\n"
            else:
                response += f"   ‚Ä¢ üíæ Dependencia del disco: {memory_hit_rate:.1f}% de hits en memoria\n"
            
            # Sugerencias de optimizaci√≥n
            response += f"\nüí° **Sugerencias de optimizaci√≥n:**\n"
            if overall_hit_rate < 30:
                response += f"   ‚Ä¢ Considera procesar documentos similares juntos\n"
                response += f"   ‚Ä¢ Revisa si hay muchos textos √∫nicos que no se repiten\n"
            
            if memory_hit_rate < 30 and total_requests > 100:
                response += f"   ‚Ä¢ Considera aumentar el tama√±o del cache en memoria\n"
                response += f"   ‚Ä¢ Los hits en disco son m√°s lentos que en memoria\n"
            
            if stats['memory_cache_size'] >= stats['max_memory_size'] * 0.9:
                response += f"   ‚Ä¢ El cache en memoria est√° casi lleno\n"
                response += f"   ‚Ä¢ Considera aumentar max_memory_size si tienes RAM disponible\n"
        
        log(f"MCP Server: Estad√≠sticas del cache obtenidas exitosamente")
        return response
        
    except Exception as e:
        log(f"MCP Server: Error obteniendo estad√≠sticas del cache: {e}")
        return f"‚ùå **Error obteniendo estad√≠sticas del cache:** {e}"

@mcp.tool()
def clear_embedding_cache_tool() -> str:
    """
    Clears the embedding cache to free up memory and disk space.
    Use this when you want to reset the cache or free up resources.
    
    Examples of when to use:
    - Freeing up memory when the system is running low on RAM
    - Resetting the cache after making changes to the embedding model
    - Clearing old cached embeddings that are no longer needed
    - Troubleshooting cache-related issues
    
    Warning: This will remove all cached embeddings and they will need to be recalculated.

    Returns:
        Confirmation message about the cache clearing operation.
    """
    log(f"MCP Server: Limpiando cache de embeddings...")
    
    try:
        clear_embedding_cache()
        
        response = "üßπ **Cache de embeddings limpiado exitosamente**\n\n"
        response += "‚úÖ Se han eliminado todos los embeddings almacenados en cache.\n"
        response += "üìù Los pr√≥ximos embeddings se calcular√°n desde cero.\n"
        response += "üíæ Se ha liberado memoria y espacio en disco.\n\n"
        response += "‚ö†Ô∏è **Nota:** Los embeddings se recalcular√°n autom√°ticamente cuando sea necesario."
        
        log(f"MCP Server: Cache de embeddings limpiado exitosamente")
        return response
        
    except Exception as e:
        log(f"MCP Server: Error limpiando cache: {e}")
        return f"‚ùå **Error limpiando cache:** {e}"

@mcp.tool()
def optimize_vector_database() -> str:
    """
    Optimiza la base de datos vectorial para mejorar el rendimiento de b√∫squedas.
    Esta herramienta reorganiza los √≠ndices internos para b√∫squedas m√°s r√°pidas.
    
    Use esta herramienta cuando:
    - Las b√∫squedas son lentas
    - Se han a√±adido muchos documentos nuevos
    - Quieres mejorar el rendimiento general del sistema
    
    Returns:
        Informaci√≥n sobre el proceso de optimizaci√≥n
    """
    log("MCP Server: Optimizando base de datos vectorial...")
    
    try:
        result = optimize_vector_store()
        
        if result["status"] == "success":
            response = f"‚úÖ **Base de datos vectorial optimizada exitosamente**\n\n"
            response += f"üìä **Estad√≠sticas antes de la optimizaci√≥n:**\n"
            stats_before = result.get("stats_before", {})
            response += f"   ‚Ä¢ Documentos totales: {stats_before.get('total_documents', 'N/A')}\n"
            
            response += f"\nüìä **Estad√≠sticas despu√©s de la optimizaci√≥n:**\n"
            stats_after = result.get("stats_after", {})
            response += f"   ‚Ä¢ Documentos totales: {stats_after.get('total_documents', 'N/A')}\n"
            
            response += f"\nüöÄ **Beneficios:**\n"
            response += f"   ‚Ä¢ B√∫squedas m√°s r√°pidas\n"
            response += f"   ‚Ä¢ Mejor precisi√≥n en resultados\n"
            response += f"   ‚Ä¢ √çndices optimizados\n"
            
        else:
            response = f"‚ùå **Error optimizando base de datos:** {result.get('message', 'Error desconocido')}"
            
        return response
        
    except Exception as e:
        log(f"MCP Server Error: Error en optimizaci√≥n: {e}")
        return f"‚ùå **Error optimizando base de datos vectorial:** {str(e)}"

@mcp.tool()
def get_vector_database_stats() -> str:
    """
    Obtiene estad√≠sticas detalladas de la base de datos vectorial.
    Incluye informaci√≥n sobre documentos, tipos de archivo y configuraci√≥n.
    
    Use esta herramienta para:
    - Verificar el estado de la base de datos
    - Analizar la distribuci√≥n de documentos
    - Diagnosticar problemas de rendimiento
    - Planificar optimizaciones
    
    Returns:
        Estad√≠sticas detalladas de la base de datos vectorial
    """
    log("MCP Server: Obteniendo estad√≠sticas de base de datos vectorial...")
    
    try:
        stats = get_vector_store_stats()
        
        if "error" in stats:
            return f"‚ùå **Error obteniendo estad√≠sticas:** {stats['error']}"
        
        response = f"üìä **Estad√≠sticas de la Base de Datos Vectorial**\n\n"
        
        response += f"üìö **Informaci√≥n General:**\n"
        response += f"   ‚Ä¢ Total de documentos: {stats.get('total_documents', 0)}\n"
        response += f"   ‚Ä¢ Nombre de colecci√≥n: {stats.get('collection_name', 'N/A')}\n"
        response += f"   ‚Ä¢ Dimensi√≥n de embeddings: {stats.get('embedding_dimension', 'N/A')}\n"
        
        # Tipos de archivo
        file_types = stats.get('file_types', {})
        if file_types:
            response += f"\nüìÑ **Distribuci√≥n por tipo de archivo:**\n"
            for file_type, count in file_types.items():
                response += f"   ‚Ä¢ {file_type}: {count} documentos\n"
        
        # M√©todos de procesamiento
        processing_methods = stats.get('processing_methods', {})
        if processing_methods:
            response += f"\nüîß **M√©todos de procesamiento:**\n"
            for method, count in processing_methods.items():
                response += f"   ‚Ä¢ {method}: {count} documentos\n"
        
        # Perfil recomendado
        try:
            recommended_profile = get_optimal_vector_store_profile()
            response += f"\nüéØ **Perfil recomendado:** {recommended_profile}\n"
        except:
            pass
        
        return response
        
    except Exception as e:
        log(f"MCP Server Error: Error obteniendo estad√≠sticas: {e}")
        return f"‚ùå **Error obteniendo estad√≠sticas de base de datos:** {str(e)}"

@mcp.tool()
def reindex_vector_database(profile: str = 'auto') -> str:
    """
    Reindexa la base de datos vectorial con una configuraci√≥n optimizada.
    Esta herramienta recrea los √≠ndices con par√°metros optimizados para el tama√±o actual.
    
    Args:
        profile: Perfil de configuraci√≥n ('small', 'medium', 'large', 'auto')
                 'auto' detecta autom√°ticamente el perfil √≥ptimo
    
    Use esta herramienta cuando:
    - Cambias el perfil de configuraci√≥n
    - Las b√∫squedas son muy lentas
    - Quieres optimizar para un tama√±o espec√≠fico de base de datos
    - Hay problemas de rendimiento persistentes
    
    ‚ö†Ô∏è **Nota:** Este proceso puede tomar tiempo dependiendo del tama√±o de la base de datos.
    
    Returns:
        Informaci√≥n sobre el proceso de reindexado
    """
    log(f"MCP Server: Reindexando base de datos vectorial con perfil '{profile}'...")
    
    try:
        result = reindex_vector_store(profile=profile)
        
        if result["status"] == "success":
            response = f"‚úÖ **Base de datos vectorial reindexada exitosamente**\n\n"
            response += f"üìä **Informaci√≥n del proceso:**\n"
            response += f"   ‚Ä¢ Perfil aplicado: {profile}\n"
            response += f"   ‚Ä¢ Documentos procesados: {result.get('documents_processed', 0)}\n"
            
            response += f"\nüöÄ **Beneficios del reindexado:**\n"
            response += f"   ‚Ä¢ √çndices optimizados para el tama√±o actual\n"
            response += f"   ‚Ä¢ B√∫squedas m√°s r√°pidas y precisas\n"
            response += f"   ‚Ä¢ Mejor uso de memoria\n"
            
        elif result["status"] == "warning":
            response = f"‚ö†Ô∏è **Advertencia:** {result.get('message', 'No hay documentos para reindexar')}"
            
        else:
            response = f"‚ùå **Error reindexando base de datos:** {result.get('message', 'Error desconocido')}"
            
        return response
        
    except Exception as e:
        log(f"MCP Server Error: Error en reindexado: {e}")
        return f"‚ùå **Error reindexando base de datos vectorial:** {str(e)}"

# --- Punto de Entrada para Correr el Servidor ---
if __name__ == "__main__":
    log("Iniciando servidor MCP RAG...")
    warm_up_rag_system()  # Calentamos el sistema al arrancar
    mcp.run(transport='stdio') 