import os
from datetime import datetime
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from markitdown import MarkItDown
from urllib.parse import urlparse

# --- Importaciones de nuestro n√∫cleo RAG ---
from rag_core import (
    get_vector_store,
    add_text_to_knowledge_base,
    add_text_to_knowledge_base_enhanced,  # Nueva funci√≥n mejorada
    load_document_with_fallbacks,         # Nueva funci√≥n de procesamiento con Unstructured
    get_qa_chain,
    log  # Importamos nuestra nueva funci√≥n de log
)

# --- Inicializaci√≥n del Servidor y Configuraci√≥n ---
load_dotenv()
mcp = FastMCP("rag_server_knowledge")

# El estado ahora solo guarda los componentes listos para usar
rag_state = {}

# Inicializamos el conversor de MarkItDown una sola vez (para URLs)
md_converter = MarkItDown()

# Carpeta donde se guardar√°n las copias en Markdown
CONVERTED_DOCS_DIR = "./converted_docs"

def warm_up_rag_system():
    """
    Precarga los componentes pesados del sistema RAG para evitar demoras
    y conflictos en la primera llamada de una herramienta.
    """
    if "warmed_up" in rag_state:
        return
    
    log("MCP Server: Calentando sistema RAG...")
    log("MCP Server: Precargando modelo de embedding en memoria...")
    
    # Esta llamada fuerza la carga del modelo de embedding
    get_vector_store()
    
    rag_state["warmed_up"] = True
    log("MCP Server: Sistema RAG caliente y listo.")

def ensure_converted_docs_directory():
    """Asegura que existe la carpeta para los documentos convertidos."""
    if not os.path.exists(CONVERTED_DOCS_DIR):
        os.makedirs(CONVERTED_DOCS_DIR)
        log(f"MCP Server: Creada carpeta para documentos convertidos: {CONVERTED_DOCS_DIR}")

def save_processed_copy(file_path: str, processed_content: str, processing_method: str = "unstructured") -> str:
    """
    Guarda una copia del documento procesado en formato Markdown.
    
    Args:
        file_path: Ruta original del archivo
        processed_content: Contenido procesado
        processing_method: M√©todo de procesamiento usado
    
    Returns:
        Ruta del archivo Markdown guardado
    """
    ensure_converted_docs_directory()
    
    # Obtener el nombre del archivo original sin extensi√≥n
    original_filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(original_filename)[0]
    
    # Crear el nombre del archivo Markdown con informaci√≥n del m√©todo
    md_filename = f"{name_without_ext}_{processing_method}.md"
    md_filepath = os.path.join(CONVERTED_DOCS_DIR, md_filename)
    
    # Guardar el contenido en el archivo Markdown
    try:
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(processed_content)
        log(f"MCP Server: Copia procesada guardada en: {md_filepath}")
        return md_filepath
    except Exception as e:
        log(f"MCP Server Advertencia: No se pudo guardar copia procesada: {e}")
        return ""

def initialize_rag():
    """
    Inicializa todos los componentes del sistema RAG usando el n√∫cleo.
    """
    if "initialized" in rag_state:
        return

    log("MCP Server: Inicializando sistema RAG v√≠a n√∫cleo...")
    
    # Obtenemos la base de datos y la cadena QA desde nuestro n√∫cleo
    vector_store = get_vector_store()
    qa_chain = get_qa_chain(vector_store)
    
    rag_state["vector_store"] = vector_store
    rag_state["qa_chain"] = qa_chain
    rag_state["initialized"] = True
    log("MCP Server: Sistema RAG inicializado exitosamente.")

# --- Implementaci√≥n de las Herramientas ---

@mcp.tool()
def learn_text(text: str, source_name: str = "manual_input") -> str:
    """
    Adds a new piece of text to the RAG knowledge base for future reference.
    Use this when you want to teach the AI new information that it should remember.
    
    Examples of when to use:
    - Adding facts, definitions, or explanations
    - Storing important information from conversations
    - Saving research findings or notes
    - Adding context about specific topics

    Args:
        text: The text content to be learned and stored in the knowledge base.
        source_name: A descriptive name for the source (e.g., "user_notes", "research_paper", "conversation_summary").
    """
    log(f"MCP Server: Procesando texto de {len(text)} caracteres de la fuente: {source_name}")
    initialize_rag()
    
    try:
        # Crear metadatos de fuente
        source_metadata = {
            "source": source_name,
            "input_type": "manual_text",
            "processed_date": datetime.now().isoformat()
        }
        
        # Usamos la funci√≥n del n√∫cleo para a√±adir el texto con metadatos
        add_text_to_knowledge_base(text, rag_state["vector_store"], source_metadata)
        log(f"MCP Server: Texto a√±adido exitosamente a la base de conocimientos")
        return f"Texto a√±adido exitosamente a la base de conocimientos. Fragmento: '{text[:70]}...' (Fuente: {source_name})"
    except Exception as e:
        log(f"MCP Server: Error al a√±adir texto: {e}")
        return f"Error al a√±adir texto: {e}"

@mcp.tool()
def learn_document(file_path: str) -> str:
    """
    Reads and processes a document file using advanced Unstructured processing, and adds it to the knowledge base.
    Use this when you want to teach the AI from document files with intelligent processing.
    
    Supported file types: PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML, ODT, ODP, ODS, RTF, 
    images (PNG, JPG, TIFF, BMP with OCR), emails (EML, MSG), and more than 25 formats total.
    
    Advanced features:
    - Intelligent document structure preservation (titles, lists, tables)
    - Automatic noise removal (headers, footers, irrelevant content)
    - Semantic chunking for better context
    - Robust fallback system for any document type
    - Structural metadata extraction
    
    Examples of when to use:
    - Processing research papers or articles with complex layouts
    - Adding content from reports or manuals with tables and lists
    - Importing data from spreadsheets with formatting
    - Converting presentations to searchable knowledge
    - Processing scanned documents with OCR
    
    The document will be intelligently processed and stored with enhanced metadata.
    A copy of the processed document is saved for verification.

    Args:
        file_path: The absolute or relative path to the document file to process.
    """
    log(f"MCP Server: Iniciando procesamiento avanzado de documento: {file_path}")
    log(f"MCP Server: DEBUG - Ruta recibida: {repr(file_path)}")
    log(f"MCP Server: DEBUG - Verificando existencia de ruta absoluta: {os.path.abspath(file_path)}")
    initialize_rag()  # Asegura que el sistema RAG est√© listo
    
    try:
        if not os.path.exists(file_path):
            log(f"MCP Server: Archivo no encontrado en la ruta: {file_path}")
            return f"Error: Archivo no encontrado en '{file_path}'"

        log(f"MCP Server: Procesando documento con sistema Unstructured avanzado...")
        
        # Usar el nuevo sistema de procesamiento con Unstructured y fallbacks
        processed_content, metadata = load_document_with_fallbacks(file_path)

        if not processed_content or processed_content.isspace():
            log(f"MCP Server: Advertencia: Documento procesado pero no se pudo extraer contenido: {file_path}")
            return f"Advertencia: El documento '{file_path}' fue procesado, pero no se pudo extraer contenido de texto."

        log(f"MCP Server: Documento procesado exitosamente ({len(processed_content)} caracteres)")
        
        # Guardar copia procesada
        log(f"MCP Server: Guardando copia procesada...")
        processing_method = metadata.get("processing_method", "unstructured_enhanced")
        processed_copy_path = save_processed_copy(file_path, processed_content, processing_method)
        
        # A√±adir contenido a la base de conocimientos con metadatos estructurales
        log(f"MCP Server: A√±adiendo contenido a la base de conocimientos con metadatos estructurales...")
        
        # Enriquecer metadatos con informaci√≥n del servidor
        enhanced_metadata = metadata.copy()
        enhanced_metadata.update({
            "input_type": "document",
            "converted_to_md": processed_copy_path if processed_copy_path else "No",
            "server_processed_date": datetime.now().isoformat()
        })
        
        # Usar la funci√≥n mejorada que soporta chunking sem√°ntico
        add_text_to_knowledge_base_enhanced(
            processed_content, 
            rag_state["vector_store"], 
            enhanced_metadata, 
            use_semantic_chunking=True
        )
        
        # Construir respuesta informativa
        file_type = enhanced_metadata.get("file_type", "desconocido")
        structural_info = enhanced_metadata.get("structural_info", {})
        
        response_parts = [
            f"‚úÖ **Documento procesado exitosamente**",
            f"üìÑ **Archivo:** {os.path.basename(file_path)}",
            f"üìã **Tipo:** {file_type.upper()}",
            f"üîß **M√©todo:** {processing_method.replace('_', ' ').title()}"
        ]
        
        # A√±adir informaci√≥n estructural si est√° disponible
        if structural_info:
            response_parts.extend([
                f"üìä **Estructura del documento:**",
                f"   ‚Ä¢ Elementos totales: {structural_info.get('total_elements', 'N/A')}",
                f"   ‚Ä¢ T√≠tulos: {structural_info.get('titles_count', 'N/A')}",
                f"   ‚Ä¢ Tablas: {structural_info.get('tables_count', 'N/A')}",
                f"   ‚Ä¢ Listas: {structural_info.get('lists_count', 'N/A')}",
                f"   ‚Ä¢ Bloques narrativos: {structural_info.get('narrative_blocks', 'N/A')}"
            ])
        
        # A√±adir informaci√≥n sobre la copia guardada
        if processed_copy_path:
            response_parts.append(f"üíæ **Copia guardada:** {processed_copy_path}")
        
        response_parts.append(f"üìö **Estado:** A√±adido a la base de conocimientos con chunking sem√°ntico")
        
        log(f"MCP Server: Proceso completado - Documento procesado con √©xito")
        return "\n".join(response_parts)

    except Exception as e:
        log(f"MCP Server: Error procesando documento '{file_path}': {e}")
        error_msg = f"‚ùå **Error procesando documento '{file_path}':** {e}"
        
        # Proporcionar informaci√≥n m√°s √∫til para el agente
        if "File not found" in str(e):
            error_msg += "\n\nüí° **Consejo:** Aseg√∫rate de que la ruta del archivo sea correcta y que el archivo exista."
        elif "UnsupportedFormatException" in str(e):
            error_msg += "\n\nüí° **Consejo:** Este formato de archivo no es compatible. El sistema soporta m√°s de 25 formatos incluyendo PDF, DOCX, PPTX, XLSX, TXT, HTML, CSV, JSON, XML, im√°genes con OCR, y m√°s."
        elif "permission" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Verifica si tienes permisos para acceder a este archivo."
        elif "tesseract" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Para procesar im√°genes con texto, instala Tesseract OCR: `choco install tesseract` (Windows) o desde GitHub."
        elif "unstructured" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Verifica que Unstructured est√© instalado correctamente: `pip install 'unstructured[local-inference,all-docs]'`"
        
        return error_msg

@mcp.tool()
def learn_from_url(url: str) -> str:
    """
    Procesa contenido de una URL (p√°gina web o video de YouTube) y lo a√±ade a la base de conocimientos.
    Use this when you want to teach the AI from web content without downloading files.
    
    Supported URL types:
    - Web pages (HTML content)
    - YouTube videos (transcripts)
    - Any URL that MarkItDown can process
    - Direct file downloads (PDF, DOCX, etc.) - will use enhanced Unstructured processing
    
    Examples of when to use:
    - Adding content from news articles or blog posts
    - Processing YouTube video transcripts
    - Importing information from web pages
    - Converting web content to searchable knowledge
    - Processing documents directly from URLs
    
    The content will be intelligently processed and stored with enhanced metadata.
    A copy of the processed content is saved for verification.

    Args:
        url: The URL of the web page or video to process.
    """
    log(f"MCP Server: Iniciando procesamiento de URL: {url}")
    initialize_rag()
    
    try:
        # Verificar si es una URL de descarga directa de archivo
        parsed_url = urlparse(url)
        file_extension = os.path.splitext(parsed_url.path)[1].lower()
        
        # Lista de extensiones que soportan procesamiento mejorado
        enhanced_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', 
                              '.txt', '.html', '.htm', '.csv', '.json', '.xml', '.rtf',
                              '.odt', '.odp', '.ods', '.md', '.yaml', '.yml']
        
        if file_extension in enhanced_extensions:
            log(f"MCP Server: Detectado archivo descargable ({file_extension}), usando procesamiento mejorado...")
            
            # Crear nombre de archivo temporal
            import tempfile
            import requests
            import signal
            
            # Configurar timeout para la descarga
            timeout_seconds = 30
            
            # Descargar el archivo con timeout
            log(f"MCP Server: Descargando archivo con timeout de {timeout_seconds} segundos...")
            response = requests.get(url, stream=True, timeout=timeout_seconds)
            response.raise_for_status()
            
            # Crear archivo temporal
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    temp_file.write(chunk)
                temp_file_path = temp_file.name
            
            log(f"MCP Server: Archivo descargado temporalmente en: {temp_file_path}")
            
            try:
                # Usar el procesamiento mejorado con timeout
                log(f"MCP Server: Iniciando procesamiento con Unstructured (puede tomar varios minutos para PDFs grandes)...")
                
                # Para PDFs, usar configuraci√≥n m√°s r√°pida para evitar colgadas
                if file_extension == '.pdf':
                    log(f"MCP Server: PDF detectado, usando configuraci√≥n optimizada para evitar timeouts...")
                    
                    # Opci√≥n 1: Intentar con PyPDF2 directamente (m√°s r√°pido para Cursor)
                    log(f"MCP Server: Intentando con PyPDF2 directo para mayor velocidad...")
                    try:
                        import PyPDF2
                        with open(temp_file_path, 'rb') as file:
                            pdf_reader = PyPDF2.PdfReader(file)
                            processed_content = ""
                            for page_num, page in enumerate(pdf_reader.pages):
                                page_text = page.extract_text()
                                if page_text:
                                    processed_content += f"\n--- P√°gina {page_num + 1} ---\n{page_text}\n"
                            
                            if processed_content.strip():
                                log(f"MCP Server: PyPDF2 directo exitoso, {len(processed_content)} caracteres extra√≠dos")
                                metadata = {
                                    "source": os.path.basename(temp_file_path),
                                    "file_path": temp_file_path,
                                    "file_type": ".pdf",
                                    "processed_date": datetime.now().isoformat(),
                                    "processing_method": "pypdf2_direct",
                                    "structural_info": {
                                        "total_elements": len(pdf_reader.pages),
                                        "titles_count": 0,
                                        "tables_count": 0,
                                        "lists_count": 0,
                                        "narrative_blocks": len(pdf_reader.pages),
                                        "other_elements": 0,
                                        "total_text_length": len(processed_content),
                                        "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                    }
                                }
                                log(f"MCP Server: Procesamiento con PyPDF2 directo completado")
                            else:
                                # Si PyPDF2 no extrae texto, intentar con Unstructured
                                log(f"MCP Server: PyPDF2 no extrajo texto, intentando con Unstructured...")
                                raise Exception("PyPDF2 no extrajo texto")
                    except Exception as e:
                        log(f"MCP Server: PyPDF2 directo fall√≥: {e}")
                        log(f"MCP Server: Intentando con Unstructured con timeout...")
                        
                        # Opci√≥n 2: Unstructured con timeout (fallback)
                        # Usar threading con timeout para evitar colgadas
                        import threading
                        import time
                        
                        elements = None
                        processing_error = None
                        
                        def process_pdf():
                            nonlocal elements, processing_error
                            try:
                                from rag_core import partition
                                log(f"MCP Server: Iniciando partici√≥n del PDF con strategy='fast'...")
                                log(f"MCP Server: Procesando archivo: {os.path.basename(temp_file_path)}")
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=1000)
                                log(f"MCP Server: Partici√≥n completada, {len(elements)} elementos extra√≠dos")
                            except Exception as e:
                                processing_error = e
                                log(f"MCP Server: Error en partici√≥n: {e}")
                        
                        # Ejecutar procesamiento en hilo separado con timeout
                        thread = threading.Thread(target=process_pdf)
                        thread.daemon = True
                        thread.start()
                        
                        # Esperar m√°ximo 30 segundos para el procesamiento
                        timeout_seconds = 30  # Reducido de 60 a 30 segundos para Cursor
                        
                        # Logs de progreso durante la espera
                        log(f"MCP Server: Esperando procesamiento (timeout: {timeout_seconds}s)...")
                        
                        # Esperar con logs de progreso cada 10 segundos
                        for i in range(0, timeout_seconds, 10):
                            thread.join(10)  # Esperar 10 segundos
                            if not thread.is_alive():
                                break
                            log(f"MCP Server: Procesamiento en progreso... ({i+10}/{timeout_seconds}s)")
                        
                        # Verificar si termin√≥ o si necesitamos esperar m√°s
                        if thread.is_alive():
                            remaining_time = timeout_seconds - (timeout_seconds // 10) * 10
                            if remaining_time > 0:
                                thread.join(remaining_time)
                        
                        if thread.is_alive():
                            log(f"MCP Server: Timeout en procesamiento de PDF despu√©s de {timeout_seconds} segundos")
                            # Intentar con configuraci√≥n m√≠nima
                            log(f"MCP Server: Intentando con configuraci√≥n m√≠nima...")
                            try:
                                from rag_core import partition
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=500)
                                log(f"MCP Server: Partici√≥n m√≠nima completada, {len(elements)} elementos extra√≠dos")
                            except Exception as e:
                                log(f"MCP Server: Error en partici√≥n m√≠nima: {e}")
                                return f"‚ùå **Error de timeout:** El procesamiento del PDF tard√≥ demasiado.\n\nüí° **Consejos:**\n- El PDF puede ser muy grande o complejo\n- Intenta con un PDF m√°s peque√±o\n- Verifica que el archivo no est√© corrupto"
                        
                        if processing_error:
                            log(f"MCP Server: Error en procesamiento: {processing_error}")
                            return f"‚ùå **Error procesando PDF:** {processing_error}\n\nüí° **Consejos:**\n- El archivo puede estar corrupto\n- Intenta con un PDF diferente\n- Verifica que el archivo sea accesible"
                        
                        if not elements:
                            log(f"MCP Server: No se pudieron extraer elementos del PDF")
                            # Intentar con PyPDF2 como fallback
                            log(f"MCP Server: Intentando con PyPDF2 como fallback...")
                            try:
                                import PyPDF2
                                with open(temp_file_path, 'rb') as file:
                                    pdf_reader = PyPDF2.PdfReader(file)
                                    processed_content = ""
                                    for page_num, page in enumerate(pdf_reader.pages):
                                        page_text = page.extract_text()
                                        if page_text:
                                            processed_content += f"\n--- P√°gina {page_num + 1} ---\n{page_text}\n"
                                    
                                    if processed_content.strip():
                                        log(f"MCP Server: PyPDF2 fallback exitoso, {len(processed_content)} caracteres extra√≠dos")
                                        metadata = {
                                            "source": os.path.basename(temp_file_path),
                                            "file_path": temp_file_path,
                                            "file_type": ".pdf",
                                            "processed_date": datetime.now().isoformat(),
                                            "processing_method": "pypdf2_fallback",
                                            "structural_info": {
                                                "total_elements": len(pdf_reader.pages),
                                                "titles_count": 0,
                                                "tables_count": 0,
                                                "lists_count": 0,
                                                "narrative_blocks": len(pdf_reader.pages),
                                                "other_elements": 0,
                                                "total_text_length": len(processed_content),
                                                "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                            }
                                        }
                                    else:
                                        return f"‚ùå **Error:** No se pudo extraer texto del PDF con ning√∫n m√©todo.\n\nüí° **Consejos:**\n- El PDF puede estar escaneado (solo im√°genes)\n- El archivo puede estar corrupto\n- Intenta con un PDF diferente"
                            except ImportError:
                                log(f"MCP Server: PyPDF2 no disponible")
                                return f"‚ùå **Error:** No se pudieron extraer elementos del PDF.\n\nüí° **Consejos:**\n- El archivo puede estar vac√≠o o corrupto\n- Intenta con un PDF diferente"
                            except Exception as e:
                                log(f"MCP Server: Error en PyPDF2 fallback: {e}")
                                return f"‚ùå **Error:** No se pudieron extraer elementos del PDF.\n\nüí° **Consejos:**\n- El archivo puede estar vac√≠o o corrupto\n- Intenta con un PDF diferente"
                        else:
                            log(f"MCP Server: Procesando elementos extra√≠dos...")
                            from rag_core import process_unstructured_elements, extract_structural_metadata
                            processed_content = process_unstructured_elements(elements)
                            log(f"MCP Server: Elementos procesados, {len(processed_content)} caracteres extra√≠dos")
                            
                            metadata = extract_structural_metadata(elements, temp_file_path)
                            metadata["processing_method"] = "unstructured_fast_pdf"
                            log(f"MCP Server: Metadatos estructurales extra√≠dos")
                else:
                    # Para otros formatos, usar el procesamiento normal
                    processed_content, metadata = load_document_with_fallbacks(temp_file_path)
                
                if not processed_content or processed_content.isspace():
                    log(f"MCP Server: Advertencia: Archivo descargado pero no se pudo extraer contenido: {url}")
                    return f"Advertencia: El archivo de la URL '{url}' fue descargado, pero no se pudo extraer contenido de texto."
                
                log(f"MCP Server: Archivo descargado y procesado exitosamente ({len(processed_content)} caracteres)")
                
                # Guardar copia procesada
                log(f"MCP Server: Guardando copia procesada...")
                processing_method = metadata.get("processing_method", "unstructured_enhanced")
                domain = parsed_url.netloc.replace('.', '_')
                path = parsed_url.path.replace('/', '_').replace('.', '_')
                if not path or path == '_':
                    path = 'homepage'
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{domain}_{path}_{timestamp}_{processing_method}.md"
                processed_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
                
                try:
                    ensure_converted_docs_directory()
                    with open(processed_filepath, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    log(f"MCP Server: Copia procesada guardada en: {processed_filepath}")
                except Exception as e:
                    log(f"MCP Server Advertencia: No se pudo guardar copia procesada: {e}")
                    processed_filepath = ""
                
                # Enriquecer metadatos
                enhanced_metadata = metadata.copy()
                enhanced_metadata.update({
                    "source": url,
                    "domain": parsed_url.netloc,
                    "input_type": "url_download",
                    "converted_to_md": processed_filepath if processed_filepath else "No",
                    "server_processed_date": datetime.now().isoformat()
                })
                
                # Usar procesamiento mejorado
                log(f"MCP Server: A√±adiendo contenido a la base de conocimientos...")
                add_text_to_knowledge_base_enhanced(
                    processed_content, 
                    rag_state["vector_store"], 
                    enhanced_metadata, 
                    use_semantic_chunking=True
                )
                
                # Construir respuesta informativa
                structural_info = enhanced_metadata.get("structural_info", {})
                
                response_parts = [
                    f"‚úÖ **Archivo descargado y procesado exitosamente**",
                    f"üåê **URL:** {url}",
                    f"üìÑ **Archivo:** {os.path.basename(parsed_url.path)}",
                    f"üìã **Tipo:** {file_extension.upper()}",
                    f"üîß **M√©todo:** {processing_method.replace('_', ' ').title()}"
                ]
                
                # A√±adir informaci√≥n estructural si est√° disponible
                if structural_info:
                    response_parts.extend([
                        f"üìä **Estructura del documento:**",
                        f"   ‚Ä¢ Elementos totales: {structural_info.get('total_elements', 'N/A')}",
                        f"   ‚Ä¢ T√≠tulos: {structural_info.get('titles_count', 'N/A')}",
                        f"   ‚Ä¢ Tablas: {structural_info.get('tables_count', 'N/A')}",
                        f"   ‚Ä¢ Listas: {structural_info.get('lists_count', 'N/A')}",
                        f"   ‚Ä¢ Bloques narrativos: {structural_info.get('narrative_blocks', 'N/A')}"
                    ])
                
                if processed_filepath:
                    response_parts.append(f"üíæ **Copia guardada:** {processed_filepath}")
                
                response_parts.append(f"üìö **Estado:** A√±adido a la base de conocimientos con chunking sem√°ntico")
                
                log(f"MCP Server: Procesamiento de URL completado exitosamente")
                return "\n".join(response_parts)
                
            finally:
                # Limpiar archivo temporal
                try:
                    os.unlink(temp_file_path)
                    log(f"MCP Server: Archivo temporal eliminado: {temp_file_path}")
                except Exception as e:
                    log(f"MCP Server Advertencia: No se pudo eliminar archivo temporal: {e}")
        
        else:
            # Procesamiento tradicional para p√°ginas web
            log(f"MCP Server: Procesando contenido web con MarkItDown...")
            
            # Usar MarkItDown para procesar la URL directamente con timeout
            try:
                result = md_converter.convert_url(url)
                markdown_content = result.text_content
            except Exception as e:
                log(f"MCP Server: Error con MarkItDown, intentando descarga directa: {e}")
                # Fallback: intentar descarga directa
                import requests
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                markdown_content = response.text

            if not markdown_content or markdown_content.isspace():
                log(f"MCP Server: Advertencia: URL procesada pero no se pudo extraer contenido: {url}")
                return f"Advertencia: La URL '{url}' fue procesada, pero no se pudo extraer contenido de texto."

            log(f"MCP Server: Contenido de URL convertido exitosamente ({len(markdown_content)} caracteres)")
            
            # Guardar copia en Markdown
            log(f"MCP Server: Guardando copia Markdown...")
            
            # Crear nombre de archivo basado en la URL
            domain = parsed_url.netloc.replace('.', '_')
            path = parsed_url.path.replace('/', '_').replace('.', '_')
            if not path or path == '_':
                path = 'homepage'
            
            # Crear nombre de archivo √∫nico
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{domain}_{path}_{timestamp}_markitdown.md"
            md_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
            
            # Guardar el contenido
            try:
                ensure_converted_docs_directory()
                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                log(f"MCP Server: Copia Markdown guardada en: {md_filepath}")
            except Exception as e:
                log(f"MCP Server Advertencia: No se pudo guardar copia Markdown: {e}")
                md_filepath = ""
            
            # A√±adir contenido a la base de conocimientos
            log(f"MCP Server: A√±adiendo contenido a la base de conocimientos...")
            
            # Crear metadatos espec√≠ficos de la URL
            url_metadata = {
                "source": url,
                "domain": parsed_url.netloc,
                "input_type": "url_web",
                "processed_date": datetime.now().isoformat(),
                "processing_method": "markitdown",
                "converted_to_md": md_filepath if md_filepath else "No"
            }
            
            # A√±adir directamente con metadatos
            add_text_to_knowledge_base(markdown_content, rag_state["vector_store"], url_metadata)
            
            # Informaci√≥n sobre el proceso completado
            response_parts = [
                f"‚úÖ **Contenido web procesado exitosamente**",
                f"üåê **URL:** {url}",
                f"üåç **Dominio:** {parsed_url.netloc}",
                f"üîß **M√©todo:** MarkItDown"
            ]
            
            if md_filepath:
                response_parts.append(f"üíæ **Copia guardada:** {md_filepath}")
            
            response_parts.append(f"üìö **Estado:** A√±adido a la base de conocimientos")
            
            log(f"MCP Server: Procesamiento de URL completado exitosamente")
            return "\n".join(response_parts)

    except requests.exceptions.Timeout:
        log(f"MCP Server: Timeout al procesar URL: {url}")
        return f"‚ùå **Error de timeout:** La URL '{url}' tard√≥ demasiado en responder.\n\nüí° **Consejos:**\n- Verifica tu conexi√≥n a internet\n- Intenta m√°s tarde\n- La URL puede estar temporalmente lenta"
    
    except requests.exceptions.ConnectionError:
        log(f"MCP Server: Error de conexi√≥n al procesar URL: {url}")
        return f"‚ùå **Error de conexi√≥n:** No se pudo conectar a la URL '{url}'.\n\nüí° **Consejos:**\n- Verifica tu conexi√≥n a internet\n- La URL puede no estar disponible\n- Intenta m√°s tarde"
    
    except Exception as e:
        log(f"MCP Server: Error procesando URL '{url}': {e}")
        error_msg = f"‚ùå **Error procesando URL '{url}':** {e}"
        
        # Proporcionar informaci√≥n m√°s √∫til para el agente
        if "404" in str(e) or "Not Found" in str(e):
            error_msg += "\n\nüí° **Consejo:** La URL no existe o no es accesible. Verifica que la URL sea correcta."
        elif "timeout" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** La p√°gina tard√≥ demasiado en cargar. Intenta m√°s tarde o verifica tu conexi√≥n a internet."
        elif "permission" in str(e).lower() or "403" in str(e):
            error_msg += "\n\nüí° **Consejo:** No tienes permisos para acceder a esta p√°gina. Algunas p√°ginas bloquean el acceso autom√°tico."
        elif "youtube" in url.lower() and "transcript" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Este video de YouTube no tiene transcripci√≥n disponible o est√° deshabilitada."
        elif "ssl" in str(e).lower() or "certificate" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Problema con el certificado SSL de la p√°gina. Intenta con una URL diferente."
        elif "download" in str(e).lower() or "connection" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Error al descargar el archivo. Verifica que la URL sea accesible y el archivo exista."
        elif "unstructured" in str(e).lower():
            error_msg += "\n\nüí° **Consejo:** Error en el procesamiento del documento. El archivo puede estar corrupto o ser muy grande."
        
        return error_msg

@mcp.tool()
def ask_rag(query: str) -> str:
    """
    Searches the knowledge base and generates an AI-powered answer based on stored information.
    Use this when you need to retrieve specific information that has been previously stored.
    
    Examples of when to use:
    - Looking up facts, definitions, or explanations that were previously added
    - Finding information from processed documents
    - Retrieving context about specific topics
    - Getting answers based on stored research or notes
    
    The response will include the answer plus a list of sources used to generate it.
    If no relevant information is found, the AI will indicate this clearly.

    Args:
        query: The specific question or information request to search for in the knowledge base.
    """
    log(f"MCP Server: Procesando pregunta: '{query[:50]}{'...' if len(query) > 50 else ''}'")
    initialize_rag()

    try:
        log(f"MCP Server: Buscando informaci√≥n relevante en la base de conocimientos...")
        qa_chain = rag_state["qa_chain"]
        response = qa_chain.invoke({"query": query})
        
        # Obtener la respuesta principal
        answer = response.get("result", "No se pudo obtener una respuesta.")
        
        # Verificar si se encontr√≥ informaci√≥n relevante
        source_documents = response.get("source_documents", [])
        
        if not source_documents:
            return "‚ùå **No se encontr√≥ informaci√≥n relevante** en la base de conocimientos para responder a tu pregunta.\n\nüí° **Sugerencias:**\n- Verifica que hayas a√±adido documentos o texto relacionado con este tema\n- Intenta reformular tu pregunta con palabras diferentes\n- Usa la herramienta `learn_text` o `learn_document` para a√±adir informaci√≥n sobre este tema"
        
        # Construir respuesta mejorada con informaci√≥n de fuentes
        enhanced_answer = f"ü§ñ **Respuesta:**\n{answer}\n\n"
        
        # A√±adir informaci√≥n de fuentes con m√°s detalles
        if source_documents:
            enhanced_answer += "üìö **Fuentes de informaci√≥n utilizadas:**\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "Fuente desconocida")
                
                # --- Mejoramos la informaci√≥n de la fuente ---
                source_info = f"   {i}. **{source_name}**"
                
                # A√±adir ruta completa si es un documento
                file_path = metadata.get("file_path")
                if file_path:
                    source_info += f"\n      - **Ruta:** `{file_path}`"
                
                # A√±adir tipo de archivo si est√° disponible
                file_type = metadata.get("file_type")
                if file_type:
                    source_info += f"\n      - **Tipo:** {file_type.upper()}"
                
                # A√±adir m√©todo de procesamiento si est√° disponible
                processing_method = metadata.get("processing_method")
                if processing_method:
                    method_display = processing_method.replace('_', ' ').title()
                    source_info += f"\n      - **Procesamiento:** {method_display}"
                
                # A√±adir informaci√≥n estructural si est√° disponible
                structural_info = metadata.get("structural_info")
                if structural_info:
                    source_info += f"\n      - **Estructura:** {structural_info.get('total_elements', 'N/A')} elementos"
                    titles_count = structural_info.get('titles_count', 0)
                    tables_count = structural_info.get('tables_count', 0)
                    lists_count = structural_info.get('lists_count', 0)
                    if titles_count > 0 or tables_count > 0 or lists_count > 0:
                        structure_details = []
                        if titles_count > 0:
                            structure_details.append(f"{titles_count} t√≠tulos")
                        if tables_count > 0:
                            structure_details.append(f"{tables_count} tablas")
                        if lists_count > 0:
                            structure_details.append(f"{lists_count} listas")
                        source_info += f" ({', '.join(structure_details)})"
                
                # Reconstruir informaci√≥n estructural desde metadatos planos
                structural_elements = []
                titles_count = metadata.get("structural_titles_count", 0)
                tables_count = metadata.get("structural_tables_count", 0)
                lists_count = metadata.get("structural_lists_count", 0)
                total_elements = metadata.get("structural_total_elements", 0)
                
                if total_elements > 0:
                    source_info += f"\n      - **Estructura:** {total_elements} elementos"
                    if titles_count > 0 or tables_count > 0 or lists_count > 0:
                        structure_details = []
                        if titles_count > 0:
                            structure_details.append(f"{titles_count} t√≠tulos")
                        if tables_count > 0:
                            structure_details.append(f"{tables_count} tablas")
                        if lists_count > 0:
                            structure_details.append(f"{lists_count} listas")
                        source_info += f" ({', '.join(structure_details)})"
                
                # A√±adir informaci√≥n de chunk si est√° disponible
                chunk_index = metadata.get("chunk_index")
                total_chunks = metadata.get("total_chunks")
                if chunk_index is not None and total_chunks:
                    source_info += f"\n      - **Fragmento:** {chunk_index + 1} de {total_chunks}"
                
                # A√±adir fecha de procesamiento
                processed_date = metadata.get("processed_date")
                if processed_date:
                    try:
                        date_obj = datetime.fromisoformat(processed_date.replace('Z', '+00:00'))
                        readable_date = date_obj.strftime("%d/%m/%Y %H:%M")
                        source_info += f"\n      - **Procesado:** {readable_date}"
                    except:
                        pass
                
                # A√±adir un fragmento del contenido relevante
                content_snippet = doc.page_content.strip().replace('\n', ' ')
                source_info += f"\n      - **Fragmento Relevante:**\n        > _{content_snippet[:150]}{'...' if len(content_snippet) > 150 else ''}_"
                
                enhanced_answer += source_info + "\n\n"
        
        # A√±adir informaci√≥n sobre la calidad de la respuesta
        num_sources = len(source_documents)
        if num_sources >= 3:
            enhanced_answer += "\n‚úÖ **Alta confianza:** Respuesta basada en m√∫ltiples fuentes"
        elif num_sources == 2:
            enhanced_answer += "\n‚ö†Ô∏è **Confianza media:** Respuesta basada en 2 fuentes"
        else:
            enhanced_answer += "\n‚ö†Ô∏è **Confianza limitada:** Respuesta basada en 1 fuente"
        
        # A√±adir informaci√≥n sobre el procesamiento si hay documentos con metadatos estructurales
        enhanced_docs = [doc for doc in source_documents if hasattr(doc, 'metadata') and doc.metadata.get("processing_method") == "unstructured_enhanced"]
        if enhanced_docs:
            enhanced_answer += f"\nüß† **Procesamiento inteligente:** {len(enhanced_docs)} fuentes procesadas con Unstructured (preservaci√≥n de estructura)"
        
        log(f"MCP Server: Respuesta generada exitosamente con {len(source_documents)} fuentes")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCP Server: Error procesando pregunta: {e}")
        return f"‚ùå **Error al procesar la pregunta:** {e}\n\nüí° **Sugerencias:**\n- Verifica que el sistema RAG est√© correctamente inicializado\n- Intenta reformular tu pregunta\n- Si el problema persiste, reinicia el servidor"

# --- Punto de Entrada para Correr el Servidor ---
if __name__ == "__main__":
    log("Iniciando servidor MCP RAG...")
    warm_up_rag_system()  # Calentamos el sistema al arrancar
    mcp.run(transport='stdio') 