import os
from datetime import datetime
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP
from markitdown import MarkItDown
from urllib.parse import urlparse

# --- å¯¼å…¥ RAG æ ¸å¿ƒæ¨¡å— ---
from rag_core import (
    add_text_to_knowledge_base,           # å°†æ–‡æœ¬æ·»åŠ åˆ°çŸ¥è¯†åº“çš„å‡½æ•°
    add_text_to_knowledge_base_enhanced,  # å¢å¼ºç‰ˆæ–‡æœ¬æ·»åŠ å‡½æ•°
    load_document_with_fallbacks,         # å¸¦å›é€€æœºåˆ¶çš„æ–‡æ¡£åŠ è½½å‡½æ•°
    get_qa_chain,                         # è·å–é—®ç­”é“¾çš„å‡½æ•°
    get_vector_store,                     # è·å–å‘é‡å­˜å‚¨çš„å‡½æ•°
    search_with_metadata_filters,         # å¸¦å…ƒæ•°æ®è¿‡æ»¤å™¨çš„æœç´¢å‡½æ•°
    create_metadata_filter,               # åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å™¨çš„å‡½æ•°
    get_document_statistics,              # è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    get_cache_stats,                      # è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    print_cache_stats,                    # æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯çš„å‡½æ•°
    clear_embedding_cache,                # æ¸…é™¤åµŒå…¥ç¼“å­˜çš„å‡½æ•°
    log,  # å¯¼å…¥æ—¥å¿—å‡½æ•°
    optimize_vector_store,
    get_vector_store_stats,
    reindex_vector_store,
    get_optimal_vector_store_profile,
    load_document_with_elements
)

# --- æœåŠ¡å™¨åˆå§‹åŒ–ä¸é…ç½® ---
load_dotenv()
mcp = FastMCP("ragmcp")

rag_state = {}  # çŠ¶æ€åªä¿å­˜å·²å°±ç»ªçš„ç»„ä»¶

md_converter = MarkItDown()  # åˆå§‹åŒ– MarkItDown è½¬æ¢å™¨ï¼ˆç”¨äº URL å¤„ç†ï¼‰

# ç”¨äºä¿å­˜ Markdown å‰¯æœ¬çš„æ–‡ä»¶å¤¹
CONVERTED_DOCS_DIR = "./converted_docs"

def warm_up_rag_system():
    """
    é¢„åŠ è½½ RAG ç³»ç»Ÿçš„é‡å‹ç»„ä»¶ï¼Œé¿å…é¦–æ¬¡è°ƒç”¨å·¥å…·æ—¶çš„å»¶è¿Ÿå’Œå†²çªã€‚
    """
    if "warmed_up" in rag_state:
        return
    
    log("MCPæœåŠ¡å™¨: æ­£åœ¨é¢„çƒ­RAGç³»ç»Ÿ...")
    log("MCPæœåŠ¡å™¨: æ­£åœ¨é¢„åŠ è½½åµŒå…¥æ¨¡å‹åˆ°å†…å­˜...")
    
    # æ­¤è°ƒç”¨å¼ºåˆ¶åŠ è½½åµŒå…¥æ¨¡å‹
    get_vector_store()
    
    rag_state["warmed_up"] = True
    log("MCPæœåŠ¡å™¨: RAGç³»ç»Ÿå·²é¢„çƒ­å¹¶å‡†å¤‡å°±ç»ªã€‚")

def ensure_converted_docs_directory():
    """ç¡®ä¿å­˜åœ¨ç”¨äºå­˜å‚¨è½¬æ¢æ–‡æ¡£çš„æ–‡ä»¶å¤¹ã€‚"""
    if not os.path.exists(CONVERTED_DOCS_DIR):
        os.makedirs(CONVERTED_DOCS_DIR)
        log(f"MCPæœåŠ¡å™¨: å·²åˆ›å»ºè½¬æ¢æ–‡æ¡£æ–‡ä»¶å¤¹: {CONVERTED_DOCS_DIR}")

def save_processed_copy(file_path: str, processed_content: str, processing_method: str = "éç»“æ„åŒ–") -> str:
    """
    ä¿å­˜å¤„ç†åçš„æ–‡æ¡£å‰¯æœ¬ä¸º Markdown æ ¼å¼ã€‚
    
    å‚æ•°ï¼š
        file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
        processed_content: å¤„ç†åçš„å†…å®¹
        processing_method: ä½¿ç”¨çš„å¤„ç†æ–¹æ³•
    
    è¿”å›ï¼š
        ä¿å­˜çš„ Markdown æ–‡ä»¶è·¯å¾„
    """
    ensure_converted_docs_directory()
    
    # è·å–åŸå§‹æ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰
    original_filename = os.path.basename(file_path)
    name_without_ext = os.path.splitext(original_filename)[0]
    
    # åˆ›å»ºåŒ…å«æ–¹æ³•ä¿¡æ¯çš„ Markdown æ–‡ä»¶å
    md_filename = f"{name_without_ext}_{processing_method}.md"
    md_filepath = os.path.join(CONVERTED_DOCS_DIR, md_filename)
    
    # ä¿å­˜å†…å®¹åˆ° Markdown æ–‡ä»¶
    try:
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(processed_content)
        log(f"MCPæœåŠ¡å™¨: å·²ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {md_filepath}")
        return md_filepath
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨è­¦å‘Š: æ— æ³•ä¿å­˜å¤„ç†åçš„å‰¯æœ¬: {e}")
        return ""

def initialize_rag():
    """
    ä½¿ç”¨æ ¸å¿ƒåˆå§‹åŒ– RAG ç³»ç»Ÿçš„æ‰€æœ‰ç»„ä»¶ã€‚
    """
    if "initialized" in rag_state:
        return

    log("MCPæœåŠ¡å™¨: é€šè¿‡æ ¸å¿ƒåˆå§‹åŒ–RAGç³»ç»Ÿ...")
    
    # ä»æ ¸å¿ƒè·å–å‘é‡å­˜å‚¨å’Œ QA é“¾
    vector_store = get_vector_store()
    qa_chain = get_qa_chain(vector_store)
    
    rag_state["vector_store"] = vector_store
    rag_state["qa_chain"] = qa_chain
    rag_state["initialized"] = True
    log("MCPæœåŠ¡å™¨: RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸã€‚")

# --- å·¥å…·å®ç° ---

@mcp.tool()
def learn_text(text: str, source_name: str = "æ‰‹åŠ¨è¾“å…¥") -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æ·»åŠ ä¸€æ®µæ–°æ–‡æœ¬ä»¥ä¾›å°†æ¥å‚è€ƒã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - æ·»åŠ äº‹å®ã€å®šä¹‰æˆ–è§£é‡Š
    - å­˜å‚¨å¯¹è¯ä¸­çš„é‡è¦ä¿¡æ¯
    - ä¿å­˜ç ”ç©¶å‘ç°æˆ–ç¬”è®°
    - æ·»åŠ ç‰¹å®šä¸»é¢˜çš„ä¸Šä¸‹æ–‡

    å‚æ•°ï¼š
        text: è¦å­¦ä¹ å¹¶å­˜å‚¨åœ¨çŸ¥è¯†åº“ä¸­çš„æ–‡æœ¬å†…å®¹ã€‚
        source_name: æ¥æºçš„æè¿°æ€§åç§°ï¼ˆä¾‹å¦‚ "ç”¨æˆ·ç¬”è®°", "ç ”ç©¶è®ºæ–‡", "å¯¹è¯æ‘˜è¦"ï¼‰ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†æ¥è‡ª {source_name} çš„æ–‡æœ¬ï¼Œå…± {len(text)} å­—ç¬¦")
    initialize_rag()
    
    try:
        # åˆ›å»ºæºå…ƒæ•°æ®
        source_metadata = {
            "source": source_name,
            "input_type": "æ‰‹åŠ¨æ–‡æœ¬",
            "processed_date": datetime.now().isoformat()
        }
        # ä½¿ç”¨æ ¸å¿ƒå‡½æ•°æ·»åŠ æ–‡æœ¬åŠå…ƒæ•°æ®
        add_text_to_knowledge_base(text, rag_state["vector_store"], source_metadata)
        log(f"MCPæœåŠ¡å™¨: æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“")
        return f"æ–‡æœ¬å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚ç‰‡æ®µ: '{text[:70]}...' (æ¥æº: {source_name})"
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}")
        return f"æ·»åŠ æ–‡æœ¬æ—¶å‡ºé”™: {e}"

@mcp.tool()
def learn_document(file_path: str) -> str:
    """
    è¯»å–å¹¶å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼Œé‡‡ç”¨é«˜çº§ Unstructured å¤„ç†å’ŒçœŸå®è¯­ä¹‰åˆ†å—ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - å¤„ç†å¤æ‚å¸ƒå±€çš„è®ºæ–‡æˆ–æ–‡ç« 
    - æ·»åŠ åŒ…å«è¡¨æ ¼å’Œåˆ—è¡¨çš„æŠ¥å‘Šæˆ–æ‰‹å†Œå†…å®¹
    - å¯¼å…¥å¸¦æ ¼å¼çš„ç”µå­è¡¨æ ¼æ•°æ®
    - å°†æ¼”ç¤ºæ–‡ç¨¿è½¬æ¢ä¸ºå¯æœç´¢çŸ¥è¯†
    - å¤„ç†å¸¦ OCR çš„æ‰«ææ–‡æ¡£

    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼šPDFã€DOCXã€PPTXã€XLSXã€TXTã€HTMLã€CSVã€JSONã€XMLã€ODTã€ODPã€ODSã€RTFã€
    å›¾åƒï¼ˆPNGã€JPGã€TIFFã€BMPï¼Œå¸¦ OCRï¼‰ã€é‚®ä»¶ï¼ˆEMLã€MSGï¼‰ç­‰ 25+ ç§æ ¼å¼ã€‚

    é«˜çº§åŠŸèƒ½ï¼š
    - åŸºäºæ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€ç« èŠ‚ã€åˆ—è¡¨ï¼‰çš„çœŸå®è¯­ä¹‰åˆ†å—
    - æ™ºèƒ½æ–‡æ¡£ç»“æ„ä¿å­˜ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€è¡¨æ ¼ï¼‰
    - è‡ªåŠ¨å»å™ªï¼ˆé¡µçœ‰ã€é¡µè„šã€æ— å…³å†…å®¹ï¼‰
    - ç»“æ„åŒ–å…ƒæ•°æ®æå–
    - å¼ºå¤§å›é€€ç³»ç»Ÿï¼Œé€‚ç”¨äºä»»ä½•æ–‡æ¡£ç±»å‹
    - é€šè¿‡è¯­ä¹‰è¾¹ç•Œå¢å¼ºä¸Šä¸‹æ–‡ä¿å­˜

    æ–‡æ¡£å°†é€šè¿‡çœŸå®è¯­ä¹‰åˆ†å—æ™ºèƒ½å¤„ç†ï¼Œå¹¶ä¸å¢å¼ºå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨ã€‚
    å¤„ç†åçš„å‰¯æœ¬å°†ä¿å­˜ä»¥ä¾›éªŒè¯ã€‚

    å‚æ•°ï¼š
        file_pathï¼šè¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶çš„ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: å¼€å§‹é«˜çº§æ–‡æ¡£å¤„ç†: {file_path}")
    log(f"MCPæœåŠ¡å™¨: è°ƒè¯• - æ¥æ”¶åˆ°è·¯å¾„: {repr(file_path)}")
    log(f"MCPæœåŠ¡å™¨: è°ƒè¯• - æ£€æŸ¥ç»å¯¹è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.abspath(file_path)}")
    initialize_rag()  # ç¡®ä¿ RAG ç³»ç»Ÿå·²å°±ç»ª
    
    try:
        if not os.path.exists(file_path):
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°æ–‡ä»¶è·¯å¾„: {file_path}")
            return f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ '{file_path}'"

        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ç”¨é«˜çº§Unstructuredç³»ç»Ÿå¤„ç†æ–‡æ¡£...")

        # ä½¿ç”¨æ–°ç³»ç»Ÿå¤„ç†ç»“æ„åŒ–å…ƒç´ 
        processed_content, metadata, structural_elements = load_document_with_elements(file_path)

        if not processed_content or processed_content.isspace():
            log(f"MCPæœåŠ¡å™¨: è­¦å‘Š: æ–‡æ¡£å·²å¤„ç†ä½†æœªèƒ½æå–å†…å®¹: {file_path}")
            return f"è­¦å‘Š: æ–‡æ¡£ '{file_path}' å·²å¤„ç†ï¼Œä½†æœªèƒ½æå–æ–‡æœ¬å†…å®¹ã€‚"

        log(f"MCPæœåŠ¡å™¨: æ–‡æ¡£å¤„ç†æˆåŠŸï¼ˆ{len(processed_content)} å­—ç¬¦ï¼‰")

        # ä¿å­˜å¤„ç†å‰¯æœ¬
        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä¿å­˜å¤„ç†å‰¯æœ¬...")
        processing_method = metadata.get("processing_method", "æœªçŸ¥")
        saved_copy_path = save_processed_copy(file_path, processed_content, processing_method)

        # æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“ï¼Œä½¿ç”¨çœŸå®è¯­ä¹‰åˆ†å—
        log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“ï¼ˆå«ç»“æ„åŒ–å…ƒæ•°æ®ï¼‰...")

        # ä½¿ç”¨ç»“æ„åŒ–å…ƒç´ è¿›è¡ŒçœŸå®è¯­ä¹‰åˆ†å—
        add_text_to_knowledge_base_enhanced(
            processed_content,
            rag_state["vector_store"],
            metadata,
            use_semantic_chunking=True,
            structural_elements=structural_elements
        )

        log(f"MCPæœåŠ¡å™¨: å¤„ç†å®Œæˆ - æ–‡æ¡£å·²æˆåŠŸå¤„ç†")

        # æ„å»ºè¯¦ç»†å“åº”
        file_name = os.path.basename(file_path)
        file_type = metadata.get("file_type", "æœªçŸ¥")
        processing_method = metadata.get("processing_method", "æœªçŸ¥")

        # åˆ†å—ä¿¡æ¯
        chunking_info = ""
        if structural_elements and len(structural_elements) > 1:
            chunking_info = f"ğŸ§  **é«˜çº§è¯­ä¹‰åˆ†å—**ï¼Œå…± {len(structural_elements)} ä¸ªç»“æ„åŒ–å…ƒç´ "
        elif metadata.get("structural_info", {}).get("total_elements", 0) > 1:
            chunking_info = f"ğŸ“Š **å¢å¼ºè¯­ä¹‰åˆ†å—**ï¼ŒåŸºäºç»“æ„åŒ–å…ƒæ•°æ®"
        else:
            chunking_info = f"ğŸ“ **ä¼ ç»Ÿåˆ†å—** ä¼˜åŒ–"

        return f"""âœ… **æ–‡æ¡£å¤„ç†æˆåŠŸ**
ğŸ“„ **æ–‡ä»¶:** {file_name}
ğŸ“‹ **ç±»å‹:** {file_type.upper()}
ğŸ”§ **å¤„ç†æ–¹æ³•:** {processing_method}
{chunking_info}
ğŸ“Š **å¤„ç†å­—ç¬¦æ•°:** {len(processed_content):,}
ğŸ’¾ **å‰¯æœ¬ä¿å­˜è·¯å¾„:** {saved_copy_path if saved_copy_path else "æ— "}"""

    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†æ–‡æ¡£ '{file_path}' æ—¶å‡ºé”™: {e}")
        return f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}"

@mcp.tool()
def learn_from_url(url: str) -> str:
    """
    å¤„ç† URL å†…å®¹ï¼ˆç½‘é¡µæˆ– YouTube è§†é¢‘ï¼‰ï¼Œå¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - æ·»åŠ æ–°é—»æ–‡ç« æˆ–åšå®¢å†…å®¹
    - å¤„ç† YouTube è§†é¢‘æ–‡å­—è®°å½•
    - ä»ç½‘é¡µå¯¼å…¥ä¿¡æ¯
    - å°†ç½‘é¡µå†…å®¹è½¬æ¢ä¸ºå¯æœç´¢çŸ¥è¯†
    - ç›´æ¥ä» URL å¤„ç†æ–‡æ¡£

    æ”¯æŒçš„ URL ç±»å‹ï¼š
    - ç½‘é¡µï¼ˆHTML å†…å®¹ï¼‰
    - YouTube è§†é¢‘ï¼ˆæ–‡å­—è®°å½•ï¼‰
    - MarkItDown å¯å¤„ç†çš„ä»»ä½• URL
    - ç›´æ¥ä¸‹è½½æ–‡ä»¶ï¼ˆPDFã€DOCX ç­‰ï¼‰- ä½¿ç”¨å¢å¼ºçš„ Unstructured å¤„ç†

    å†…å®¹å°†è¢«æ™ºèƒ½å¤„ç†å¹¶ä¸å¢å¼ºå…ƒæ•°æ®ä¸€èµ·å­˜å‚¨ã€‚
    å¤„ç†åçš„å†…å®¹å‰¯æœ¬å°†ä¿å­˜ä»¥ä¾›éªŒè¯ã€‚

    å‚æ•°ï¼š
        urlï¼šè¦å¤„ç†çš„ç½‘é¡µæˆ–è§†é¢‘çš„ URLã€‚
    """
    log(f"MCPæœåŠ¡å™¨: å¼€å§‹å¤„ç†URL: {url}")
    initialize_rag()
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºç›´æ¥æ–‡ä»¶ä¸‹è½½é“¾æ¥
        parsed_url = urlparse(url)
        file_extension = os.path.splitext(parsed_url.path)[1].lower()
        
        # æ”¯æŒå¢å¼ºå¤„ç†çš„æ‰©å±•ååˆ—è¡¨
        enhanced_extensions = ['.pdf', '.docx', '.doc', '.pptx', '.ppt', '.xlsx', '.xls', 
                              '.txt', '.html', '.htm', '.csv', '.json', '.xml', '.rtf',
                              '.odt', '.odp', '.ods', '.md', '.yaml', '.yml']
        
        if file_extension in enhanced_extensions:
            log(f"MCPæœåŠ¡å™¨: æ£€æµ‹åˆ°å¯ä¸‹è½½æ–‡ä»¶ï¼ˆ{file_extension}ï¼‰ï¼Œä½¿ç”¨å¢å¼ºå¤„ç†...")
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å
            import tempfile
            import requests
            import signal
            
            # é…ç½®ä¸‹è½½è¶…æ—¶
            timeout_seconds = 30
            
            # ä¸‹è½½æ–‡ä»¶å¹¶è®¾ç½®è¶…æ—¶
            log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä¸‹è½½æ–‡ä»¶ï¼Œè¶…æ—¶æ—¶é—´ {timeout_seconds} ç§’...")
            response = requests.get(url, stream=True, timeout=timeout_seconds)
            response.raise_for_status()
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
                for chunk in response.iter_content(chunk_size=8192):
                    temp_file.write(chunk)
                temp_file_path = temp_file.name
            
            log(f"MCPæœåŠ¡å™¨: æ–‡ä»¶å·²ä¸´æ—¶ä¸‹è½½åˆ°: {temp_file_path}")
            
            try:
                # ä½¿ç”¨å¢å¼ºå¤„ç†å¹¶è®¾ç½®è¶…æ—¶
                log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ç”¨Unstructuredå¤„ç†ï¼ˆå¤§PDFå¯èƒ½éœ€æ•°åˆ†é’Ÿï¼‰...")
                
                # é’ˆå¯¹PDFä½¿ç”¨æ›´å¿«é…ç½®é¿å…è¶…æ—¶
                if file_extension == '.pdf':
                    log(f"MCPæœåŠ¡å™¨: æ£€æµ‹åˆ°PDFï¼Œä½¿ç”¨ä¼˜åŒ–é…ç½®é¿å…è¶…æ—¶...")
                    
                    # é€‰é¡¹1ï¼šç›´æ¥ä½¿ç”¨PyPDF2ï¼ˆå¯¹Cursoræ›´å¿«ï¼‰
                    log(f"MCPæœåŠ¡å™¨: å°è¯•ç”¨PyPDF2ç›´æ¥å¤„ç†ä»¥æå‡é€Ÿåº¦...")
                    try:
                        import PyPDF2
                        with open(temp_file_path, 'rb') as file:
                            pdf_reader = PyPDF2.PdfReader(file)
                            processed_content = ""
                            for page_num, page in enumerate(pdf_reader.pages):
                                page_text = page.extract_text()
                                if page_text:
                                    processed_content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                            
                            if processed_content.strip():
                                log(f"MCPæœåŠ¡å™¨: PyPDF2ç›´æ¥å¤„ç†æˆåŠŸï¼Œæå–äº†{len(processed_content)}å­—ç¬¦")
                                metadata = {
                                    "source": os.path.basename(temp_file_path),
                                    "file_path": temp_file_path,
                                    "file_type": ".pdf",
                                    "processed_date": datetime.now().isoformat(),
                                    "processing_method": "pypdf2_direct",
                                    "structural_info": {
                                        "total_elements": len(pdf_reader.pages),
                                        "titles_count": 0,
                                        "tables_count": 0,
                                        "lists_count": 0,
                                        "narrative_blocks": len(pdf_reader.pages),
                                        "other_elements": 0,
                                        "total_text_length": len(processed_content),
                                        "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                    }
                                }
                                log(f"MCPæœåŠ¡å™¨: PyPDF2ç›´æ¥å¤„ç†å®Œæˆ")
                            else:
                                # å¦‚æœPyPDF2æ— æ³•æå–æ–‡æœ¬ï¼Œå°è¯•Unstructured
                                log(f"MCPæœåŠ¡å™¨: PyPDF2æœªæå–åˆ°æ–‡æœ¬ï¼Œå°è¯•Unstructured...")
                                raise Exception("PyPDF2æœªæå–åˆ°æ–‡æœ¬")
                    except Exception as e:
                        log(f"MCPæœåŠ¡å™¨: PyPDF2ç›´æ¥å¤„ç†å¤±è´¥: {e}")
                        log(f"MCPæœåŠ¡å™¨: å°è¯•ç”¨Unstructuredå¹¶è®¾ç½®è¶…æ—¶...")
                        
                        # é€‰é¡¹2ï¼šä½¿ç”¨Unstructuredå¹¶è®¾ç½®è¶…æ—¶ï¼ˆå›é€€æ–¹æ¡ˆï¼‰
                        # ä½¿ç”¨çº¿ç¨‹æ§åˆ¶è¶…æ—¶ä»¥é¿å…å¡æ­»
                        import threading
                        import time
                        
                        elements = None
                        processing_error = None
                        
                        def process_pdf():
                            nonlocal elements, processing_error
                            try:
                                from rag_core import partition
                                log(f"MCPæœåŠ¡å™¨: å¼€å§‹ç”¨strategy='fast'è¿›è¡ŒPDFåˆ†åŒº...")
                                log(f"MCPæœåŠ¡å™¨: å¤„ç†æ–‡ä»¶: {os.path.basename(temp_file_path)}")
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=1000)
                                log(f"MCPæœåŠ¡å™¨: åˆ†åŒºå®Œæˆï¼Œæå–äº†{len(elements)}ä¸ªå…ƒç´ ")
                            except Exception as e:
                                processing_error = e
                                log(f"MCPæœåŠ¡å™¨: åˆ†åŒºå‡ºé”™: {e}")
                        
                        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œå¤„ç†å¹¶è®¾ç½®è¶…æ—¶
                        thread = threading.Thread(target=process_pdf)
                        thread.daemon = True
                        thread.start()
                        
                        # æœ€å¤šç­‰å¾…30ç§’å¤„ç†å®Œæˆ
                        timeout_seconds = 30  # ä»60ç§’é™åˆ°30ç§’ï¼ˆé€‚åˆCursorï¼‰
                        
                        # å¤„ç†æœŸé—´æ˜¾ç¤ºè¿›åº¦æ—¥å¿—
                        log(f"MCPæœåŠ¡å™¨: ç­‰å¾…å¤„ç†ï¼ˆè¶…æ—¶: {timeout_seconds}ç§’ï¼‰...")
                        
                        # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        for i in range(0, timeout_seconds, 10):
                            thread.join(10)  # ç­‰å¾…10ç§’
                            if not thread.is_alive():
                                break
                            log(f"MCPæœåŠ¡å™¨: å¤„ç†è¿›è¡Œä¸­... ({i+10}/{timeout_seconds}ç§’)")
                        
                        # æ£€æŸ¥æ˜¯å¦å®Œæˆæˆ–éœ€è¦ç»§ç»­ç­‰å¾…
                        if thread.is_alive():
                            remaining_time = timeout_seconds - (timeout_seconds // 10) * 10
                            if remaining_time > 0:
                                thread.join(remaining_time)
                        
                        if thread.is_alive():
                            log(f"MCPæœåŠ¡å™¨: PDFå¤„ç†åœ¨{timeout_seconds}ç§’åè¶…æ—¶")
                            # å°è¯•ä½¿ç”¨æœ€å°é…ç½®
                            log(f"MCPæœåŠ¡å™¨: å°è¯•æœ€å°é…ç½®...")
                            try:
                                from rag_core import partition
                                elements = partition(filename=temp_file_path, strategy="fast", max_partition=500)
                                log(f"MCPæœåŠ¡å™¨: æœ€å°åˆ†åŒºå®Œæˆï¼Œæå–äº†{len(elements)}ä¸ªå…ƒç´ ")
                            except Exception as e:
                                log(f"MCPæœåŠ¡å™¨: æœ€å°åˆ†åŒºå‡ºé”™: {e}")
                                return f"âŒ **è¶…æ—¶é”™è¯¯:** PDFå¤„ç†æ—¶é—´è¿‡é•¿ã€‚\n\nğŸ’¡ **å»ºè®®ï¼š**\n- PDFå¯èƒ½è¿‡å¤§æˆ–å¤æ‚\n- å°è¯•è¾ƒå°çš„PDF\n- æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸå"
                        
                        if processing_error:
                            log(f"MCPæœåŠ¡å™¨: å¤„ç†å‡ºé”™: {processing_error}")
                            return f"âŒ **PDFå¤„ç†é”™è¯¯:** {processing_error}\n\nğŸ’¡ **å»ºè®®ï¼š**\n- æ–‡ä»¶å¯èƒ½æŸå\n- å°è¯•å…¶ä»–PDF\n- ç¡®ä¿æ–‡ä»¶å¯è®¿é—®"
                        
                        if not elements:
                            log(f"MCPæœåŠ¡å™¨: æ— æ³•ä»PDFä¸­æå–å…ƒç´ ")
                            # å°è¯•ä½¿ç”¨PyPDF2ä½œä¸ºå›é€€æ–¹æ¡ˆ
                            log(f"MCPæœåŠ¡å™¨: å°è¯•ç”¨PyPDF2ä½œä¸ºå›é€€æ–¹æ¡ˆ...")
                            try:
                                import PyPDF2
                                with open(temp_file_path, 'rb') as file:
                                    pdf_reader = PyPDF2.PdfReader(file)
                                    processed_content = ""
                                    for page_num, page in enumerate(pdf_reader.pages):
                                        page_text = page.extract_text()
                                        if page_text:
                                            processed_content += f"\n--- ç¬¬{page_num + 1}é¡µ ---\n{page_text}\n"
                                    
                                    if processed_content.strip():
                                        log(f"MCPæœåŠ¡å™¨: PyPDF2å›é€€æˆåŠŸï¼Œæå–äº†{len(processed_content)}å­—ç¬¦")
                                        metadata = {
                                            "source": os.path.basename(temp_file_path),
                                            "file_path": temp_file_path,
                                            "file_type": ".pdf",
                                            "processed_date": datetime.now().isoformat(),
                                            "processing_method": "pypdf2_fallback",
                                            "structural_info": {
                                                "total_elements": len(pdf_reader.pages),
                                                "titles_count": 0,
                                                "tables_count": 0,
                                                "lists_count": 0,
                                                "narrative_blocks": len(pdf_reader.pages),
                                                "other_elements": 0,
                                                "total_text_length": len(processed_content),
                                                "avg_element_length": len(processed_content) / len(pdf_reader.pages) if pdf_reader.pages else 0
                                            }
                                        }
                                    else:
                                        return f"âŒ **é”™è¯¯:** æ— æ³•ç”¨ä»»ä½•æ–¹æ³•ä»PDFä¸­æå–æ–‡æœ¬ã€‚\n\nğŸ’¡ **å»ºè®®:**\n- PDFå¯èƒ½ä¸ºæ‰«æç‰ˆï¼ˆä»…å›¾åƒï¼‰\n- æ–‡ä»¶å¯èƒ½æŸå\n- å°è¯•å…¶ä»–PDF"
                            except ImportError:
                                log(f"MCPæœåŠ¡å™¨: PyPDF2 ä¸å¯ç”¨")
                                return f"âŒ **é”™è¯¯:** æ— æ³•ä»PDFä¸­æå–å…ƒç´ ã€‚\n\nğŸ’¡ **å»ºè®®:**\n- æ–‡ä»¶å¯èƒ½ä¸ºç©ºæˆ–æŸå\n- å°è¯•å…¶ä»–PDF"
                            except Exception as e:
                                log(f"MCPæœåŠ¡å™¨: PyPDF2å›é€€å¤±è´¥: {e}")
                                return f"âŒ **é”™è¯¯:** æ— æ³•ä»PDFä¸­æå–ä»»ä½•æ–‡æœ¬ã€‚\n\nğŸ’¡ **å»ºè®®:**\n- PDFå¯èƒ½ä¸ºæ‰«æç‰ˆï¼ˆä»…å›¾åƒï¼‰\n- æ–‡ä»¶å¯èƒ½æŸå\n- å°è¯•å…¶ä»–PDF"
                        else:
                            log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†æå–çš„å…ƒç´ ...")
                            from rag_core import process_unstructured_elements, extract_structural_metadata
                            processed_content = process_unstructured_elements(elements)
                            log(f"MCPæœåŠ¡å™¨: å…ƒç´ å¤„ç†å®Œæˆï¼Œæå–äº†{len(processed_content)}å­—ç¬¦")
                            
                            metadata = extract_structural_metadata(elements, temp_file_path)
                            metadata["processing_method"] = "unstructured_fast_pdf"
                            log(f"MCPæœåŠ¡å™¨: ç»“æ„åŒ–å…ƒæ•°æ®æå–å®Œæˆ")
                else:
                    # å¯¹äºå…¶ä»–æ ¼å¼ï¼Œä½¿ç”¨æ­£å¸¸å¤„ç†
                    processed_content, metadata = load_document_with_fallbacks(temp_file_path)
                
                if not processed_content or processed_content.isspace():
                    log(f"MCPæœåŠ¡å™¨: è­¦å‘Š: æ–‡ä»¶å·²ä¸‹è½½ä½†æœªèƒ½æå–å†…å®¹: {url}")
                    return f"è­¦å‘Š: URL '{url}' çš„æ–‡ä»¶å·²ä¸‹è½½ï¼Œä½†æœªèƒ½æå–æ–‡æœ¬å†…å®¹ã€‚"
                
                log(f"MCPæœåŠ¡å™¨: æ–‡ä»¶ä¸‹è½½å¹¶å¤„ç†æˆåŠŸï¼ˆ{len(processed_content)} å­—ç¬¦ï¼‰")
                
                # ä¿å­˜å¤„ç†åçš„å‰¯æœ¬
                log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä¿å­˜å¤„ç†å‰¯æœ¬...")
                processing_method = metadata.get("processing_method", "unstructured_enhanced")
                domain = parsed_url.netloc.replace('.', '_')
                path = parsed_url.path.replace('/', '_').replace('.', '_')
                if not path or path == '_':
                    path = 'homepage'
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{domain}_{path}_{timestamp}_{processing_method}.md"
                processed_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
                
                try:
                    ensure_converted_docs_directory()
                    with open(processed_filepath, 'w', encoding='utf-8') as f:
                        f.write(processed_content)
                    log(f"MCPæœåŠ¡å™¨: å·²ä¿å­˜å¤„ç†å‰¯æœ¬: {processed_filepath}")
                except Exception as e:
                    log(f"MCPæœåŠ¡å™¨è­¦å‘Š: æ— æ³•ä¿å­˜å¤„ç†å‰¯æœ¬: {e}")
                    processed_filepath = ""
                
                # ä¸°å¯Œå…ƒæ•°æ®
                enhanced_metadata = metadata.copy()
                enhanced_metadata.update({
                    "source": url,
                    "domain": parsed_url.netloc,
                    "input_type": "url_download",
                    "converted_to_md": processed_filepath if processed_filepath else "No",
                    "server_processed_date": datetime.now().isoformat()
                })
                
                # ä½¿ç”¨å¢å¼ºå¤„ç†
                log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“...")
                add_text_to_knowledge_base_enhanced(
                    processed_content, 
                    rag_state["vector_store"], 
                    enhanced_metadata, 
                    use_semantic_chunking=True
                )
                
                # æ„å»ºä¿¡æ¯æ€§å›ç­”
                structural_info = enhanced_metadata.get("structural_info", {})
                
                response_parts = [
                    f"âœ… **æ–‡ä»¶ä¸‹è½½å¹¶å¤„ç†æˆåŠŸ**",
                    f"ğŸŒ **URL:** {url}",
                    f"ğŸ“„ **æ–‡ä»¶:** {os.path.basename(parsed_url.path)}",
                    f"ğŸ“‹ **ç±»å‹:** {file_extension.upper()}",
                    f"ğŸ”§ **æ–¹æ³•:** {processing_method.replace('_', ' ').title()}"
                ]
                
                # å¦‚æœæœ‰ç»“æ„ä¿¡æ¯åˆ™æ·»åŠ 
                if structural_info:
                    response_parts.extend([
                        f"ğŸ“Š **æ–‡æ¡£ç»“æ„:**",
                        f"   â€¢ æ€»å…ƒç´ : {structural_info.get('total_elements', 'N/A')}",
                        f"   â€¢ æ ‡é¢˜: {structural_info.get('titles_count', 'N/A')}",
                        f"   â€¢ è¡¨æ ¼: {structural_info.get('tables_count', 'N/A')}",
                        f"   â€¢ åˆ—è¡¨: {structural_info.get('lists_count', 'N/A')}",
                        f"   â€¢ å™è¿°å—: {structural_info.get('narrative_blocks', 'N/A')}"
                    ])
                
                if processed_filepath:
                    response_parts.append(f"ğŸ’¾ **å‰¯æœ¬å·²ä¿å­˜:** {processed_filepath}")
                
                response_parts.append(f"ğŸ“š **çŠ¶æ€:** å·²é€šè¿‡è¯­ä¹‰åˆ†å—æ·»åŠ åˆ°çŸ¥è¯†åº“")
                
                log(f"MCPæœåŠ¡å™¨: URLå¤„ç†å®Œæˆ")
                return "\n".join(response_parts)
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_file_path)
                    log(f"MCPæœåŠ¡å™¨: ä¸´æ—¶æ–‡ä»¶å·²åˆ é™¤: {temp_file_path}")
                except Exception as e:
                    log(f"MCPæœåŠ¡å™¨è­¦å‘Š: æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {e}")
        
        else:
            # ç½‘é¡µçš„ä¼ ç»Ÿå¤„ç†
            log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ç”¨MarkItDownå¤„ç†ç½‘é¡µå†…å®¹...")
            
            # ä½¿ç”¨MarkItDownç›´æ¥å¤„ç†URLå¹¶è®¾ç½®è¶…æ—¶
            try:
                result = md_converter.convert_url(url)
                markdown_content = result.text_content
            except Exception as e:
                log(f"MCPæœåŠ¡å™¨: MarkItDownå¤„ç†å¤±è´¥ï¼Œå°è¯•ç›´æ¥ä¸‹è½½: {e}")
                # å›é€€ï¼šå°è¯•ç›´æ¥ä¸‹è½½
                import requests
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                markdown_content = response.text

            if not markdown_content or markdown_content.isspace():
                log(f"MCPæœåŠ¡å™¨: è­¦å‘Š: URLå·²å¤„ç†ä½†æœªèƒ½æå–å†…å®¹: {url}")
                return f"è­¦å‘Š: URL '{url}' å·²å¤„ç†ï¼Œä½†æœªèƒ½æå–æ–‡æœ¬å†…å®¹ã€‚"

            log(f"MCPæœåŠ¡å™¨: URLå†…å®¹è½¬æ¢æˆåŠŸï¼ˆ{len(markdown_content)} å­—ç¬¦ï¼‰")
            
            # ä¿å­˜Markdownå‰¯æœ¬
            log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä¿å­˜Markdownå‰¯æœ¬...")            # Crear nombre de archivo basado en la URL
            domain = parsed_url.netloc.replace('.', '_')
            path = parsed_url.path.replace('/', '_').replace('.', '_')
            if not path or path == '_':
                path = 'homepage'
            
            # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{domain}_{path}_{timestamp}_markitdown.md"
            md_filepath = os.path.join(CONVERTED_DOCS_DIR, filename)
            
            # ä¿å­˜å†…å®¹
            try:
                ensure_converted_docs_directory()
                with open(md_filepath, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                log(f"MCPæœåŠ¡å™¨: å·²ä¿å­˜Markdownå‰¯æœ¬: {md_filepath}")
            except Exception as e:
                log(f"MCPæœåŠ¡å™¨è­¦å‘Š: æ— æ³•ä¿å­˜Markdownå‰¯æœ¬: {e}")
                md_filepath = ""
            
            # æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“
            log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ·»åŠ å†…å®¹åˆ°çŸ¥è¯†åº“...")
            
            # åˆ›å»ºURLç‰¹å®šçš„å…ƒæ•°æ®
            url_metadata = {
                "source": url,
                "domain": parsed_url.netloc,
                "input_type": "url_web",
                "processed_date": datetime.now().isoformat(),
                "processing_method": "markitdown",
                "converted_to_md": md_filepath if md_filepath else "No"
            }
            
            # ç›´æ¥æ·»åŠ å…ƒæ•°æ®
            add_text_to_knowledge_base(markdown_content, rag_state["vector_store"], url_metadata)
            
            # å®Œæˆè¿‡ç¨‹çš„ä¿¡æ¯
            response_parts = [
                f"âœ… **ç½‘é¡µå†…å®¹å¤„ç†æˆåŠŸ**",
                f"ğŸŒ **URL:** {url}",
                f"ğŸŒ **åŸŸå:** {parsed_url.netloc}",
                f"ğŸ”§ **æ–¹æ³•:** MarkItDown"
            ]
            
            if md_filepath:
                response_parts.append(f"ğŸ’¾ **å‰¯æœ¬å·²ä¿å­˜:** {md_filepath}")
            
            response_parts.append(f"ğŸ“š **çŠ¶æ€:** å·²æ·»åŠ åˆ°çŸ¥è¯†åº“")
            
            log(f"MCPæœåŠ¡å™¨: URLå¤„ç†å®Œæˆ")
            return "\n".join(response_parts)

    except requests.exceptions.Timeout:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†URLè¶…æ—¶: {url}")
        return f"âŒ **è¶…æ—¶é”™è¯¯:** URL '{url}' å“åº”è¿‡æ…¢ã€‚\n\nğŸ’¡ **å»ºè®®ï¼š**\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- ç¨åé‡è¯•\n- ç›®æ ‡ç½‘ç«™å¯èƒ½ä¸´æ—¶ç¼“æ…¢"
    except requests.exceptions.ConnectionError:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†URLæ—¶è¿æ¥é”™è¯¯: {url}")
        return f"âŒ **è¿æ¥é”™è¯¯:** æ— æ³•è¿æ¥åˆ° URL '{url}'ã€‚\n\nğŸ’¡ **å»ºè®®ï¼š**\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- ç›®æ ‡ç½‘ç«™å¯èƒ½ä¸å¯ç”¨\n- ç¨åé‡è¯•"
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†URL '{url}' æ—¶å‡ºé”™: {e}")
        error_msg = f"âŒ **å¤„ç† URL '{url}' æ—¶å‡ºé”™:** {e}"
        # æä¾›æ›´æœ‰ç”¨çš„å»ºè®®
        if "404" in str(e) or "Not Found" in str(e):
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** URL ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦æ­£ç¡®ã€‚"
        elif "timeout" in str(e).lower():
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** é¡µé¢åŠ è½½è¿‡æ…¢ï¼Œè¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥ç½‘ç»œã€‚"
        elif "permission" in str(e).lower() or "403" in str(e):
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** æ²¡æœ‰è®¿é—®æƒé™ï¼Œéƒ¨åˆ†ç½‘ç«™ç¦æ­¢è‡ªåŠ¨è®¿é—®ã€‚"
        elif "youtube" in url.lower() and "transcript" in str(e).lower():
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** æ­¤ YouTube è§†é¢‘æ— å¯ç”¨å­—å¹•æˆ–å·²ç¦ç”¨ã€‚"
        elif "ssl" in str(e).lower() or "certificate" in str(e).lower():
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** SSL è¯ä¹¦é—®é¢˜ï¼Œè¯·å°è¯•å…¶ä»–é“¾æ¥ã€‚"
        elif "download" in str(e).lower() or "connection" in str(e).lower():
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** ä¸‹è½½æ–‡ä»¶å‡ºé”™ï¼Œè¯·ç¡®è®¤é“¾æ¥å¯è®¿é—®ä¸”æ–‡ä»¶å­˜åœ¨ã€‚"
        elif "unstructured" in str(e).lower():
            error_msg += "\n\nğŸ’¡ **å»ºè®®ï¼š** æ–‡æ¡£å¤„ç†å‡ºé”™ï¼Œæ–‡ä»¶å¯èƒ½æŸåæˆ–è¿‡å¤§ã€‚"
        return error_msg

@mcp.tool()
def ask_rag(query: str) -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æé—®ï¼Œå¹¶æ ¹æ®å­˜å‚¨çš„ä¿¡æ¯è¿”å›ç­”æ¡ˆã€‚
    ä½¿ç”¨åœºæ™¯ï¼š
    - è¯¢é—®ç‰¹å®šä¸»é¢˜æˆ–æ¦‚å¿µ
    - è¯·æ±‚è§£é‡Šæˆ–å®šä¹‰
    - ä»å¤„ç†è¿‡çš„æ–‡æ¡£è·å–ä¿¡æ¯
    - åŸºäºå·²å­¦ä¹ çš„æ–‡æœ¬æˆ–æ–‡æ¡£è·å–ç­”æ¡ˆ

    ç³»ç»Ÿå°†æœç´¢æ‰€æœ‰å­˜å‚¨çš„çŸ¥è¯†ï¼Œå¹¶è¿”å›æœ€ç›¸å…³çš„ç»“æœã€‚

    å‚æ•°ï¼š
        query: è¦å‘çŸ¥è¯†åº“æå‡ºçš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†é—®é¢˜: {query}")
    initialize_rag()
    
    try:
        # ä½¿ç”¨æ ‡å‡†QAé“¾ï¼ˆæ— è¿‡æ»¤å™¨ï¼‰
        qa_chain = get_qa_chain(rag_state["vector_store"])
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰ç›¸å…³ä¿¡æ¯
        if not source_documents:
            # æ— å¯ç”¨æ¥æºï¼Œå¯èƒ½ç³»ç»Ÿå°šæœªåŠ è½½ç›¸å…³ä¿¡æ¯
            enhanced_answer = f"ğŸ¤– **å›ç­”:**\n\nâŒ **æœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼Œæ— æ³•å›ç­”ã€‚**\n\n"
            enhanced_answer += "ğŸ’¡ **å»ºè®®:**\n"
            enhanced_answer += "â€¢ ç¡®ä¿å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æé—®\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€\n"
            enhanced_answer += "â€¢ è€ƒè™‘åŠ è½½æ›´å¤šç›¸å…³æ–‡æ¡£\n\n"
            enhanced_answer += "âš ï¸ **æç¤º:** ç³»ç»Ÿä»…åŸºäºå·²åŠ è½½ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥æº")
            return enhanced_answer
        
        # æ£€æŸ¥å›ç­”æ˜¯å¦å¯èƒ½æ˜¯å¹»è§‰
        # å¦‚æœæ²¡æœ‰æ¥æºä½†æœ‰å›ç­”ï¼Œå¯èƒ½æ˜¯å¹»è§‰ç»“æœ
        if len(source_documents) == 0 and answer.strip():
            # æœ‰å›ç­”ä½†æ— æ¥æºï¼Œå¯èƒ½ä¸ºå¹»è§‰ç»“æœ
            enhanced_answer = f"ğŸ¤– **å›ç­”:**\n\nâŒ **æœªæ‰¾åˆ°ç‰¹å®šä¿¡æ¯ï¼Œæ— æ³•å‡†ç¡®å›ç­”ã€‚**\n\n"
            enhanced_answer += "ğŸ’¡ **å»ºè®®:**\n"
            enhanced_answer += "â€¢ ç¡®ä¿å·²åŠ è½½ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£\n"
            enhanced_answer += "â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æé—®\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æ£€æŸ¥çŸ¥è¯†åº“çŠ¶æ€\n\n"
            enhanced_answer += "âš ï¸ **æç¤º:** ç³»ç»Ÿä»…åŸºäºå·²åŠ è½½ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰å›ç­”ï¼ˆæ— æ¥æºï¼‰")
            return enhanced_answer
        
        # å¦‚æœæœ‰å¯ç”¨æ¥æºï¼Œæ„å»ºæ­£å¸¸å›ç­”
        enhanced_answer = f"ğŸ¤– **å›ç­”:**\n\n{answer}\n"
        
        # æ·»åŠ æ¥æºä¿¡æ¯åŠæ›´å¤šè¯¦æƒ…
        if source_documents:
            enhanced_answer += "ğŸ“š **ä½¿ç”¨çš„ä¿¡æ¯æ¥æº:**\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "æœªçŸ¥æ¥æº")
                
                # --- æ”¹è¿›æ¥æºä¿¡æ¯ ---
                source_info = f"   {i}. **{source_name}**"
                
                # å¦‚æœæ˜¯æ–‡æ¡£åˆ™æ·»åŠ å®Œæ•´è·¯å¾„
                file_path = metadata.get("file_path")
                if file_path:
                    source_info += f"\n      - **è·¯å¾„:** `{file_path}`"
                
                # å¦‚æœæœ‰æ–‡ä»¶ç±»å‹åˆ™æ·»åŠ 
                file_type = metadata.get("file_type")
                if file_type:
                    source_info += f"\n      - **ç±»å‹:** {file_type.upper()}"
                
                # å¦‚æœæœ‰å¤„ç†æ–¹æ³•åˆ™æ·»åŠ 
                processing_method = metadata.get("processing_method")
                if processing_method:
                    method_display = processing_method.replace('_', ' ').title()
                    source_info += f"\n      - **å¤„ç†æ–¹æ³•:** {method_display}"
                
                # å¦‚æœæœ‰ç»“æ„ä¿¡æ¯åˆ™æ·»åŠ 
                structural_info = metadata.get("structural_info")
                if structural_info:
                    source_info += f"\n      - **ç»“æ„:** {structural_info.get('total_elements', 'N/A')} ä¸ªå…ƒç´ "
                    titles_count = structural_info.get('titles_count', 0)
                    tables_count = structural_info.get('tables_count', 0)
                    lists_count = structural_info.get('lists_count', 0)
                    if titles_count > 0 or tables_count > 0 or lists_count > 0:
                        structure_details = []
                        if titles_count > 0:
                            structure_details.append(f"{titles_count} ä¸ªæ ‡é¢˜")
                        if tables_count > 0:
                            structure_details.append(f"{tables_count} ä¸ªè¡¨æ ¼")
                        if lists_count > 0:
                            structure_details.append(f"{lists_count} ä¸ªåˆ—è¡¨")
                        source_info += f" ({', '.join(structure_details)})"
                
                # ä»æ‰å¹³å…ƒæ•°æ®é‡æ„ç»“æ„ä¿¡æ¯
                structural_elements = []
                titles_count = metadata.get("structural_titles_count", 0)
                tables_count = metadata.get("structural_tables_count", 0)
                lists_count = metadata.get("structural_lists_count", 0)
                total_elements = metadata.get("structural_total_elements", 0)
                
                if total_elements > 0:
                    structural_details = []
                    if titles_count > 0:
                        structural_details.append(f"{titles_count} ä¸ªæ ‡é¢˜")
                    if tables_count > 0:
                        structural_details.append(f"{tables_count} ä¸ªè¡¨æ ¼")
                    if lists_count > 0:
                        structural_details.append(f"{lists_count} ä¸ªåˆ—è¡¨")
                    
                    if structural_details:
                        source_info += f"\n      - **ç»“æ„:** {', '.join(structural_details)}"
                
                enhanced_answer += source_info + "\n\n"
        
        # æ·»åŠ å›ç­”è´¨é‡ä¿¡æ¯
        num_sources = len(source_documents)
        if num_sources >= 3:
            enhanced_answer += "\nâœ… **é«˜å¯ä¿¡åº¦:** å›ç­”åŸºäºå¤šä¸ªæ¥æº"
        elif num_sources == 2:
            enhanced_answer += "\nâš ï¸ **ä¸­ç­‰å¯ä¿¡åº¦:** å›ç­”åŸºäº2ä¸ªæ¥æº"
        else:
            enhanced_answer += "\nâš ï¸ **æœ‰é™å¯ä¿¡åº¦:** å›ç­”åŸºäº1ä¸ªæ¥æº"
        
        # å¦‚æœæœ‰æ–‡æ¡£ä½¿ç”¨äº†ç»“æ„åŒ–å…ƒæ•°æ®å¤„ç†åˆ™æ·»åŠ ä¿¡æ¯
        enhanced_docs = [doc for doc in source_documents if hasattr(doc, 'metadata') and doc.metadata.get("processing_method") == "unstructured_enhanced"]
        if enhanced_docs:
            enhanced_answer += f"\nğŸ§  **æ™ºèƒ½å¤„ç†:** {len(enhanced_docs)} ä¸ªæ¥æºä½¿ç”¨äº†Unstructuredå¤„ç†ï¼ˆä¿ç•™ç»“æ„ï¼‰"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸç”Ÿæˆå›ç­”ï¼Œå…±{len(source_documents)}ä¸ªæ¥æº")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ **å¤„ç†é—®é¢˜æ—¶å‡ºé”™:** {e}\n\nğŸ’¡ **å»ºè®®:**\n- æ£€æŸ¥RAGç³»ç»Ÿæ˜¯å¦æ­£ç¡®åˆå§‹åŒ–\n- å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜\n- å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·é‡å¯æœåŠ¡å™¨"

@mcp.tool()
def ask_rag_filtered(query: str, file_type: str = None, min_tables: int = None, min_titles: int = None, processing_method: str = None) -> str:
    """
    å‘ RAG çŸ¥è¯†åº“æå‡ºé—®é¢˜ï¼Œä½¿ç”¨ç‰¹å®šè¿‡æ»¤å™¨èšç„¦æœç´¢èŒƒå›´ã€‚
    é€‚ç”¨äºä»ç‰¹å®šç±»å‹çš„æ–‡æ¡£æˆ–å…·æœ‰ç‰¹å®šç‰¹å¾çš„æ–‡æ¡£ä¸­è·å–ä¿¡æ¯ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - ä»…åœ¨PDFæ–‡æ¡£ä¸­æœç´¢: file_type=".pdf"
    - æŸ¥æ‰¾åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£: min_tables=1
    - æŸ¥æ‰¾ç»“æ„è‰¯å¥½çš„æ–‡æ¡£: min_titles=5
    - åœ¨å¢å¼ºå¤„ç†çš„æ–‡æ¡£ä¸­æœç´¢: processing_method="unstructured_enhanced"
    
    é€šè¿‡è¿‡æ»¤æœç´¢èŒƒå›´æä¾›æ›´ç²¾å‡†å’Œç›¸å…³çš„ç»“æœã€‚

    å‚æ•°ï¼š
        query: å‘çŸ¥è¯†åº“æå‡ºçš„é—®é¢˜æˆ–æŸ¥è¯¢ã€‚
        file_type: æŒ‰æ–‡ä»¶ç±»å‹è¿‡æ»¤ï¼ˆä¾‹å¦‚ ".pdf", ".docx", ".txt"ï¼‰
        min_tables: æ–‡æ¡£å¿…é¡»åŒ…å«çš„æœ€å°‘è¡¨æ ¼æ•°é‡
        min_titles: æ–‡æ¡£å¿…é¡»åŒ…å«çš„æœ€å°‘æ ‡é¢˜æ•°é‡
        processing_method: æŒ‰å¤„ç†æ–¹æ³•è¿‡æ»¤ï¼ˆä¾‹å¦‚ "unstructured_enhanced", "markitdown"ï¼‰
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨å¤„ç†å¸¦è¿‡æ»¤å™¨çš„é—®é¢˜: {query}")
    log(f"MCPæœåŠ¡å™¨: åº”ç”¨çš„è¿‡æ»¤å™¨ - ç±»å‹: {file_type}, è¡¨æ ¼: {min_tables}, æ ‡é¢˜: {min_titles}, æ–¹æ³•: {processing_method}")
    initialize_rag()
    
    try:
        # åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å™¨
        metadata_filter = create_metadata_filter(
            file_type=file_type,
            processing_method=processing_method,
            min_tables=min_tables,
            min_titles=min_titles
        )
        
        # ä½¿ç”¨å¸¦è¿‡æ»¤å™¨çš„QAé“¾
        qa_chain = get_qa_chain(rag_state["vector_store"], metadata_filter)
        response = qa_chain.invoke({"query": query})
        
        answer = response.get("result", "")
        source_documents = response.get("source_documents", [])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¬¦åˆè¿‡æ»¤å™¨çš„ç›¸å…³ä¿¡æ¯
        if not source_documents:
            # æ²¡æœ‰ç¬¦åˆè¿‡æ»¤å™¨çš„æ¥æº
            enhanced_answer = f"ğŸ” **å›ç­”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:**\n\nâŒ **åœ¨çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„ç›¸å…³ä¿¡æ¯ã€‚**\n\n"
            
            # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
            if metadata_filter:
                enhanced_answer += "ğŸ“‹ **åº”ç”¨çš„è¿‡æ»¤å™¨:**\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   â€¢ æœ€å°‘è¡¨æ ¼æ•°: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   â€¢ æœ€å°‘æ ‡é¢˜æ•°: {value['$gte']}\n"
                enhanced_answer += "\n"
            
            enhanced_answer += "ğŸ’¡ **å»ºè®®:**\n"
            enhanced_answer += "â€¢ å°è¯•æ”¾å®½è¿‡æ»¤å™¨ä»¥è·å¾—æ›´å¤šç»“æœ\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£ç±»å‹\n"
            enhanced_answer += "â€¢ è€ƒè™‘ä½¿ç”¨ `ask_rag()` ä¸å¸¦è¿‡æ»¤å™¨æœç´¢æ•´ä¸ªçŸ¥è¯†åº“\n"
            enhanced_answer += "â€¢ ç¡®è®¤å·²åŠ è½½ç¬¦åˆæŒ‡å®šæ¡ä»¶çš„æ–‡æ¡£\n\n"
            enhanced_answer += "âš ï¸ **æ³¨æ„:** è¿‡æ»¤å™¨å¯èƒ½è¿‡äºä¸¥æ ¼ã€‚å°è¯•ä½¿ç”¨æ›´å®½æ¾çš„è¿‡æ»¤å™¨ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„æ¥æº")
            return enhanced_answer
        
        # éªŒè¯å“åº”æ˜¯å¦å¯èƒ½æ˜¯å¹»è§‰
        if len(source_documents) == 0 and answer.strip():
            enhanced_answer = f"ğŸ” **å“åº”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:**\n\nâŒ **æœªæ‰¾åˆ°ç¬¦åˆæŒ‡å®šè¿‡æ»¤å™¨çš„ç‰¹å®šä¿¡æ¯ã€‚**\n\n"
            
            # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
            if metadata_filter:
                enhanced_answer += "ğŸ“‹ **åº”ç”¨çš„è¿‡æ»¤å™¨:**\n"
                for key, value in metadata_filter.items():
                    if key == "file_type":
                        enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                    elif key == "processing_method":
                        enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                    elif key == "structural_tables_count":
                        enhanced_answer += f"   â€¢ MÃ­nimo de tablas: {value['$gte']}\n"
                    elif key == "structural_titles_count":
                        enhanced_answer += f"   â€¢ MÃ­nimo de tÃ­tulos: {value['$gte']}\n"
                enhanced_answer += "\nğŸ’¡ **å»ºè®®:**\n"
            enhanced_answer += "â€¢ å°è¯•æ”¾å®½è¿‡æ»¤å™¨ä»¥è·å¾—æ›´å¤šç»“æœ\n"
            enhanced_answer += "â€¢ ä½¿ç”¨ `get_knowledge_base_stats()` æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£ç±»å‹\n"
            enhanced_answer += "â€¢ è€ƒè™‘ä½¿ç”¨ä¸å¸¦è¿‡æ»¤å™¨çš„ `ask_rag()` æœç´¢æ•´ä¸ªçŸ¥è¯†åº“\n\n"
            enhanced_answer += "âš ï¸ **æ³¨æ„:** è¿‡æ»¤å™¨å¯èƒ½è¿‡äºä¸¥æ ¼ï¼Œè¯·å°è¯•ä½¿ç”¨æ›´å®½æ³›çš„è¿‡æ»¤å™¨ã€‚"
            
            log(f"MCPæœåŠ¡å™¨: è¿‡æ»¤å“åº”æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰ï¼ˆæ— æ¥æºï¼‰")
            return enhanced_answer
        
        # å¦‚æœæœ‰æ¥æºï¼Œæ„å»ºæ­£å¸¸å›ç­”
        enhanced_answer = f"ğŸ” **å›ç­”ï¼ˆå·²åº”ç”¨è¿‡æ»¤å™¨ï¼‰:**\n\n{answer}\n"
        
        # æ˜¾ç¤ºåº”ç”¨çš„è¿‡æ»¤å™¨
        if metadata_filter:
            enhanced_answer += "\nğŸ“‹ **åº”ç”¨çš„è¿‡æ»¤å™¨:**\n"
            for key, value in metadata_filter.items():
                if key == "file_type":
                    enhanced_answer += f"   â€¢ æ–‡ä»¶ç±»å‹: {value}\n"
                elif key == "processing_method":
                    enhanced_answer += f"   â€¢ å¤„ç†æ–¹æ³•: {value.replace('_', ' ').title()}\n"
                elif key == "structural_tables_count":
                    enhanced_answer += f"   â€¢ MÃ­nimo de tablas: {value['$gte']}\n"
                elif key == "structural_titles_count":
                    enhanced_answer += f"   â€¢ MÃ­nimo de tÃ­tulos: {value['$gte']}\n"
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        if source_documents:
            enhanced_answer += f"\nğŸ“š **æ‰¾åˆ°çš„æ¥æº ({len(source_documents)}):**\n\n"
            for i, doc in enumerate(source_documents, 1):
                metadata = doc.metadata if hasattr(doc, 'metadata') else {}
                source_name = metadata.get("source", "Fuente desconocida")
                
                source_info = f"   {i}. **{source_name}**"
                
                # InformaciÃ³n estructural
                tables_count = metadata.get("structural_tables_count", 0)
                titles_count = metadata.get("structural_titles_count", 0)
                file_type = metadata.get("file_type", "")
                
                structural_details = []
                if tables_count > 0:
                    structural_details.append(f"{tables_count} tablas")
                if titles_count > 0:
                    structural_details.append(f"{titles_count} tÃ­tulos")
                
                if structural_details:
                    source_info += f" ({', '.join(structural_details)})"
                
                if file_type:
                    source_info += f" [{file_type.upper()}]"
                
                enhanced_answer += source_info + "\n"
        
        # è¿‡æ»¤æœç´¢ä¿¡æ¯
        enhanced_answer += f"\nğŸ¯ **è¿‡æ»¤æœç´¢:** ç»“æœä»…é™äºç¬¦åˆæŒ‡å®šæ¡ä»¶çš„æ–‡æ¡£ã€‚"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸç”Ÿæˆè¿‡æ»¤å›ç­”ï¼Œå…±{len(source_documents)}ä¸ªæ¥æº")
        return enhanced_answer
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: å¤„ç†è¿‡æ»¤é—®é¢˜æ—¶å‡ºé”™: {e}")
        return f"âŒ **å¤„ç†è¿‡æ»¤é—®é¢˜æ—¶å‡ºé”™:** {e}"

@mcp.tool()
def get_knowledge_base_stats() -> str:
    """
    è·å–çŸ¥è¯†åº“çš„ç»¼åˆç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ–‡æ¡£ç±»å‹ã€å¤„ç†æ–¹æ³•å’Œç»“æ„ä¿¡æ¯ã€‚
    ç”¨äºäº†è§£çŸ¥è¯†åº“ä¸­æœ‰å“ªäº›ä¿¡æ¯ä»¥åŠå¦‚ä½•å¤„ç†çš„ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - æ£€æŸ¥çŸ¥è¯†åº“ä¸­æœ‰å¤šå°‘æ–‡æ¡£
    - äº†è§£æ–‡ä»¶ç±»å‹çš„åˆ†å¸ƒ
    - æŸ¥çœ‹ä½¿ç”¨äº†å“ªäº›å¤„ç†æ–¹æ³•
    - åˆ†æå­˜å‚¨æ–‡æ¡£çš„ç»“æ„å¤æ‚æ€§
    
    è¿™æœ‰åŠ©äºæ‚¨å°±æœç´¢å†…å®¹å’Œå¦‚ä½•è¿‡æ»¤æŸ¥è¯¢åšå‡ºæ˜æ™ºçš„å†³å®šã€‚

    è¿”å›ï¼š
        å…³äºçŸ¥è¯†åº“å†…å®¹çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯...")
    initialize_rag()
    
    try:
        stats = get_document_statistics(rag_state["vector_store"])
        
        if "error" in stats:
            return f"âŒ **è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™:** {stats['error']}"
        
        if stats.get("total_documents", 0) == 0:
            return "ğŸ“Š **çŸ¥è¯†åº“ä¸ºç©º**\n\nçŸ¥è¯†åº“ä¸­æ²¡æœ‰å­˜å‚¨çš„æ–‡æ¡£ã€‚"
        
        # æ„å»ºè¯¦ç»†å›ç­”
        response = f"ğŸ“Š **çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯**\n\n"
        response += f"ğŸ“š **æ–‡æ¡£æ€»æ•°:** {stats['total_documents']}\n\n"
        
        # æ–‡ä»¶ç±»å‹
        if stats["file_types"]:
            response += "ğŸ“„ **æ–‡ä»¶ç±»å‹:**\n"
            for file_type, count in sorted(stats["file_types"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                response += f"   â€¢ {file_type.upper()}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # å¤„ç†æ–¹æ³•
        if stats["processing_methods"]:
            response += "ğŸ”§ **å¤„ç†æ–¹æ³•:**\n"
            for method, count in sorted(stats["processing_methods"].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / stats["total_documents"]) * 100
                method_display = method.replace('_', ' ').title()
                response += f"   â€¢ {method_display}: {count} ({percentage:.1f}%)\n"
            response += "\n"
        
        # ç»“æ„ç»Ÿè®¡
        structural = stats["structural_stats"]
        response += "ğŸ—ï¸ **ç»“æ„ä¿¡æ¯:**\n"
        response += f"   â€¢ åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£: {structural['documents_with_tables']}\n"
        response += f"   â€¢ åŒ…å«æ ‡é¢˜çš„æ–‡æ¡£: {structural['documents_with_titles']}\n"
        response += f"   â€¢ åŒ…å«åˆ—è¡¨çš„æ–‡æ¡£: {structural['documents_with_lists']}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡è¡¨æ ¼æ•°: {structural['avg_tables_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡æ ‡é¢˜æ•°: {structural['avg_titles_per_doc']:.1f}\n"
        response += f"   â€¢ æ¯æ–‡æ¡£å¹³å‡åˆ—è¡¨æ•°: {structural['avg_lists_per_doc']:.1f}\n\n"
        
        # æœç´¢å»ºè®®
        response += "ğŸ’¡ **æœç´¢å»ºè®®:**\n"
        if structural['documents_with_tables'] > 0:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `min_tables=1` æœç´¢åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£ä¿¡æ¯\n"
        if structural['documents_with_titles'] > 5:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `min_titles=5` æœç´¢ç»“æ„è‰¯å¥½çš„æ–‡æ¡£\n"
        if ".pdf" in stats["file_types"]:
            response += f"   â€¢ ä½¿ç”¨ `ask_rag_filtered` å¸¦ `file_type=\".pdf\"` ä»…æœç´¢PDFæ–‡æ¡£\n"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸè·å–ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ **è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™:** {e}"

@mcp.tool()
def get_embedding_cache_stats() -> str:
    """
    è·å–åµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    ç”¨äºç›‘æ§ç¼“å­˜æ•ˆç‡å¹¶äº†è§£ç³»ç»Ÿæ€§èƒ½ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡ä»¥æŸ¥çœ‹ç³»ç»Ÿæ˜¯å¦é«˜æ•ˆè¿è¡Œ
    - ç›‘æ§ç¼“å­˜çš„å†…å­˜ä½¿ç”¨æƒ…å†µ
    - äº†è§£åµŒå…¥é‡å¤ä½¿ç”¨çš„é¢‘ç‡
    - è°ƒè¯•æ€§èƒ½é—®é¢˜
    
    è¿™æœ‰åŠ©äºæ‚¨ä¼˜åŒ–ç³»ç»Ÿå¹¶äº†è§£å…¶è¡Œä¸ºã€‚

    è¿”å›ï¼š
        å…³äºåµŒå…¥ç¼“å­˜æ€§èƒ½çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨è·å–åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_cache_stats()
        
        if not stats:
            return "ğŸ“Š **åµŒå…¥ç¼“å­˜ä¸å¯ç”¨**\n\nåµŒå…¥ç¼“å­˜æœªåˆå§‹åŒ–ã€‚"
        
        # æ„å»ºè¯¦ç»†å›ç­”
        response = f"ğŸ“Š **åµŒå…¥ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯**\n\n"
        
        # ä¸»è¦æŒ‡æ ‡
        response += f"ğŸ”„ **ç¼“å­˜æ´»åŠ¨:**\n"
        response += f"   â€¢ æ€»è¯·æ±‚æ•°: {stats['total_requests']}\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­æ¬¡æ•°: {stats['memory_hits']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­æ¬¡æ•°: {stats['disk_hits']}\n"
        response += f"   â€¢ æœªå‘½ä¸­æ¬¡æ•°: {stats['misses']}\n\n"
        
        # æˆåŠŸç‡
        response += f"ğŸ“ˆ **æˆåŠŸç‡:**\n"
        response += f"   â€¢ å†…å­˜å‘½ä¸­ç‡: {stats['memory_hit_rate']}\n"
        response += f"   â€¢ ç£ç›˜å‘½ä¸­ç‡: {stats['disk_hit_rate']}\n"
        response += f"   â€¢ æ€»å‘½ä¸­ç‡: {stats['overall_hit_rate']}\n\n"
        
        # å†…å­˜ä½¿ç”¨
        response += f"ğŸ’¾ **å†…å­˜ä½¿ç”¨:**\n"
        response += f"   â€¢ å†…å­˜ä¸­çš„åµŒå…¥: {stats['memory_cache_size']}\n"
        response += f"   â€¢ æœ€å¤§å†…å­˜å¤§å°: {stats['max_memory_size']}\n"
        response += f"   â€¢ ç¼“å­˜ç›®å½•: {stats['cache_directory']}\n\n"
        
        # æ€§èƒ½åˆ†æ
        total_requests = stats['total_requests']
        if total_requests > 0:
            memory_hit_rate = float(stats['memory_hit_rate'].rstrip('%'))
            overall_hit_rate = float(stats['overall_hit_rate'].rstrip('%'))
            
            response += f"ğŸ¯ **æ€§èƒ½åˆ†æ:**\n"
            
            if overall_hit_rate > 70:
                response += f"   â€¢ âœ… æ€§èƒ½å“è¶Š: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            elif overall_hit_rate > 50:
                response += f"   â€¢ âš ï¸ æ€§èƒ½ä¸­ç­‰: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ âŒ æ€§èƒ½è¾ƒä½: {overall_hit_rate:.1f}% å‘½ä¸­ç‡\n"
            
            if memory_hit_rate > 50:
                response += f"   â€¢ ğŸš€ å†…å­˜ç¼“å­˜é«˜æ•ˆ: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            else:
                response += f"   â€¢ ğŸ’¾ ä¾èµ–ç£ç›˜å­˜å‚¨: {memory_hit_rate:.1f}% å†…å­˜å‘½ä¸­ç‡\n"
            
            # ä¼˜åŒ–å»ºè®®
            response += f"\nğŸ’¡ **ä¼˜åŒ–å»ºè®®:**\n"
            if overall_hit_rate < 30:
                response += f"   â€¢ è€ƒè™‘åŒæ—¶å¤„ç†ç›¸ä¼¼çš„æ–‡æ¡£\n"
                response += f"   â€¢ æ£€æŸ¥æ˜¯å¦æœ‰å¾ˆå¤šä¸é‡å¤çš„ç‹¬ç‰¹æ–‡æœ¬\n"
            
            if memory_hit_rate < 30 and total_requests > 100:
                response += f"   â€¢ è€ƒè™‘å¢åŠ å†…å­˜ç¼“å­˜å¤§å°\n"
                response += f"   â€¢ ç£ç›˜å‘½ä¸­æ¯”å†…å­˜å‘½ä¸­é€Ÿåº¦æ…¢\n"
            
            if stats['memory_cache_size'] >= stats['max_memory_size'] * 0.9:
                response += f"   â€¢ å†…å­˜ç¼“å­˜æ¥è¿‘æ»¡è½½\n"
                response += f"   â€¢ å¦‚æœ‰å¯ç”¨RAMï¼Œè€ƒè™‘å¢åŠ max_memory_size\n"
        
        log(f"MCPæœåŠ¡å™¨: æˆåŠŸè·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        return f"âŒ **è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™:** {e}"

@mcp.tool()
def clear_embedding_cache_tool() -> str:
    """
    æ¸…é™¤åµŒå…¥ç¼“å­˜ä»¥é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚
    å½“æ‚¨æƒ³é‡ç½®ç¼“å­˜æˆ–é‡Šæ”¾èµ„æºæ—¶ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    - ç³»ç»Ÿå†…å­˜ä¸è¶³æ—¶é‡Šæ”¾å†…å­˜
    - æ›´æ”¹åµŒå…¥æ¨¡å‹åé‡ç½®ç¼“å­˜
    - æ¸…é™¤ä¸å†éœ€è¦çš„æ—§ç¼“å­˜åµŒå…¥
    - æ•…éšœæ’é™¤ç¼“å­˜ç›¸å…³é—®é¢˜
    
    è­¦å‘Š: è¿™å°†åˆ é™¤æ‰€æœ‰ç¼“å­˜çš„åµŒå…¥ï¼Œéœ€è¦é‡æ–°è®¡ç®—ã€‚

    è¿”å›ï¼š
        å…³äºç¼“å­˜æ¸…é™¤æ“ä½œçš„ç¡®è®¤æ¶ˆæ¯ã€‚
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨æ¸…é™¤åµŒå…¥ç¼“å­˜...")
    
    try:
        clear_embedding_cache()
        
        response = "ğŸ§¹ **åµŒå…¥ç¼“å­˜æ¸…é™¤æˆåŠŸ**\n\n"
        response += "âœ… å·²åˆ é™¤æ‰€æœ‰ç¼“å­˜ä¸­å­˜å‚¨çš„åµŒå…¥ã€‚\n"
        response += "ğŸ“ ä¸‹æ¬¡éœ€è¦æ—¶å°†ä»å¤´è®¡ç®—åµŒå…¥ã€‚\n"
        response += "ğŸ’¾ å·²é‡Šæ”¾å†…å­˜å’Œç£ç›˜ç©ºé—´ã€‚\n\n"
        response += "âš ï¸ **æ³¨æ„:** éœ€è¦æ—¶åµŒå…¥å°†è‡ªåŠ¨é‡æ–°è®¡ç®—ã€‚"
        
        log(f"MCPæœåŠ¡å™¨: åµŒå…¥ç¼“å­˜æ¸…é™¤æˆåŠŸ")
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨: æ¸…é™¤ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return f"âŒ **æ¸…é™¤ç¼“å­˜æ—¶å‡ºé”™:** {e}"

@mcp.tool()
def optimize_vector_database() -> str:
    """
    ä¼˜åŒ–å‘é‡æ•°æ®åº“ä»¥æé«˜æœç´¢æ€§èƒ½ã€‚
    æ­¤å·¥å…·é‡æ–°ç»„ç»‡å†…éƒ¨ç´¢å¼•ä»¥å®ç°æ›´å¿«çš„æœç´¢ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·å½“ï¼š
    - æœç´¢é€Ÿåº¦ç¼“æ…¢
    - æ·»åŠ äº†è®¸å¤šæ–°æ–‡æ¡£
    - æƒ³è¦æé«˜ç³»ç»Ÿæ•´ä½“æ€§èƒ½
    
    è¿”å›ï¼š
        å…³äºä¼˜åŒ–è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log("MCPæœåŠ¡å™¨: æ­£åœ¨ä¼˜åŒ–å‘é‡æ•°æ®åº“...")
    
    try:
        result = optimize_vector_store()
        
        if result["status"] == "success":
            response = f"âœ… **å‘é‡æ•°æ®åº“ä¼˜åŒ–æˆåŠŸ**\n\n"
            response += f"ğŸ“Š **ä¼˜åŒ–å‰ç»Ÿè®¡:**\n"
            stats_before = result.get("stats_before", {})
            response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats_before.get('total_documents', 'N/A')}\n"
            
            response += f"\nğŸ“Š **ä¼˜åŒ–åç»Ÿè®¡:**\n"
            stats_after = result.get("stats_after", {})
            response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats_after.get('total_documents', 'N/A')}\n"
            
            response += f"\nğŸš€ **ä¼˜åŠ¿:**\n"
            response += f"   â€¢ æœç´¢é€Ÿåº¦æ›´å¿«\n"
            response += f"   â€¢ ç»“æœç²¾åº¦æ›´é«˜\n"
            response += f"   â€¢ ç´¢å¼•å·²ä¼˜åŒ–\n"
            
        else:
            response = f"âŒ **ä¼˜åŒ–æ•°æ®åº“æ—¶å‡ºé”™:** {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: ä¼˜åŒ–å‡ºé”™: {e}")
        return f"âŒ **ä¼˜åŒ–å‘é‡æ•°æ®åº“æ—¶å‡ºé”™:** {str(e)}"

@mcp.tool()
def get_vector_database_stats() -> str:
    """
    è·å–å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚
    åŒ…æ‹¬æ–‡æ¡£ã€æ–‡ä»¶ç±»å‹å’Œé…ç½®ä¿¡æ¯ã€‚
    
    ä½¿ç”¨æ­¤å·¥å…·æ¥ï¼š
    - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
    - åˆ†ææ–‡æ¡£åˆ†å¸ƒ
    - è¯Šæ–­æ€§èƒ½é—®é¢˜
    - è§„åˆ’ä¼˜åŒ–
    
    è¿”å›ï¼š
        å‘é‡æ•°æ®åº“çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    """
    log("MCPæœåŠ¡å™¨: æ­£åœ¨è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
    
    try:
        stats = get_vector_store_stats()
        
        if "error" in stats:
            return f"âŒ **è·å–ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™:** {stats['error']}"
        
        response = f"ğŸ“Š **å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯**\n\n"
        
        response += f"ğŸ“š **åŸºæœ¬ä¿¡æ¯:**\n"
        response += f"   â€¢ æ–‡æ¡£æ€»æ•°: {stats.get('total_documents', 0)}\n"
        response += f"   â€¢ é›†åˆåç§°: {stats.get('collection_name', 'N/A')}\n"
        response += f"   â€¢ åµŒå…¥ç»´åº¦: {stats.get('embedding_dimension', 'N/A')}\n"
        
        # æ–‡ä»¶ç±»å‹
        file_types = stats.get('file_types', {})
        if file_types:
            response += f"\nğŸ“„ **æŒ‰æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:**\n"
            for file_type, count in file_types.items():
                response += f"   â€¢ {file_type}: {count} ä¸ªæ–‡æ¡£\n"
        
        # å¤„ç†æ–¹æ³•
        processing_methods = stats.get('processing_methods', {})
        if processing_methods:
            response += f"\nğŸ”§ **å¤„ç†æ–¹æ³•:**\n"
            for method, count in processing_methods.items():
                response += f"   â€¢ {method}: {count} ä¸ªæ–‡æ¡£\n"
        
        # æ¨èé…ç½®æ–‡ä»¶
        try:
            recommended_profile = get_optimal_vector_store_profile()
            response += f"\nğŸ¯ **æ¨èé…ç½®:** {recommended_profile}\n"
        except:
            pass
        
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: è·å–ç»Ÿè®¡ä¿¡æ¯å‡ºé”™: {e}")
        return f"âŒ **è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºé”™:** {str(e)}"

@mcp.tool()
def reindex_vector_database(profile: str = 'auto') -> str:
    """
    ä½¿ç”¨ä¼˜åŒ–é…ç½®é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“ã€‚
    æ­¤å·¥å…·ç”¨å½“å‰å¤§å°çš„ä¼˜åŒ–å‚æ•°é‡æ–°åˆ›å»ºç´¢å¼•ã€‚
    
    å‚æ•°:
        profile: é…ç½®æ–‡ä»¶ ('small', 'medium', 'large', 'auto')
                 'auto' è‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®
    
    ä½¿ç”¨æ­¤å·¥å…·å½“ï¼š
    - æ›´æ”¹é…ç½®æ–‡ä»¶
    - æœç´¢éå¸¸ç¼“æ…¢
    - æƒ³è¦ä¸ºç‰¹å®šæ•°æ®åº“å¤§å°ä¼˜åŒ–
    - å­˜åœ¨æŒç»­çš„æ€§èƒ½é—®é¢˜
    
    âš ï¸ **æ³¨æ„:** æ­¤è¿‡ç¨‹å¯èƒ½éœ€è¦æ—¶é—´ï¼Œå–å†³äºæ•°æ®åº“å¤§å°ã€‚
    
    è¿”å›ï¼š
        å…³äºé‡æ–°ç´¢å¼•è¿‡ç¨‹çš„ä¿¡æ¯
    """
    log(f"MCPæœåŠ¡å™¨: æ­£åœ¨ä½¿ç”¨é…ç½®'{profile}'é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“...")
    
    try:
        result = reindex_vector_store(profile=profile)
        
        if result["status"] == "success":
            response = f"âœ… **å‘é‡æ•°æ®åº“é‡æ–°ç´¢å¼•æˆåŠŸ**\n\n"
            response += f"ğŸ“Š **å¤„ç†ä¿¡æ¯:**\n"
            response += f"   â€¢ åº”ç”¨çš„é…ç½®: {profile}\n"
            response += f"   â€¢ å¤„ç†çš„æ–‡æ¡£: {result.get('documents_processed', 0)}\n"
            
            response += f"\nğŸš€ **é‡æ–°ç´¢å¼•çš„å¥½å¤„:**\n"
            response += f"   â€¢ é’ˆå¯¹å½“å‰å¤§å°ä¼˜åŒ–çš„ç´¢å¼•\n"
            response += f"   â€¢ æ›´å¿«æ›´ç²¾ç¡®çš„æœç´¢\n"
            response += f"   â€¢ æ›´å¥½çš„å†…å­˜ä½¿ç”¨\n"
            
        elif result["status"] == "warning":
            response = f"âš ï¸ **è­¦å‘Š:** {result.get('message', 'æ²¡æœ‰æ–‡æ¡£éœ€è¦é‡æ–°ç´¢å¼•')}"
            
        else:
            response = f"âŒ **é‡æ–°ç´¢å¼•æ•°æ®åº“æ—¶å‡ºé”™:** {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            
        return response
        
    except Exception as e:
        log(f"MCPæœåŠ¡å™¨é”™è¯¯: é‡æ–°ç´¢å¼•å‡ºé”™: {e}")
        return f"âŒ **é‡æ–°ç´¢å¼•å‘é‡æ•°æ®åº“æ—¶å‡ºé”™:** {str(e)}"

# --- è¿è¡ŒæœåŠ¡å™¨çš„å…¥å£ç‚¹ ---
if __name__ == "__main__":
    log("æ­£åœ¨å¯åŠ¨MCP RAGæœåŠ¡å™¨...")
    warm_up_rag_system()  # å¯åŠ¨æ—¶é¢„çƒ­ç³»ç»Ÿ
    mcp.run(transport='stdio') 